{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import statsmodels\n",
    "from collections import Counter\n",
    "import cpi\n",
    "import ast\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_dict(df):\n",
    "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
    "    for column in dict_columns:\n",
    "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df\n",
    "\n",
    "def classify_movie_budget(budget):\n",
    "    if budget == 0:\n",
    "        return 'no data'\n",
    "    elif budget < 400000:\n",
    "        return 'micro-budget'\n",
    "    elif budget < 2000000:\n",
    "        return 'low-budget'\n",
    "    elif budget < 10000000:\n",
    "        return 'middle-budget'\n",
    "    else:\n",
    "        return 'high-budget'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     3000 non-null   int64  \n",
      " 1   belongs_to_collection  604 non-null    object \n",
      " 2   budget                 3000 non-null   int64  \n",
      " 3   genres                 2993 non-null   object \n",
      " 4   homepage               946 non-null    object \n",
      " 5   imdb_id                3000 non-null   object \n",
      " 6   original_language      3000 non-null   object \n",
      " 7   original_title         3000 non-null   object \n",
      " 8   overview               2992 non-null   object \n",
      " 9   popularity             3000 non-null   float64\n",
      " 10  poster_path            2999 non-null   object \n",
      " 11  production_companies   2844 non-null   object \n",
      " 12  production_countries   2945 non-null   object \n",
      " 13  release_date           3000 non-null   object \n",
      " 14  runtime                2998 non-null   float64\n",
      " 15  spoken_languages       2980 non-null   object \n",
      " 16  status                 3000 non-null   object \n",
      " 17  tagline                2403 non-null   object \n",
      " 18  title                  3000 non-null   object \n",
      " 19  Keywords               2724 non-null   object \n",
      " 20  cast                   2987 non-null   object \n",
      " 21  crew                   2984 non-null   object \n",
      " 22  revenue                3000 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(18)\n",
      "memory usage: 539.2+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
    "dataset_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset_original.copy()\n",
    "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
    "\n",
    "\n",
    "def fix_date(date):\n",
    "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
    "    if x.year > 2020:\n",
    "        year = x.year - 100\n",
    "    else:\n",
    "        year = x.year\n",
    "    return datetime.datetime(year,x.month,x.day)\n",
    "\n",
    "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
    "\n",
    "def adjust_price_to_inflation(price, date):\n",
    "    return int(cpi.inflate(price, date.year))\n",
    "\n",
    "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
    "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def map_list_to_integer(lists):\n",
    "    a = {}\n",
    "    for l in lists:\n",
    "        for name in l:\n",
    "            if(name not in a):\n",
    "                a[name] = len(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
    "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
    "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cast_names = cast_mapping.keys()\n",
    "# cast_names\n",
    "# for name in cast_names:\n",
    "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
    "\n",
    "def get_dictionary(s):\n",
    "    try:\n",
    "        d = eval(s)\n",
    "    except:\n",
    "        d = {}\n",
    "    return d\n",
    "\n",
    "for col in json_cols + ['belongs_to_collection'] :\n",
    "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
    "    \n",
    "dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
    "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
    "\n",
    "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -5))\n",
    "# dataset.cast.str.join('|').str.get_dummies(drop_first=True).add_prefix('actor_')\n",
    "\n",
    "dataset = dataset.explode(\"cast\")\n",
    "dataset = pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      " 1/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      " 1/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      " 1/4:\n",
      "train = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train.info()\n",
      " 1/5: train.head()\n",
      " 1/6: train.describe(include='all')\n",
      " 1/7: train.describe(include='all')\n",
      " 1/8:\n",
      "# Missing values \n",
      "train.isna().sum()\n",
      " 1/9:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "1/10:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "1/11:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "1/12:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/13:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"r\")\n",
      "plt.show()\n",
      "1/14:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"y\")\n",
      "plt.show()\n",
      "1/15:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"p\")\n",
      "plt.show()\n",
      "1/16:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"k\")\n",
      "plt.show()\n",
      "1/17:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"m\")\n",
      "plt.show()\n",
      "1/18:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/19:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", color=\"b\")\n",
      "plt.show()\n",
      "1/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"hex\", color=\"b\")\n",
      "plt.show()\n",
      "1/21:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", color=\"b\")\n",
      "plt.show()\n",
      "1/22:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", logistic=True, color=\"b\")\n",
      "plt.show()\n",
      "1/23:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "1/24:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "1/25:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", logistic=True, color=\"b\")\n",
      "plt.show()\n",
      "1/26:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", x_estimator=train.mean, color=\"b\")\n",
      "plt.show()\n",
      "1/27:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", x_estimator=train.info().mean, color=\"b\")\n",
      "plt.show()\n",
      "1/28:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", x_estimator=train.describe().mean, color=\"b\")\n",
      "plt.show()\n",
      "1/29:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", color=\"b\")\n",
      "plt.show()\n",
      "1/30:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/31:\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/32:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, color=\"b\")\n",
      "plt.show()\n",
      "1/33:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/34:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "1/35:\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      " 2/1:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      " 2/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      " 2/3:\n",
      "\n",
      "train = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train.info()\n",
      " 2/4:\n",
      "\n",
      "train.head()\n",
      " 2/5:\n",
      "\n",
      "train.describe(include='all')\n",
      " 2/6:\n",
      "\n",
      "# Missing values \n",
      "train.isna().sum()\n",
      " 2/7:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      " 2/8:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      " 2/9:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "2/10:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "2/11:\n",
      "\n",
      "sns.distplot(train.revenue, height=11)\n",
      "2/12:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "1/36:\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "1/37:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/13:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "2/14:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/15:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "2/16:\n",
      "\n",
      "train = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train.info()\n",
      "2/17:\n",
      "\n",
      "train.head()\n",
      "2/18:\n",
      "\n",
      "train.describe(include='all')\n",
      "2/19:\n",
      "\n",
      "# Missing values \n",
      "train.isna().sum()\n",
      "2/20:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/21:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/22:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/23:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      " 3/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/ed'])\n",
      " 4/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 5/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 5/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      " 6/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 6/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      " 7/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 7/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      " 8/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 8/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      " 9/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 9/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "10/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "10/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "11/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "11/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "12/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "12/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "13/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "13/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "14/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "14/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "15/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "15/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "16/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "16/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "17/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "17/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "18/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "18/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "18/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(np.diff(r_peaks))\n",
      "print(np.average(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(np.diff(r_peaks))\n",
      "print(np.average(r_peaks/100))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(np.diff(r_peaks))\n",
      "print(np.average(r_peaks)/100)\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(np.diff(r_peaks))\n",
      "print(np.average(np.diff(r_peaks)))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \" + np.average(np.diff(r_peaks)))\n",
      "print(\"Heart rate:\" + np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/8:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Heart rate:\", np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "19/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "19/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "20/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "20/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "20/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "plt.cla()\n",
      "plt.plot(r_peaks, np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "20/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "plt.cla()\n",
      "plt.plot(r_peaks.drop(1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "20/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "# r_peaks = []\n",
      "# for peak in peaksAll:\n",
      "#     if ecg[peak] > 0.3:\n",
      "#         r_peaks.append(peak)\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "plt.cla()\n",
      "plt.plot(r_peaks.drop(1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "21/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "21/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "22/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "22/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "23/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "23/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "24/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "24/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "25/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "25/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "26/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "26/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "26/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "26/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "26/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "26/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "26/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "27/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "27/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "28/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "28/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "29/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "29/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "30/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "30/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "31/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "31/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "32/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "32/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "33/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "33/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "34/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "34/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "35/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "35/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "36/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "36/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "36/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", 60/np.diff(r_peaks)*1000)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "36/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", 60/np.diff(r_peaks)*100)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "37/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "37/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(r_peaks, hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "# prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "# \n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "# \n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/8:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "# \n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "# \n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/9:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def moving_average(a, n=3):\n",
      "    ret = np.cumsum(a, dtype=float)\n",
      "    ret[n:] = ret[n:] - ret[:-n]\n",
      "    return ret[n - 1:] / n\n",
      "\n",
      "\n",
      "prt_peaks, _ = moving_average(ecg)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/10:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def moving_average(a, n=3):\n",
      "    ret = np.cumsum(a, dtype=float)\n",
      "    ret[n:] = ret[n:] - ret[:-n]\n",
      "    return ret[n - 1:] / n\n",
      "\n",
      "\n",
      "prt_peaks, _ = moving_average(ecg, 3)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/11:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks, _ = running_mean(ecg, 3)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/12:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/13:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "plot(prt_peaks)\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/14:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/2:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, prt_peaks, \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT = find_peaks(ecg_filt, height=0.2)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/8:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/9:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/10:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/11:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/12:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/13:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 50)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/14:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 30)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/15:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 10)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/16:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/17:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 8)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/18:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 7)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/19:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/20:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/21:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 1)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/22:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 0.1)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/23:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 1)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/24:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/25:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 1)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/26:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/27:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.1)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/28:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.02)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/29:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=-0.02)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/30:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.02)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/31:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.02)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/32:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/33:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/34:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/35:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/36:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/37:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/38:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/39:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.06, 0.2])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/40:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.1, 0.2])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/41:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.2, 0.3])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/42:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.2, 0.3])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/43:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.2, 0.3])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "# todo: plot PRT peaks\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/44:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "# todo: plot PRT peaks\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/45:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "# todo: plot PRT peaks\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "38/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "38/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "38/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : DICOM\n",
      "\n",
      "import pydicom\n",
      "from tkinter import *\n",
      "from PIL import Image, ImageTk\n",
      "\n",
      "class MainWindow():\n",
      "\n",
      "    ds = pydicom.dcmread(\"./data/head.dcm\")\n",
      "    data = ds.pixel_array\n",
      "\n",
      "    def __init__(self, main):\n",
      "        # print patient name\n",
      "        print(self.ds.PatientName)\n",
      "\n",
      "        # prepare canvas\n",
      "        self.canvas = Canvas(main, width=512, height=512)\n",
      "        self.canvas.grid(row=0, column=0)\n",
      "        self.canvas.bind(\"<Button-1>\", self.initWindow)\n",
      "        self.canvas.bind(\"<B1-Motion>\", self.updateWindow)\n",
      "        self.canvas.bind(\"<Button-3>\", self.initMeasure)\n",
      "        self.canvas.bind(\"<B3-Motion>\", self.updateMeasure)\n",
      "        self.canvas.bind(\"<ButtonRelease-3>\", self.finishMeasure)\n",
      "\n",
      "        # load image\n",
      "        # todo: apply transform\n",
      "        #self.array = self.transformData(self.data, self.winWidth, self.winCenter)\n",
      "        self.array = self.data\n",
      "        self.image = Image.fromarray(self.array)\n",
      "        self.image = self.image.resize((512, 512), Image.ANTIALIAS)\n",
      "        self.img = ImageTk.PhotoImage(image=self.image, master=root)\n",
      "        self.image_on_canvas = self.canvas.create_image(0, 0, anchor = NW, image = self.img)\n",
      "\n",
      "\n",
      "    def transformData(self, data, window, level):\n",
      "        # todo: transform data\n",
      "        return data\n",
      "\n",
      "\n",
      "    def initWindow(self, event):\n",
      "        # todo: save mouse position\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def updateWindow(self, event):\n",
      "        # todo: modify window width and center\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "        #self.array2 = self.transformData(self.data, self.winWidth, self.winCenter)\n",
      "        #self.image2 = Image.fromarray(self.array2)\n",
      "        #self.image2 = self.image2.resize((512, 512), Image.ANTIALIAS)\n",
      "        #self.img2 = ImageTk.PhotoImage(image=self.image2, master=root)\n",
      "        #self.canvas.itemconfig(self.image_on_canvas, image = self.img2)\n",
      "\n",
      "\n",
      "    def initMeasure(self, event):\n",
      "        # todo: save mouse position\n",
      "        # todo: create line\n",
      "        # hint: self.canvas.create_line(...)\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def updateMeasure(self, event):\n",
      "        # todo: update line\n",
      "        # hint: self.canvas.coords(...)\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def finishMeasure(self, event):\n",
      "        # todo: print measured length in mm\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "\n",
      "\n",
      "#----------------------------------------------------------------------\n",
      "\n",
      "root = Tk()\n",
      "MainWindow(root)\n",
      "root.mainloop()\n",
      "39/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "39/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "40/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "40/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "41/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "41/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "42/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "42/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "43/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "43/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "44/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "44/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "45/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "45/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "46/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "46/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "47/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "47/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "48/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "48/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "49/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "49/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "50/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "50/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "51/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "51/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "52/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "52/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "53/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "53/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "54/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "54/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "55/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "56/1:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      " %matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "56/2:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/3:\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      " %matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "56/4:\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "56/5:\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/6:\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/7:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "56/8:\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/9:\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "56/10:\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "56/11:\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/12:\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/13:\n",
      "# NumPy - przeglądanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "56/14:\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem równym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "56/15:\n",
      "filename = 'iris.csv'\n",
      "dataset = csv.read\n",
      "57/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "56/16:\n",
      "filename = 'iris.csv'\n",
      "dataset = csv.reader(filename, delimiter=',')\n",
      "56/17:\n",
      "filename = 'iris.csv'\n",
      "dataset = pd.read_csv(filename, delimiter=',')\n",
      "56/18:\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "56/19: print(dataset.size)\n",
      "56/20: dataset.head(20)\n",
      "56/21:\n",
      "# descriptions\n",
      "print(dataset.description())\n",
      "56/22:\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "56/23:\n",
      "# class distribution\n",
      "print(dataset.groupby(variety).size())\n",
      "56/24:\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "56/25:\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "56/26:\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "56/27:\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "56/28:\n",
      "array = dataset.values\n",
      "X = array[0]\n",
      "Y = array[1]\n",
      "56/29:\n",
      "array = dataset.values\n",
      "X = array[0]\n",
      "Y = array[1]\n",
      "X\n",
      "56/30:\n",
      "array = dataset.values\n",
      "X = array[:,0]\n",
      "Y = array[1]\n",
      "X\n",
      "56/31:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[1]\n",
      "X\n",
      "56/32:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[1]\n",
      "array\n",
      "56/33:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[1]\n",
      "X\n",
      "56/34:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "Y\n",
      "56/35:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "56/36:\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "56/37:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = x[0.8:, :]\n",
      "56/38:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[0.8:, :]\n",
      "56/39:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[0.8:]\n",
      "56/40:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[x[:80,:], x[80:,:]]\n",
      "56/41:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:]\n",
      "56/42:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80,:]. Y[80:,:]\n",
      "56/43:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80]. Y[80:]\n",
      "56/44:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "56/45:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "X_train.size()\n",
      "56/46:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "X_train\n",
      "56/47:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "X_train.size\n",
      "56/48:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "X_test.size\n",
      "56/49:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "56/50:\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "56/51:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "Y_train.size\n",
      "56/52:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "Y_test.size\n",
      "56/53:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "56/54:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "Y.size\n",
      "56/55:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "56/56:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "56/57:\n",
      "\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/58:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/59:\n",
      "\n",
      "# NumPy - przeglądanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "56/60:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem równym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "56/61:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "56/62:\n",
      "\n",
      "print(dataset.size)\n",
      "56/63:\n",
      "\n",
      "dataset.head(20)\n",
      "56/64:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "56/65:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "56/66:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "56/67:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "56/68:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "56/69:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "56/70:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "56/71:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_seed()\n",
      "56/72:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "56/73:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "56/74:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "56/75:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = getattr(name)()\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "names = [\"M1\", \"M2\", \"M3\"]\n",
      "results = [0.5, 0.6, 0.7]\n",
      "56/76:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = getattr(self,name)()\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "names = [\"M1\", \"M2\", \"M3\"]\n",
      "results = [0.5, 0.6, 0.7]\n",
      "56/77:\n",
      "\n",
      "names = [LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = name\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "names = [\"M1\", \"M2\", \"M3\"]\n",
      "results = [0.5, 0.6, 0.7]\n",
      "56/78:\n",
      "\n",
      "names = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = name\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "names = [\"M1\", \"M2\", \"M3\"]\n",
      "results = [0.5, 0.6, 0.7]\n",
      "56/79:\n",
      "\n",
      "names = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = name\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "print(results)\n",
      "56/80:\n",
      "\n",
      "names = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in names:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "56/81:\n",
      "\n",
      "names = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in names:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "56/82:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "56/83:\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "56/84:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "56/85:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "56/86:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "56/87:\n",
      "\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/88:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/89:\n",
      "\n",
      "# NumPy - przeglądanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "56/90:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem równym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "56/91:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "56/92:\n",
      "\n",
      "print(dataset.size)\n",
      "56/93:\n",
      "\n",
      "dataset.head(20)\n",
      "56/94:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "56/95:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "56/96:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "56/97:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "56/98:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "56/99:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "56/100:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "56/101:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "56/102:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "56/103:\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "56/104:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "56/105:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "58/1:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "58/2:\n",
      "\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "58/3:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "58/4:\n",
      "\n",
      "# NumPy - przeglądanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "58/5:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem równym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "58/6:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "58/7:\n",
      "\n",
      "print(dataset.size)\n",
      "58/8:\n",
      "\n",
      "dataset.head(20)\n",
      "58/9:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "58/10:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "58/11:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "58/12:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "58/13:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "58/14:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "58/15:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "58/16:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "58/17:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "58/18:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "58/19:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "58/20:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "58/21:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "60/1:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "60/2:\n",
      "\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "60/3:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "60/4:\n",
      "\n",
      "# NumPy - przeglądanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "60/5:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem równym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "60/6:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "60/7:\n",
      "\n",
      "print(dataset.size)\n",
      "60/8:\n",
      "\n",
      "dataset.head(20)\n",
      "60/9:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "60/10:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "60/11:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "60/12:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "60/13:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "60/14:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "60/15:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "60/16:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "60/17:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "60/18:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "60/19:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "60/20:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/1:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "61/2:\n",
      "\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "61/3:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "61/4:\n",
      "\n",
      "# NumPy - przeglądanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "61/5:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem równym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "61/6:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "61/7:\n",
      "\n",
      "print(dataset.size)\n",
      "61/8:\n",
      "\n",
      "dataset.head(20)\n",
      "61/9:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "61/10:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "61/11:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "61/12:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "61/13:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "61/14:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "61/15:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "61/16:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "61/17:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/18:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/19:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/20:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "62/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/Lab0-student'])\n",
      "61/21:\n",
      "\n",
      "names = [\"LR\", \"KNeighbors\", \"DecisionTree\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/22:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/23:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwiązanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - może powodować błędy\n",
      "# %matplotlib notebook\n",
      "61/24:\n",
      "\n",
      "# Załaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "61/25:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "61/26:\n",
      "\n",
      "# NumPy - przeglądanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "61/27:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem równym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "61/28:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "61/29:\n",
      "\n",
      "print(dataset.size)\n",
      "61/30:\n",
      "\n",
      "dataset.head(20)\n",
      "61/31:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "61/32:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "61/33:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "61/34:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "61/35:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "61/36:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "61/37:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "61/38:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "61/39:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/40:\n",
      "\n",
      "names = [\"LR\", \"KNeighbors\", \"DecisionTree\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/41:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/42:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/43:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "61/44:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/45:\n",
      "\n",
      "names = [\"LR\", \"KNeighbors\", \"DecisionTree\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/46:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/47:\n",
      "\n",
      "names = [\"LR\", \"KNeighbors\", \"DecisionTree\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmów\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/48:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/49:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "63/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "63/2:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/8:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5.5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/9:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 4)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/10:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 6)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/11:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 6)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "64/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "64/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "65/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "65/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "66/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "66/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "67/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "67/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "68/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "68/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "69/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "69/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "70/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "70/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "71/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "71/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "72/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "72/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "73/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "73/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "74/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "75/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "75/2:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : DICOM\n",
      "\n",
      "import pydicom\n",
      "from tkinter import *\n",
      "from PIL import Image, ImageTk\n",
      "\n",
      "\n",
      "class MainWindow():\n",
      "    ds = pydicom.dcmread(\"./data/head.dcm\")\n",
      "    data = ds.pixel_array\n",
      "    winWidth = ds.WindowWidth\n",
      "    winCenter = ds.WindowCenter\n",
      "    fromX = 0\n",
      "    fromY = 0\n",
      "    measurementFromX = 0\n",
      "    measurementFromY = 0\n",
      "\n",
      "    def __init__(self, main):\n",
      "        # print patient name\n",
      "        print(self.ds.PatientName)\n",
      "\n",
      "        # prepare canvas\n",
      "        self.canvas = Canvas(main, width=512, height=512)\n",
      "        self.canvas.grid(row=0, column=0)\n",
      "        self.canvas.bind(\"<Button-1>\", self.initWindow)\n",
      "        self.canvas.bind(\"<B1-Motion>\", self.updateWindow)\n",
      "        self.canvas.bind(\"<Button-3>\", self.initMeasure)\n",
      "        self.canvas.bind(\"<B3-Motion>\", self.updateMeasure)\n",
      "        self.canvas.bind(\"<ButtonRelease-3>\", self.finishMeasure)\n",
      "        self.canvas.bind(\"\")\n",
      "\n",
      "        # load image\n",
      "        self.array = self.transformData(self.data, self.winWidth, self.winCenter)\n",
      "        self.image = Image.fromarray(self.array)\n",
      "        self.image = self.image.resize((512, 512), Image.ANTIALIAS)\n",
      "        self.img = ImageTk.PhotoImage(image=self.image, master=root)\n",
      "        self.image_on_canvas = self.canvas.create_image(0, 0, anchor=NW, image=self.img)\n",
      "\n",
      "    def transformData(self, data, window, level):\n",
      "        start = level - window / 2\n",
      "        return (data - start) / window * 255\n",
      "\n",
      "    def initWindow(self, event):\n",
      "        self.fromX = event.x\n",
      "        self.fromY = event.y\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def updateWindow(self, event):\n",
      "        self.winCenter -= event.x - self.fromX\n",
      "        self.winWidth -= event.y - self.fromY\n",
      "        self.winCenter = max(self.winCenter, 0)\n",
      "        self.fromX = event.x\n",
      "        self.fromY = event.y\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "        self.array2 = self.transformData(self.data, self.winWidth, self.winCenter)\n",
      "        self.image2 = Image.fromarray(self.array2)\n",
      "        self.image2 = self.image2.resize((512, 512), Image.ANTIALIAS)\n",
      "        self.img2 = ImageTk.PhotoImage(image=self.image2, master=root)\n",
      "        self.canvas.itemconfig(self.image_on_canvas, image = self.img2)\n",
      "\n",
      "    def initMeasure(self, event):\n",
      "        # todo: save mouse position\n",
      "        self.measurementFromX = event.x\n",
      "        self.measurementFromY = event.y\n",
      "        # todo: create line\n",
      "        self.canvas.create_line(event.x, event.y)\n",
      "        # hint: self.canvas.create_line(...)\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def updateMeasure(self, event):\n",
      "        # todo: update line\n",
      "        self.canvas.create_line(self.measurementFromX, self.measurementFromX, event.x, event.y)\n",
      "        # hint: self.canvas.coords(...)\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def finishMeasure(self, event):\n",
      "        # todo: print measured length in mm\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "\n",
      "root = Tk()\n",
      "MainWindow(root)\n",
      "root.mainloop()\n",
      "76/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "77/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "77/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "78/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "78/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "79/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "79/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "80/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "80/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "81/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "81/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "82/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "82/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "83/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "83/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "84/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "84/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "85/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "85/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "86/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "87/1:\n",
      "\n",
      "print(__doc__)\n",
      "87/2:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "87/3:\n",
      "\n",
      "df = pd.read_csv('./nyt-frame.csv', header = 0)\n",
      "87/4:\n",
      "\n",
      "print(__doc__)\n",
      "87/5:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "87/6:\n",
      "\n",
      "df = pd.read_csv('./nyt-frame.csv', header = 0)\n",
      "87/7:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "87/8:\n",
      "\n",
      "print(original_headers)\n",
      "87/9:\n",
      "\n",
      "df\n",
      "87/10:\n",
      "\n",
      "df.loc[:,'class.labels']\n",
      "87/11:\n",
      "\n",
      "df.size\n",
      "87/12:\n",
      "\n",
      "df.shape\n",
      "87/13:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "87/14:\n",
      "\n",
      "df.shape\n",
      "87/15:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "87/16:\n",
      "\n",
      "array = data.as_matrix()\n",
      "87/17:\n",
      "\n",
      "array = data.values\n",
      "87/18:\n",
      "\n",
      "array.shape\n",
      "87/19:\n",
      "\n",
      "print(__doc__)\n",
      "87/20:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "87/21:\n",
      "\n",
      "df = pd.read_csv('./nyt-frame.csv', header = 0)\n",
      "87/22:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "87/23:\n",
      "\n",
      "print(original_headers)\n",
      "87/24:\n",
      "\n",
      "df\n",
      "87/25:\n",
      "\n",
      "df.loc[:,'class.labels']\n",
      "87/26:\n",
      "\n",
      "df.size\n",
      "87/27:\n",
      "\n",
      "df.shape\n",
      "87/28:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "87/29:\n",
      "\n",
      "df.shape\n",
      "87/30:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "87/31:\n",
      "\n",
      "array = data.values\n",
      "87/32:\n",
      "\n",
      "array.shape\n",
      "87/33:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "87/34:\n",
      "\n",
      "pca = PCA()\n",
      "87/35:\n",
      "\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "87/36:\n",
      "\n",
      "y = df.loc[:,'class.labels']\n",
      "reds = y == 'art'\n",
      "blues = y == 'music'\n",
      "87/37:\n",
      "\n",
      "header = list(df.columns.values[9:])\n",
      "sample_word = np.random.choice(header, 20, replace=False)\n",
      "print(sample_word)\n",
      "87/38:\n",
      "\n",
      "indices = np.random.choice(array.shape[0], 2, replace=False)\n",
      "sample = array[indices,:]\n",
      "87/39:\n",
      "\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(reds), 0], X_pca[np.array(reds), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.scatter(X_pca[np.array(blues), 0], X_pca[np.array(blues), 1], c=\"blue\",s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "87/40:\n",
      "\n",
      "k = 15\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "sorted = list(np.argsort(pc2))\n",
      "print('----------PC2-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "87/41:\n",
      "\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "87/42:\n",
      "\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:10]))\n",
      "88/1:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/2:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "88/3:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/4:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "88/5:\n",
      "\n",
      "print(original_headers)\n",
      "88/6:\n",
      "\n",
      "df\n",
      "88/7:\n",
      "\n",
      "df.loc[:,'class.labels']\n",
      "88/8:\n",
      "\n",
      "print(__doc__)\n",
      "88/9:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "88/10:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/11:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "88/12:\n",
      "\n",
      "print(original_headers)\n",
      "88/13:\n",
      "\n",
      "df\n",
      "88/14:\n",
      "\n",
      "df.size\n",
      "88/15:\n",
      "\n",
      "df.shape\n",
      "88/16:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "88/17:\n",
      "\n",
      "df.shape\n",
      "88/18:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "88/19:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "data_scaled.mean(axis=0)\n",
      "88/20:\n",
      "\n",
      "array = data_scaled.values\n",
      "88/21: array = data_scaled\n",
      "88/22: array = data_scaled\n",
      "88/23: array.shape\n",
      "88/24:\n",
      "\n",
      "print(__doc__)\n",
      "88/25:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "88/26:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/27:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "88/28:\n",
      "\n",
      "print(original_headers)\n",
      "88/29:\n",
      "\n",
      "df\n",
      "88/30:\n",
      "\n",
      "df.size\n",
      "88/31:\n",
      "\n",
      "df.shape\n",
      "88/32:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "88/33:\n",
      "\n",
      "df.shape\n",
      "88/34:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "88/35:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "data_scaled.mean(axis=0)\n",
      "88/36:\n",
      "\n",
      "array = data_scaled.values\n",
      "88/37:\n",
      "\n",
      "print(__doc__)\n",
      "88/38:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "88/39:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/40:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "88/41:\n",
      "\n",
      "print(original_headers)\n",
      "88/42:\n",
      "\n",
      "df\n",
      "88/43:\n",
      "\n",
      "df.size\n",
      "88/44:\n",
      "\n",
      "df.shape\n",
      "88/45:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "88/46:\n",
      "\n",
      "df.shape\n",
      "88/47:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "88/48:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "data_scaled.mean(axis=0)\n",
      "88/49:\n",
      "\n",
      "array = data_scaled\n",
      "88/50:\n",
      "\n",
      "array.shape\n",
      "88/51:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "88/52:\n",
      "\n",
      "pca = PCA()\n",
      "88/53:\n",
      "\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "88/54:\n",
      "\n",
      "y = df.loc[:,'class.labels']\n",
      "reds = y == 'art'\n",
      "blues = y == 'music'\n",
      "88/55:\n",
      "header = list(df.columns.values[9:])\n",
      "sample_word = np.random.choice(header, 20, replace=False)\n",
      "print(sample_word)\n",
      "88/56: data_scaled.mean(axis=0)\n",
      "88/57: data_scaled.variance(axis=0)\n",
      "88/58: data_scaled.var(axis=0)\n",
      "88/59:\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "88/60:\n",
      "\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "88/61:\n",
      "\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:10]))\n",
      "88/62: data_scaled.var(axis=0)\n",
      "88/63: data_scaled.var(axis=0)\n",
      "88/64: array = data_scaled\n",
      "88/65: array.shape\n",
      "88/66:\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "88/67:\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "88/68:\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:10]))\n",
      "88/69:\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:2]))\n",
      "print(sum(variance_ratio[0:3]))\n",
      "88/70:\n",
      "k = 15\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/71:\n",
      "k = 10\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/72:\n",
      "k = 11\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/73:\n",
      "k = 12\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/74:\n",
      "k = 11\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/75:\n",
      "k = 11\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/76:\n",
      "pc1 = pca.components_[0]\n",
      "3\n",
      "pc2 = pca.components_[1]\n",
      "print(’Attribute, PC1, PC2’)\n",
      "for i in range(0,pc1.shape[0]):\n",
      "print(attributes[i] + ’:’ + repr(pc1[i]) + ’:’ + repr(pc2[i]))\n",
      "88/77:\n",
      "pc1 = pca.components_[0]\n",
      "3\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/78:\n",
      "pc1 = pca.components_[0]\n",
      "3\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/79:\n",
      "k = 2\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "sorted = list(np.argsort(pc2))\n",
      "print('----------PC2-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "88/80:\n",
      "k = 3\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "sorted = list(np.argsort(pc2))\n",
      "print('----------PC2-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "88/81:\n",
      "pc1 = pca.components_[0]\n",
      "3\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/82:\n",
      "pc1 = pca.components_[0]\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/83: y = df.loc[:,'Vehicle Name']\n",
      "89/1:\n",
      "\n",
      "print(__doc__)\n",
      "89/2:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "89/3:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "89/4:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "89/5:\n",
      "\n",
      "print(original_headers)\n",
      "89/6:\n",
      "\n",
      "df\n",
      "89/7:\n",
      "\n",
      "df.size\n",
      "89/8:\n",
      "\n",
      "df.shape\n",
      "89/9:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "89/10:\n",
      "\n",
      "df.shape\n",
      "89/11:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "89/12:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "89/13:\n",
      "\n",
      "data_scaled.mean(axis=0)\n",
      "89/14:\n",
      "\n",
      "data_scaled.var(axis=0)\n",
      "89/15:\n",
      "\n",
      "array = data_scaled\n",
      "89/16:\n",
      "\n",
      "array.shape\n",
      "89/17:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "89/18:\n",
      "\n",
      "pca = PCA()\n",
      "89/19:\n",
      "\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "89/20:\n",
      "\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "89/21:\n",
      "\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:2]))\n",
      "print(sum(variance_ratio[0:3]))\n",
      "89/22:\n",
      "\n",
      "pc1 = pca.components_[0]\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/84: y = df.loc[:,'Vehicle Name']\n",
      "88/85: y = df.loc[:,'Vehicle Name']\n",
      "88/86:\n",
      "y = df.loc[:,'Vehicle Name']\n",
      "print(y)\n",
      "88/87: y = df.loc[:,'Vehicle Name']\n",
      "88/88:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(y), 0], X_pca[np.array(y), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/89:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(true), 0], X_pca[np.array(y), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/90:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(True), 0], X_pca[np.array(y), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/91:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(True), 0], X_pca[np.array(True), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "89/23:\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/92:\n",
      "x = X_pca[:,0]\n",
      "y = X_pca[:,1]\n",
      "88/93:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(True), 0], X_pca[np.array(y), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/94:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/95:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "\n",
      "for i, txt in enumerate(n):\n",
      "    ax.annotate(txt, (z[i], y[i]))\n",
      "\n",
      "plt.show()\n",
      "88/96:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(z, y)\n",
      "for i, txt in enumerate(n):\n",
      "    ax.annotate(txt, (z[i], y[i]))\n",
      "\n",
      "plt.show()\n",
      "88/97:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(z, y)\n",
      "for i, txt in enumerate(labels):\n",
      "    ax.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "88/98:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "for i, txt in enumerate(labels):\n",
      "    ax.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "88/99:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "for i, txt in enumerate(labels):\n",
      "    plt.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "88/100:\n",
      "plt.figure(figsize=(30, 20))\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "for i, txt in enumerate(labels):\n",
      "    plt.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "88/101:\n",
      "def biplot(score,coeff,pcax,pcay,labels=None):\n",
      "    pca1=pcax-1\n",
      "    pca2=pcay-1\n",
      "    xs = score[:,pca1]\n",
      "    ys = score[:,pca2]\n",
      "    n=score.shape[1]\n",
      "    scalex = 1.0/(xs.max()- xs.min())\n",
      "    scaley = 1.0/(ys.max()- ys.min())\n",
      "    plt.scatter(xs*scalex,ys*scaley)\n",
      "    for i in range(n):\n",
      "        plt.arrow(0, 0, coeff[i,pca1], coeff[i,pca2],color='r',alpha=0.5) \n",
      "        if labels is None:\n",
      "            plt.text(coeff[i,pca1]* 1.15, coeff[i,pca2] * 1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
      "        else:\n",
      "            plt.text(coeff[i,pca1]* 1.15, coeff[i,pca2] * 1.15, labels[i], color='g', ha='center', va='center')\n",
      "    plt.xlim(-1,1)\n",
      "    plt.ylim(-1,1)\n",
      "    plt.xlabel(\"PC{}\".format(pcax))\n",
      "    plt.ylabel(\"PC{}\".format(pcay))\n",
      "    plt.grid()\n",
      "88/102: biplot(score,pca.components_,1,2,labels=categories)\n",
      "88/103: biplot(y,X_pca.components_,1,2,labels=categories)\n",
      "88/104: biplot(x, y,1,2,labels=labels)\n",
      "88/105: biplot(X_pca,x,1,2,labels=labels)\n",
      "88/106: biplot(X_pca,y,1,2,labels=labels)\n",
      "88/107: biplot(X_pca,X_pca,1,2,labels=labels)\n",
      "88/108: biplot(data,pca.components_,1,2,labels=labels)\n",
      "88/109:\n",
      "def myplot(score,coeff,labels=None):\n",
      "    plt.figure(figsize=(15,10))\n",
      "    plt.xlabel(\"PC{}\".format(1))\n",
      "    plt.ylabel(\"PC{}\".format(2))\n",
      "    xs = score[:,0]\n",
      "    ys = score[:,1]\n",
      "    n = coeff.shape[0]\n",
      "    scalex = 1.0/(xs.max() - xs.min())\n",
      "    scaley = 1.0/(ys.max() - ys.min())\n",
      "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
      "    for i in range(n):\n",
      "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
      "        if labels is None:\n",
      "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
      "        else:\n",
      "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
      "\n",
      "print(pca.components_[0:2, :])\n",
      "myplot(X_pca[:,0:2],np.transpose(pca.components_[0:2, :]),attributes)\n",
      "axes = plt.gca()\n",
      "axes.set_ylim([-1.0,1.0])\n",
      "plt.show()\n",
      "89/24:\n",
      "\n",
      "print(__doc__)\n",
      "89/25:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "89/26:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "89/27:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "89/28:\n",
      "\n",
      "print(original_headers)\n",
      "89/29:\n",
      "\n",
      "df\n",
      "89/30:\n",
      "\n",
      "df.size\n",
      "89/31:\n",
      "\n",
      "df.shape\n",
      "89/32:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "89/33:\n",
      "\n",
      "df.shape\n",
      "89/34:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "89/35:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "89/36:\n",
      "\n",
      "data_scaled.mean(axis=0)\n",
      "89/37:\n",
      "\n",
      "data_scaled.var(axis=0)\n",
      "89/38:\n",
      "\n",
      "array = data_scaled\n",
      "89/39:\n",
      "\n",
      "array.shape\n",
      "89/40:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "89/41:\n",
      "\n",
      "pca = PCA()\n",
      "89/42:\n",
      "\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "89/43:\n",
      "\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "89/44:\n",
      "\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:2]))\n",
      "print(sum(variance_ratio[0:3]))\n",
      "89/45:\n",
      "\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "89/46:\n",
      "\n",
      "k = 3\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "sorted = list(np.argsort(pc2))\n",
      "print('----------PC2-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "89/47:\n",
      "\n",
      "x = X_pca[:,0]\n",
      "y = X_pca[:,1]\n",
      "89/48:\n",
      "\n",
      "plt.figure(figsize=(30, 20))\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "for i, txt in enumerate(labels):\n",
      "    plt.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "89/49:\n",
      "\n",
      "def myplot(score,coeff,labels=None):\n",
      "    plt.figure(figsize=(15,10))\n",
      "    plt.xlabel(\"PC{}\".format(1))\n",
      "    plt.ylabel(\"PC{}\".format(2))\n",
      "    xs = score[:,0]\n",
      "    ys = score[:,1]\n",
      "    n = coeff.shape[0]\n",
      "    scalex = 1.0/(xs.max() - xs.min())\n",
      "    scaley = 1.0/(ys.max() - ys.min())\n",
      "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
      "    for i in range(n):\n",
      "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
      "        if labels is None:\n",
      "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
      "        else:\n",
      "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
      "\n",
      "print(pca.components_[0:2, :])\n",
      "myplot(X_pca[:,0:2],np.transpose(pca.components_[0:2, :]),attributes)\n",
      "axes = plt.gca()\n",
      "axes.set_ylim([-1.0,1.0])\n",
      "plt.show()\n",
      "90/1:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "90/2:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "90/3:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "90/4:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "90/5:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "90/6:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "90/7:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "90/8:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    \n",
      "    #TODO\n",
      "90/9:\n",
      "\n",
      "plt.scatter(#TODO)\n",
      "plt.show()\n",
      "91/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "91/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "91/3:\n",
      "\n",
      "train\n",
      "91/4:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "91/5:\n",
      "\n",
      "train = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train.info()\n",
      "91/6:\n",
      "\n",
      "train.head()\n",
      "91/7:\n",
      "\n",
      "train.describe(include='all')\n",
      "91/8:\n",
      "\n",
      "# Missing values \n",
      "train.isna().sum()\n",
      "91/9:\n",
      "\n",
      "train\n",
      "91/10:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/11:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/12:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/13:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "91/14:\n",
      "\n",
      "train[\"budget\"]\n",
      "91/15:\n",
      "\n",
      "train[\"budget\"]\n",
      "cpi(train[\"budget\"], train[\"release_date\"])\n",
      "91/16:\n",
      "\n",
      "for movie in train :\n",
      "    cpi(movie[\"budget\"], movie[\"release_date\"])\n",
      "91/17:\n",
      "\n",
      "for movie in train :\n",
      "    cpi(movie[\"budget\"], movie[\"release_date\"])\n",
      "91/18:\n",
      "\n",
      "for movie in train :\n",
      "    print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/19:\n",
      "\n",
      "for movie in train :\n",
      "    movie\n",
      "    # print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/20:\n",
      "\n",
      "for movie in train :\n",
      "    print(movie)\n",
      "    # print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/21:\n",
      "\n",
      "for movie in train.rows :\n",
      "    print(movie)\n",
      "    # print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/22:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    print(movie)\n",
      "    # print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/23:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/24:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[\"budget\"])\n",
      "91/25:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[0])\n",
      "91/26:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/27:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/28:\n",
      "\n",
      "for movie in train.columns() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/29:\n",
      "\n",
      "for movie in train.iteritems :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/30:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/31:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie)\n",
      "91/32:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[\"budget\"])\n",
      "91/33:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/34:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/35:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/36:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[0])\n",
      "91/37:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/38:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/39:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie)\n",
      "91/40:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[0])\n",
      "91/41:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[3])\n",
      "91/42:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[\"budget\"])\n",
      "91/43:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/44:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[3])\n",
      "91/45:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[4])\n",
      "91/46:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[5])\n",
      "91/47:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[6])\n",
      "91/48:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie)\n",
      "91/49:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[\"budget\"])\n",
      "91/50:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/51:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))\n",
      "91/52:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(movie[\"release_date\"].year)\n",
      "91/53:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(movie[\"release_date\"].year())\n",
      "91/54:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/55:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "from datetime import date\n",
      "91/56:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(date.strftime(movie[\"release_date\"], \"%m/%d/%Y\") )\n",
      "91/57:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%Y\") )\n",
      "91/58:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\") )\n",
      "91/59:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year )\n",
      "91/60:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    print(cpi.inflate(movie[\"budget\"], year))\n",
      "91/61:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    print(cpi.inflate(movie[\"budget\"], year).to_int)\n",
      "91/62:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/63:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    print(movie[\"release_date\"])\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/64:\n",
      "\n",
      "pd.to_datetime(train[\"release_date\"])\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/65:\n",
      "\n",
      "print(pd.to_datetime(train[\"release_date\"]))\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/66:\n",
      "\n",
      "pd.to_datetime(train[\"release_date\"])\n",
      "def fix_date(x):\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train[\"release_date\"].apply(fix_date)\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/67:\n",
      "\n",
      "def fix_date(x):\n",
      "    pd.to_datetime(x, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train[\"release_date\"].apply(fix_date)\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/68:\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train[\"release_date\"].apply(fix_date)\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/69:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    print(movie[\"release_date\"] )\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/70:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/71:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%Y-%m-%d\").year\n",
      "    print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/72:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    print(int(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year)))\n",
      "91/73:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "    \n",
      "train[\"budget\"] = train_original[\"budget\", \"release_date\"].apply(adjust_price_to_inflation)\n",
      "91/74:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/75:\n",
      "\n",
      "train_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train_original.info()\n",
      "91/76:\n",
      "\n",
      "train_original.head()\n",
      "91/77:\n",
      "\n",
      "train_original.describe(include='all')\n",
      "91/78:\n",
      "\n",
      "# Missing values \n",
      "train_original.isna().sum()\n",
      "91/79:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "    \n",
      "train[\"budget\"] = train_original[\"budget\", \"release_date\"].apply(adjust_price_to_inflation)\n",
      "91/80:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "# def adjust_price_to_inflation(price, date):\n",
      "#     return int(cpi.inflate(price, date.year))\n",
      "#     \n",
      "# train[\"budget\"] = train_original[\"budget\", \"release_date\"].apply(adjust_price_to_inflation)\n",
      "    \n",
      "print(train_original[\"budget\", \"release_date\"])\n",
      "91/81:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "# def adjust_price_to_inflation(price, date):\n",
      "#     return int(cpi.inflate(price, date.year))\n",
      "#     \n",
      "# train[\"budget\"] = train_original[\"budget\", \"release_date\"].apply(adjust_price_to_inflation)\n",
      "    \n",
      "print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/82:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train_original[[\"budget\", \"release_date\"]].apply(adjust_price_to_inflation)\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/83:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = adjust_price_to_inflation(train_original[\"budget\"], train_original[\"release_date\"])\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/84:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x['budget'], x['release_date']))\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/85:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]))\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/86:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/87:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/88:\n",
      "\n",
      "train_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train_original.info()\n",
      "91/89:\n",
      "\n",
      "train_original.head()\n",
      "91/90:\n",
      "\n",
      "train_original.describe(include='all')\n",
      "91/91:\n",
      "\n",
      "# Missing values \n",
      "train_original.isna().sum()\n",
      "91/92:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "91/93:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/94:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/95:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/96:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "91/97:\n",
      "\n",
      "print(train)\n",
      "91/98:\n",
      "\n",
      "train.head()\n",
      "91/99:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "91/100:\n",
      "\n",
      "train.head()\n",
      "91/101:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "91/102:\n",
      "\n",
      "train.head()\n",
      "91/103:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "91/104:\n",
      "\n",
      "train.head()\n",
      "91/105:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "train[\"revenue\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/106:\n",
      "\n",
      "train.head()\n",
      "91/107:\n",
      "\n",
      "train.head()\n",
      "91/108:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/109:\n",
      "\n",
      "train_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train_original.info()\n",
      "91/110:\n",
      "\n",
      "train_original.head()\n",
      "91/111:\n",
      "\n",
      "train_original.describe(include='all')\n",
      "91/112:\n",
      "\n",
      "# Missing values \n",
      "train_original.isna().sum()\n",
      "91/113:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "train[\"revenue\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/114:\n",
      "\n",
      "train.head()\n",
      "91/115:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/116:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/117:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/118:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "91/119:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/120:\n",
      "\n",
      "train_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train_original.info()\n",
      "91/121:\n",
      "\n",
      "train_original.head()\n",
      "91/122:\n",
      "\n",
      "train_original.describe(include='all')\n",
      "91/123:\n",
      "\n",
      "# Missing values \n",
      "train_original.isna().sum()\n",
      "91/124:\n",
      "\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "train[\"revenue\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/125:\n",
      "\n",
      "train.head()\n",
      "91/126:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/127:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/128:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/129:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "92/1:\n",
      "\n",
      "# loading recquired libraries \n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# evironment configuration\n",
      "np.set_printoptions(linewidth=2000)\n",
      "np.set_printoptions(threshold=2000)\n",
      "92/2:\n",
      "\n",
      "# data reading\n",
      "data_file_path = \"./tmdb-box-office-prediction/complete_movie_dataset.csv\"\n",
      "dataset = pd.read_csv(data_file_path, encoding=\"ISO-8859-1\")\n",
      "92/3:\n",
      "\n",
      "# print basic informations obout dataset\n",
      "print(\"Dataset size:\", dataset.size)\n",
      "print(\"Dataset shape:\", dataset.shape)\n",
      "print(\"\")\n",
      "print(dataset.info())\n",
      "print(\"\")\n",
      "print(\"Example records: \")\n",
      "dataset.head(2) #uncomment to see example record\n",
      "92/4:\n",
      "\n",
      "# Decription of numerical column\n",
      "dataset.describe(include = [np.number])\n",
      "92/5:\n",
      "\n",
      "# Description of objectvalue columns\n",
      "dataset.describe(include = ['O'])\n",
      "92/6:\n",
      "\n",
      "# All column description\n",
      "dataset.describe(include='all')\n",
      "92/7:\n",
      "\n",
      "# Missing values \n",
      "dataset.isna().sum()\n",
      "91/130:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/131:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/132:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/133:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/134:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/135:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/136:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/137:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/138:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/139:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/140:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/141:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/142:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv', encoding=\"ISO-8859-1\")\n",
      "dataset_original.info()\n",
      "91/143:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/144:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv', encoding=\"ISO-8859-1\")\n",
      "dataset_original.info()\n",
      "91/145:\n",
      "\n",
      "dataset_original.head()\n",
      "91/146:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "91/147:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "91/148:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/149:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/150:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/151:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "91/152:\n",
      "\n",
      "dataset_original.head()\n",
      "91/153:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "91/154:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "91/155:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/156:\n",
      "\n",
      "dataset.head()\n",
      "91/157:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/158:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/159:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/160:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "91/161:\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/162:\n",
      "\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].describe\n",
      "91/163:\n",
      "\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "dataset.describe\n",
      "91/164:\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "dataset.describe(include=\"release_date\")\n",
      "91/165:\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].describe()\n",
      "91/166:\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].describe(include=\"all\")\n",
      "91/167:\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].info()\n",
      "91/168:\n",
      "\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].min()\n",
      "dataset[\"release_date\"].max()\n",
      "91/169:\n",
      "\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "print(dataset[\"release_date\"].min())\n",
      "print(dataset[\"release_date\"].max())\n",
      "91/170:\n",
      "\n",
      "#Określenie cech zbiorów wartości (np. w kontekście dat minimalna, maksymalna wartość, w kontekście słów kluczowych, czy pochodzą ze zbioru predefiniowanego, itd.).\n",
      "print(\"Oldest: \",dataset[\"release_date\"].min())\n",
      "print(\"Newest: \",dataset[\"release_date\"].max())\n",
      "91/171:\n",
      "dataset.groupby(dataset[\"release_date\"].year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/172:\n",
      "dataset.groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/173:\n",
      "dataset.groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/174:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/175:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "91/176:\n",
      "\n",
      "dataset_original.head()\n",
      "91/177:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "91/178:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "91/179:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/180:\n",
      "\n",
      "dataset.head()\n",
      "91/181:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/182:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/183:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/184:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "91/185:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "91/186:\n",
      "dataset.groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/187:\n",
      "dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/188:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped)\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/189:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.distplot(grouped)\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/190:\n",
      "dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.distplot(grouped)\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/191:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "fig, ax = plt.subplots(1,1)\n",
      "\n",
      "# pass ax here\n",
      "sns.distplot(grouped, ax=ax)\n",
      "plt.show()\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/192:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped.transpose())\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/193:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped.transpose())\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/194:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/195:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped[\"release_date\"]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/196:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped[0]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/197:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.info()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/198:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.describe()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/199:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/200:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.hist()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/201:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/202:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/203:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.first()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/204:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/205:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[0]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/206:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[1]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/207:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[3]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/208:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[10]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/209:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[200]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/210:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[100]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/211:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[50]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/212:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.columns\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/213:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.columns()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/214:\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.columns()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/215:\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/216:\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.distplot()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/217:\n",
      "\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/218:\n",
      "\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/219:\n",
      "\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/220:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/221:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=dataset, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/222:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/223:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.release_date\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/224:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped[\"release_date\"]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/225:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/226:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.info()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/227:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.describe()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/228:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.columns()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/229:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.head(10)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/230:\n",
      "grouped = dataset[\"release_date\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.head(10)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/231:\n",
      "grouped = dataset[\"release_date\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/232:\n",
      "grouped = dataset[\"release_date\"].groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/233:\n",
      "grouped = dataset[\"release_date\"].groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/234:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/235:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/236:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/237:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/238:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/239:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/240:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/241:\n",
      "grouped = dataset.groupby(\"release_date\".dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/242:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/243:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/244:\n",
      ".count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/245:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False)\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/246:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).groups\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/247:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).sum()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/248:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(\"count\")\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/249:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False)\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/250:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({\"count\": \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/251:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({\"n\": \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/252:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({\"n\": \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/253:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({count: \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/254:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({id: \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/255:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({number: \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/256:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(number: \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/257:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(count: \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/258:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(count= \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/259:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(total= \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/260:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg((\"id\", \"count\"))\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/261:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg((\"release_date\", \"count\"))\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/262:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(total= \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/263:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False)\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/264:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).groups\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/265:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/266:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(\"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/267:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year).agg(\"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/268:\n",
      "grouped = dataset[[\"id\"]].groupby(dataset[\"release_date\"].dt.year).agg(\"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/269:\n",
      "grouped = dataset[[\"id\"]].groupby(dataset[\"release_date\"].dt.year).agg(id=\"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/270:\n",
      "grouped = dataset[[\"id\"]].groupby(dataset[\"release_date\"].dt.year).agg({\"count\":\"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/271:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg({\"count\":\"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/272:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg({\"total\":\"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/273:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year)[\"anount\"]\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/274:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year)[\"release_date\"]\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/275:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"]\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/276:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].groups\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/277:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/278:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/279:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/280:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/281:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/282:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[[\"release_date\", \"id\"]].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/283:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[[\"release_date\", \"id\"]].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/284:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[[\"release_date\", \"id\"]].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/285:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/286:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/287:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/288:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).value_counts()\n",
      "\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/289:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/290:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/291:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/292:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/293:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/294:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).agg(['sum', 'mean', 'max'])\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/295:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).agg(['count', 'mean', 'max'])\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/296:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/297:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/298:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.index\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/299:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.add_suffix('_Count').reset_index()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/300:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.reset_index()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/301:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "\n",
      "sns.distplot(grouped.reset_index())\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/302:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/303:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.id)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/304:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/305:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.release_date, bins=grouped.id)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/306:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.id, bins=grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/307:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/308:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/309:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={'id':'count'})\n",
      "\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/310:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":'count'})\n",
      "\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/311:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":'count'})\n",
      "\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/312:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/313:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "91/314:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "91/315:\n",
      "\n",
      "sns.jointplot(x=\"category\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/1:\n",
      "\n",
      "sns.jointplot(x=\"category\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/4:\n",
      "\n",
      "dataset_original.head()\n",
      "93/5:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/6:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "93/7:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/8:\n",
      "\n",
      "dataset.head()\n",
      "93/9:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/10:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/11:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/12:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/13:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/14:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/15:\n",
      "\n",
      "sns.jointplot(x=\"category\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/316: cpi.update()\n",
      "93/16:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/17:\n",
      "\n",
      "cpi.update()\n",
      "91/317:\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "94/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/ed'])\n",
      "94/2: import cpi\n",
      "94/3: cpi\n",
      "94/4: cpi.update()\n",
      "93/18:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/19:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/20:\n",
      "\n",
      "dataset_original.head()\n",
      "93/21:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/22:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "93/23:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/24:\n",
      "\n",
      "dataset.head()\n",
      "93/25:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/26:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/27:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/28:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/29:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/30:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/31:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/32:\n",
      "sns.catplot(x='release_date_weekday', y='revenue', data=dataset)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "93/33:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "\n",
      "sns.catplot(x='release_date', y='revenue', data=dataset)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "93/34:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "\n",
      "sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "91/318:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"id\"].reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/319:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"]\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/320:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/321:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].groups\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/322:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].avg().reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/323:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].agg(\"avg\").reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/324:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].mean().reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/325:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].mean().reset_index().rename(columns=\"release_date\", \"weekday\")\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/326:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].mean().reset_index().rename(columns={\"release_date\", \"weekday\"})\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/327:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].mean().reset_index().rename(columns={\"release_date\": \"weekday\"})\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/328:\n",
      "dataset['weekday'] = dataset[['release_date']].apply(lambda x: dt.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/329:\n",
      "dataset['weekday'] = dataset[['release_date']].apply(lambda x: dataset.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/330:\n",
      "dataset['weekday'] = dataset[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/331:\n",
      "temp = dataset[(dataset.revenue != 0).any()]\n",
      "temp\n",
      "# dataset['weekday'] = dataset[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/332:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "\n",
      "temp\n",
      "# dataset['weekday'] = dataset[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/333:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset[\"weekday\"] = temp[\"release_date\"].apply(lambda x: datetime.datetime.strftime(x[\"release_date\"], \"%A\"), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/334:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset[\"weekday\"] = temp[\"release_date\"].apply(lambda x: datetime.datetime.strftime(x[\"release_date\"], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/335:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/336:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/337:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "91/338:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped, height=11)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "91/339:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "91/340:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.histplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Mean revenue on different days of week of release')\n",
      "91/341:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.plot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Mean revenue on different days of week of release')\n",
      "91/342:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.distplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Mean revenue on different days of week of release')\n",
      "91/343:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Mean revenue on different days of week of release')\n",
      "91/344:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "91/345:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=dataset, height=8)\n",
      "91/346:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=dataset, height=8, kind = \"box\")\n",
      "91/347:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=dataset, height=8, kind = \"boxen\")\n",
      "91/348:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=dataset, height=8)\n",
      "91/349:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "91/350:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "91/351:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "91/352:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "91/353:\n",
      "temp = dataset[dataset.revenue != 0 && dataset.revenue != 1]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "91/354:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp = temp[temp.revenue != 1]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "91/355:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "93/35:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/36:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/37:\n",
      "\n",
      "dataset_original.head()\n",
      "93/38:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/39:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "93/40:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/41:\n",
      "\n",
      "dataset.head()\n",
      "93/42:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/43:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/44:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/45:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/46:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/47:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/48:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "93/49:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/50:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/51:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/52:\n",
      "\n",
      "dataset_original.head()\n",
      "93/53:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/54:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "93/55:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/56:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "93/57:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/58:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/59:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/60:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/61:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/62:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/63:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "93/64:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/65:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/66:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/67:\n",
      "\n",
      "dataset_original.head()\n",
      "93/68:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/69:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "print(dataset_original.revenue.min())\n",
      "93/70:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/71:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "93/72:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/73:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/74:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/75:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/76:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/77:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/78:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "93/79:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/1:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "print(dataset_original.revenue.min())\n",
      "96/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "96/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "96/4:\n",
      "\n",
      "dataset_original.head()\n",
      "96/5:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "96/6:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "print(dataset_original.revenue.min())\n",
      "96/7:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "96/8:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "96/9:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/10:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/11:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/12:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "96/13:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "96/14:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "96/15:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "96/16:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/17:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "91/356:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "# \n",
      "dataset[dataset.revenue == 1]\n",
      "96/18:\n",
      "\n",
      "genres = dataset['genres'].apply(lambda x: [i['genres'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in lang for i in j]).most_common(5)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/19:\n",
      "\n",
      "genre=df_train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/20:\n",
      "\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/21:\n",
      "\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/22:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "96/23:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "96/24:\n",
      "\n",
      "dataset_original.head()\n",
      "96/25:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "96/26:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "96/27:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "96/28:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "96/29:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/30:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/31:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/32:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "96/33:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "96/34:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "96/35:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "96/36:\n",
      "\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/37:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/38:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/39:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/40:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/41:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/357:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by day\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/358:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/359:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(train['release_year'].sort_values())\n",
      "plt.title(\"Movie Release count by Year\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "91/360:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie Release count by Year\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "91/361:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "plt.show()\n",
      "91/362:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "91/363:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "91/364:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "91/365:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "\n",
      "train.groupby('release_month').agg('mean')['revenue'].plot(kind='bar',color='navy',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "91/366:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "\n",
      "dataset.groupby('release_month').agg('mean')['revenue'].plot(kind='bar',color='navy',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "91/367:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='navy',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "91/368:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "91/369:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "91/370:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(train['release_month'].sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/371:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_month'].sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/372:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/373:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/374:\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "91/375:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "        \n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/42:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "96/43:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "96/44:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "96/45:\n",
      "\n",
      "dataset_original.head()\n",
      "96/46:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "96/47:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "96/48:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "96/49:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "96/50:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/51:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/52:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/53:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "96/54:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "96/55:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "96/56:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "96/57:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/58:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/59:\n",
      "\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "96/60:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "        \n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/61:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "96/62:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/63:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "        \n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/64:\n",
      "\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/65:\n",
      "\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "96/66:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "96/67:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "96/68:\n",
      "\n",
      "dataset_original.head()\n",
      "96/69:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "96/70:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "96/71:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "96/72:\n",
      "\n",
      "dataset.head()\n",
      "96/73:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/74:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/75:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/76:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "96/77:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "96/78:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "96/79:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "96/80:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/81:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/82:\n",
      "\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "96/83:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "96/84:\n",
      "\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "96/85:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "96/86:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/376:\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count], height=11)\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "91/377:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "91/378:\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count], height=11)\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "91/379:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "91/380:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "# sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "# plt.title(\"Movies by category\")\n",
      "# plt.show()\n",
      "count\n",
      "91/381:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "# sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "# plt.title(\"Movies by category\")\n",
      "# plt.show()\n",
      "genre\n",
      "91/382:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "91/383:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "91/384:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(15,10))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "91/385:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(15,10))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "96/87:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "96/88:\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "97/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "98/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "98/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "99/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "99/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "100/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "100/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "101/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "101/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "102/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "102/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "103/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "103/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "104/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "104/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "105/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "105/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "106/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "106/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "107/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "107/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "108/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "108/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "109/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "109/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "110/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "110/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "111/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "111/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "112/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "112/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "113/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "113/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "114/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "114/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "115/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "115/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "116/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "116/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "117/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "117/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "118/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "118/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "119/1: import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "120/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "120/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "121/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "121/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "122/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "122/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "123/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "123/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "124/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "124/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "125/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "125/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "126/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "126/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "127/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "127/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "128/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "128/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "129/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "130/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "130/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "131/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "131/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "132/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "132/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "133/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "133/2:\n",
      "def test(x):\n",
      "    return x*5\n",
      "133/3: x = 2\n",
      "133/4: test(x)\n",
      "133/5: x\n",
      "134/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "135/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "135/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "136/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "136/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "137/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "137/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "138/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "138/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "139/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "139/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "140/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "140/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "141/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "141/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "142/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "142/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "143/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "143/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "144/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "144/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "145/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "145/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "146/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "146/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "147/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "147/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "148/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "148/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "149/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "149/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "150/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "150/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "151/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "151/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "152/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "152/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "153/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "153/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "154/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "154/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "155/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "155/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "156/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "156/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "157/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "157/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "158/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "158/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "159/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "159/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "160/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "160/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "161/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "161/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "162/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "162/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "163/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "163/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "164/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "164/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "165/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "165/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "166/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "166/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "167/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "167/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "168/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "168/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "169/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "169/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "170/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "170/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "171/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "171/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "172/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "172/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "173/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "173/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "174/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "174/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "175/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "175/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "176/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "176/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "177/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "177/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "178/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "178/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "179/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "179/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "180/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "180/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "181/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "181/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "182/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "182/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "183/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "183/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "184/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "184/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "185/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "185/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "186/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "186/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "187/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "187/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "188/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "188/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "189/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "189/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "190/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "190/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "191/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "191/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "192/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "192/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "193/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "193/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "194/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "194/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "195/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "195/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "196/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "196/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "197/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "197/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "198/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "198/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "199/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "199/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "200/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "200/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "201/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "201/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "202/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "202/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "203/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "204/1:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/2:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "204/3:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "204/4:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "204/5:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "204/6:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "204/7:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "204/8:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    \n",
      "    #TODO\n",
      "204/9:\n",
      "\n",
      "plt.scatter(#TODO)\n",
      "plt.show()\n",
      "204/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/11:\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "204/12:\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "204/13:\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "204/14:\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "204/15:\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "204/16:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/17:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "204/18:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "204/19:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.show()\n",
      "204/20:\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "204/21:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/22:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = np.array(training_X).reshape(-1,1)\n",
      "training_y = np.array(training_y).reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/23:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = np.array(training_X).reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/24:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/25:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/26:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred)\n",
      "\n",
      "plt.show()\n",
      "204/27:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred, color=\"red\")\n",
      "\n",
      "plt.show()\n",
      "204/28:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred, color=\"green\")\n",
      "\n",
      "plt.show()\n",
      "204/29:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(n)\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/30:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict([n])\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/31:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict([n])\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/32:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1, 1))\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/33:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1, 1))[0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/34:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1, 1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/35:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1, -1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/36:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/37:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/38:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/39:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200)\n",
      "204/40:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(1000)\n",
      "204/41:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(1000)\n",
      "204/42:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(1000)\n",
      "204/43:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(2000)\n",
      "204/44:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(2000)\n",
      "204/45:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(4000)\n",
      "204/46:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(4000)\n",
      "204/47:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/48:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/49:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(10)\n",
      "204/50:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(1)\n",
      "204/51:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/52:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred, color=\"green\")\n",
      "\n",
      "plt.show()\n",
      "204/53:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/54:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/55:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/56:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/57:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/58:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/59:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/60:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/61:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/62:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/63:\n",
      "\n",
      "coef = regr.coef_[0][0]\n",
      "intercept = regr.intercept_[0]\n",
      "\n",
      "print('Coefficient: \\n', coef)\n",
      "print('Intercept: \\n', intercept)\n",
      "204/64:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "204/65:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "204/66:\n",
      "\n",
      "print(\"Test 1: \",run_series_cpu(4032003),\" Predicted:\", run_series_cpu_predicted_time(4032003),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(4032003))\n",
      "print(\"Test 2: \",run_series_cpu(54300),\" Predicted:\", run_series_cpu_predicted_time(54300),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(54300))\n",
      "print(\"Test 3: \",run_series_cpu(765403),\" Predicted:\", run_series_cpu_predicted_time(765403),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(765403))\n",
      "print(\"Test 4: \",run_series_cpu(84673),\" Predicted:\", run_series_cpu_predicted_time(84673),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(84673))\n",
      "print(\"Test 5: \",run_series_cpu(5436574),\" Predicted:\", run_series_cpu_predicted_time(5436574),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(5436574))\n",
      "204/67:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/68:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "204/69:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "204/70:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "204/71:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "204/72:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "204/73:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "204/74:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "204/75:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "204/76:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/77:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/78:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "204/79:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "204/80:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "204/81:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "204/82:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "204/83:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "204/84:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "204/85:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "204/86:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/1:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "205/2:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "205/3:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "205/4:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "205/5:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "205/6:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "205/7:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "205/8:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "205/9:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "205/10:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/11:\n",
      "\n",
      "def predicted_time(n):\n",
      " return regr.intercept_[0][0] + regr.coef_[0] * n\n",
      "205/12:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "205/13:\n",
      "\n",
      "print(\"Test 1: \",run_series_cpu(4032003),\" Predicted:\", run_series_cpu_predicted_time(4032003),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(4032003))\n",
      "print(\"Test 2: \",run_series_cpu(54300),\" Predicted:\", run_series_cpu_predicted_time(54300),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(54300))\n",
      "print(\"Test 3: \",run_series_cpu(765403),\" Predicted:\", run_series_cpu_predicted_time(765403),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(765403))\n",
      "print(\"Test 4: \",run_series_cpu(84673),\" Predicted:\", run_series_cpu_predicted_time(84673),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(84673))\n",
      "print(\"Test 5: \",run_series_cpu(5436574),\" Predicted:\", run_series_cpu_predicted_time(5436574),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(5436574))\n",
      "205/14:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "205/15:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "205/16:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "205/17:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "205/18:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "205/19:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "205/20:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations=10):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "205/21:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "205/22:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "205/23:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/24:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "205/25:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "205/26:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "205/27:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "205/28:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "205/29:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "205/30:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations=10):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "205/31:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N))\n",
      "205/32:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "205/33:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/34:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = np.array(training_X).reshape(-1,1)\n",
      "training_y = np.array(training_y).reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/35:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "205/36:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "205/37:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "205/38:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "205/39:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "205/40:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "205/41:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations=10):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "205/42:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N))\n",
      "205/43:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "205/44:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = np.array(training_X).reshape(-1,1)\n",
      "training_y = np.array(training_y).reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/45:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred, color=\"green\")\n",
      "\n",
      "plt.show()\n",
      "205/46:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "205/47:\n",
      "\n",
      "coef = regr.coef_[0][0]\n",
      "intercept = regr.intercept_[0]\n",
      "\n",
      "print('Coefficient: \\n', coef)\n",
      "print('Intercept: \\n', intercept)\n",
      "205/48:\n",
      "\n",
      "def predicted_time(n):\n",
      " return regr.intercept_[0][0] + regr.coef_[0] * n\n",
      "205/49:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "205/50:\n",
      "\n",
      "print(\"Test 1: \",run_series_cpu(4032003),\" Predicted:\", run_series_cpu_predicted_time(4032003),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(4032003))\n",
      "print(\"Test 2: \",run_series_cpu(54300),\" Predicted:\", run_series_cpu_predicted_time(54300),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(54300))\n",
      "print(\"Test 3: \",run_series_cpu(765403),\" Predicted:\", run_series_cpu_predicted_time(765403),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(765403))\n",
      "print(\"Test 4: \",run_series_cpu(84673),\" Predicted:\", run_series_cpu_predicted_time(84673),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(84673))\n",
      "print(\"Test 5: \",run_series_cpu(5436574),\" Predicted:\", run_series_cpu_predicted_time(5436574),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(5436574))\n",
      "205/51:\n",
      "\n",
      "def predicted_time(n):\n",
      " return regr.intercept_[0] + regr.coef_[0][0] * n\n",
      "205/52:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "205/53:\n",
      "\n",
      "print(\"Test 1: \",run_series_cpu(4032003),\" Predicted:\", run_series_cpu_predicted_time(4032003),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(4032003))\n",
      "print(\"Test 2: \",run_series_cpu(54300),\" Predicted:\", run_series_cpu_predicted_time(54300),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(54300))\n",
      "print(\"Test 3: \",run_series_cpu(765403),\" Predicted:\", run_series_cpu_predicted_time(765403),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(765403))\n",
      "print(\"Test 4: \",run_series_cpu(84673),\" Predicted:\", run_series_cpu_predicted_time(84673),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(84673))\n",
      "print(\"Test 5: \",run_series_cpu(5436574),\" Predicted:\", run_series_cpu_predicted_time(5436574),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(5436574))\n",
      "205/54:\n",
      "\n",
      "# Podsumowanie\n",
      "\n",
      "* Predykcja na podstawie modelu `LinearRegression` polega na obliczeniu wartości funkcji liniowej regresji \n",
      "* Oszacowane czasy wykonywania obliczeń są bliskie rzeczywistym czasom wykonania\n",
      "205/55:\n",
      "# Podsumowanie\n",
      "\n",
      "* Predykcja na podstawie modelu `LinearRegression` polega na obliczeniu wartości funkcji liniowej regresji \n",
      "* Oszacowane czasy wykonywania obliczeń są bliskie rzeczywistym czasom wykonania\n",
      "206/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "207/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "207/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "208/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "208/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "209/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "209/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "210/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "210/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "211/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "211/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "212/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "212/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "213/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "213/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "214/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "214/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "215/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "215/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "216/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "216/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "217/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "217/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "218/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "218/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "206/2: (.8 * (1 - .8))\n",
      "219/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "219/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "220/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "221/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "221/2:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "221/3:\n",
      "\n",
      "dataset_original.head()\n",
      "221/4:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "221/5:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "221/6:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "221/7:\n",
      "\n",
      "dataset.head()\n",
      "221/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "221/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "221/10:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "221/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "221/12:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "221/13:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "221/14:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "221/15:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "221/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "221/17:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "221/18:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "221/19:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "221/20:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "221/21:\n",
      "\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "221/22:\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "221/23:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "222/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "222/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "222/3: dataset_original.head()\n",
      "222/4: dataset_original.describe(include='all')\n",
      "222/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "222/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "222/7: dataset.head()\n",
      "222/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "222/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "222/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "222/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "222/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "222/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "222/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "222/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "222/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "222/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "222/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "222/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "222/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "222/21:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "222/22:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "223/1:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "224/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "224/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "224/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "224/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "224/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "224/6:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"2. Revenue by release weekday\")\n",
      "plt.show()\n",
      "224/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "224/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "224/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "224/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "224/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "224/12:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "224/13:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "224/14:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "225/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "225/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "225/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "225/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "225/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "225/6:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"2. Revenue by release weekday\")\n",
      "plt.show()\n",
      "225/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "225/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "225/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "225/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "225/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "225/12:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "225/13:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "225/14:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "225/15:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "225/16:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "226/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "226/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "226/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "226/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "226/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "226/6:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"2. Revenue by release weekday\")\n",
      "plt.show()\n",
      "226/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "226/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "226/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "226/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "226/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "226/12:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "226/13:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "226/14:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "226/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movies releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "226/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movies releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "226/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movies releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "226/18:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movies releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "226/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "226/20:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "226/21:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "226/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "226/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Revenue by month\")\n",
      "plt.show()\n",
      "226/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "226/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "226/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "226/27:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Movies by category\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "226/28:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "226/29: dataset[\"TV\" in dataset['genre'].str]\n",
      "226/30: dataset[\"TV\" in dataset['genres'].str]\n",
      "226/31: dataset['genres'].str\n",
      "226/32: dataset['genres']\n",
      "226/33: dataset['genres'].name\n",
      "226/34: dataset['genres']['name']\n",
      "226/35: dataset[\"TV\" in dataset['genres']]\n",
      "226/36: \"TV\" in dataset['genres']\n",
      "226/37: dataset['genres']\n",
      "226/38: dataset['genres'].str.contains(\"TV\")\n",
      "226/39: dataset[dataset['genres'].str.contains(\"TV\")]\n",
      "226/40: dataset[dataset['genres'].str.contains(\"TV\", na=False)]\n",
      "226/41: Najwięcej wydanych zostało dramatów a najmniej westernów\n",
      "226/42: dataset[dataset[\"genres\"].str.contains(\"Family\", no=False)]\n",
      "226/43: dataset[dataset[\"genres\"].str.contains(\"Family\", na=False)]\n",
      "226/44: dataset[dataset[\"genres\"].str.contains(\"family\", na=False)]\n",
      "226/45: dataset[dataset[\"genres\"].str.contains(\".*Family.*\", na=False)]\n",
      "226/46: dataset[dataset[\"genres\"].str.contains(\"Family\", regex=False, na=False)]\n",
      "226/47: dataset[dataset[\"genres\"].str.contains(\"Family\", regex=False, na=False)]\n",
      "226/48: dataset[dataset[\"genres\"].str.contains(\"Family\", regex=False, na=False)]\n",
      "226/49: genres\n",
      "226/50: genres[\"Family\"]\n",
      "226/51: genres\n",
      "226/52: genres\n",
      "227/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "227/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "227/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "227/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "227/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "227/11:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "227/12: genres\n",
      "227/13:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 50\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"5. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "222/23:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "dataset_original.info()\n",
      "222/24:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "222/25: oscars.head()\n",
      "222/26: oscars.head(50)\n",
      "222/27: oscars[oscars[\"winner\"] ==True]\n",
      "222/28: oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "222/29: oscars_winners = oscars[oscars[\"winner\"] == True and !oscars[\"film\"].isnan()]\n",
      "222/30: oscars_winners = oscars[oscars[\"winner\"] == True and not oscars[\"film\"].isnan()]\n",
      "222/31: oscars_winners = oscars[oscars[\"winner\"] == True and not oscars[\"film\"] != NaN]\n",
      "222/32: oscars_winners = oscars[oscars[\"winner\"] == True and oscars[\"film\"].str != \"NaN\"]\n",
      "222/33: oscars_winners = oscars[oscars[\"winner\"] == True and not oscars[\"film\"].str.match(\"NaN\")]\n",
      "222/34: oscars_winners = oscars[oscars[\"winner\"] == True and not oscars[\"film\"].str.match(\"NaN\")]\n",
      "222/35:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "oscars_winners = oscars_winners[~oscars_winners[\"film\"].str.match(\"NaN\")]\n",
      "222/36:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "oscars_winners = oscars_winners[\"film\"].notnull()\n",
      "222/37:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "222/38:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "222/39:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "222/40:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "222/41:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "222/42:\n",
      "\n",
      "merged_inner = pd.merge(left=dataset, right=oscars_winners, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "222/43:\n",
      "\n",
      "df_dataset_lower['title'] = dataset['title'].str.lower()\n",
      "df_oscars_lower['film'] = oscars_winners['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=df_dataset_lower, right=df_oscars_lower, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "222/44:\n",
      "\n",
      "dataset['title'] = dataset['title'].str.lower()\n",
      "oscars_winners['film'] = oscars_winners['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=df_dataset_lower, right=df_oscars_lower, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "222/45:\n",
      "\n",
      "dataset['title'] = dataset['title'].str.lower()\n",
      "oscars_winners['film'] = oscars_winners['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset, right=oscars_winners, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "222/46:\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "227/14:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "227/15:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "227/16:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "227/17:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/18:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/19:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/20:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/21:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/22:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "227/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "227/24:\n",
      "\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "227/25:\n",
      "\n",
      "genres\n",
      "227/26:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "227/27: oscars.head(10)\n",
      "227/28:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "oscar_movies\n",
      "227/29:\n",
      "\n",
      "dataset = oscar_movies\n",
      "227/30:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/31:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/32:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/33:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/34:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/35:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "227/36:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/37:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/38:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/39: Najwięcej filmów oscarowych wydanych zostało w grudniu a najmniej w styczniu, lutym i lipcu\n",
      "227/40:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/41:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates()\n",
      "oscar_movies\n",
      "227/42:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "oscar_movies.drop_duplicates()\n",
      "oscar_movies\n",
      "227/43:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"imdb_id\")\n",
      "oscar_movies\n",
      "227/44:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies\n",
      "227/45:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\").reset)index()\n",
      "oscar_movies\n",
      "227/46:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\").reset_index()\n",
      "oscar_movies\n",
      "227/47:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies\n",
      "227/48:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/49: dataset = oscar_movies\n",
      "227/50: dataset = oscar_movies\n",
      "227/51:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/52:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/53:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/54:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/55:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/56:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"12. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/57:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "227/58:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies.drop(columns=[\"film\"])\n",
      "oscar_movies\n",
      "227/59:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies.drop(columns=[\"film\"])\n",
      "oscar_movies\n",
      "227/60:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "oscar_movies\n",
      "227/61:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "oscar_movies\n",
      "227/62:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "# dataset_lc = dataset.copy()\n",
      "# oscars_winners_lc = oscars_winners.copy()\n",
      "# dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "# oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "# oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "# oscar_movies\n",
      "227/63:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscars_winners_lc\n",
      "# oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "# oscar_movies\n",
      "227/64:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "oscar_movies\n",
      "227/65:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "dataset_lc\n",
      "# oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "# oscar_movies\n",
      "228/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "228/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "228/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "228/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "228/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "228/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "228/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "228/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "228/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "228/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "228/11:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "228/12: genres\n",
      "228/13:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "228/14: oscars.head(10)\n",
      "228/15:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "oscar_movies\n",
      "228/16:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies\n",
      "228/17:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(\"film\")\n",
      "228/18:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\"])\n",
      "228/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\"])\n",
      "oscar_movies\n",
      "228/20: dataset = oscar_movies\n",
      "228/21:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "228/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "228/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "228/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "228/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"12. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "228/26:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "229/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "229/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "229/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "229/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "229/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "229/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "229/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "229/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "229/11:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "229/12: genres\n",
      "229/13:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "229/14: oscars.head(10)\n",
      "229/15:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\"])\n",
      "oscar_movies\n",
      "229/16: dataset = oscar_movies\n",
      "229/17:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "229/18:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "229/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "229/20:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "229/21:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"12. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "229/22:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/23:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "229/24:\n",
      "import ast\n",
      "dict_columns = ['genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/25:\n",
      "import ast\n",
      "dict_columns = ['genres']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/26:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"]).replace({{}: np.NaN})\n",
      "oscar_movies\n",
      "229/27:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"]).replace({'{}': np.NaN})\n",
      "oscar_movies\n",
      "229/28:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "229/29:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) or x == {} else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/30:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if x == {} or pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/31:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "dataset_lc\n",
      "# oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "# oscar_movies\n",
      "229/32:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "230/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "230/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "230/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "230/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "230/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "230/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "230/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "230/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "230/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "temp = dataset.copy()\n",
      "temp = text_to_dict(temp)\n",
      "230/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "230/11:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "231/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "231/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "231/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "231/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "231/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "231/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "231/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "231/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "231/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "231/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "231/12:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/13: genres\n",
      "231/14:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "231/15: oscars.head(10)\n",
      "231/16:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "231/17: dataset = oscar_movies\n",
      "231/18:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "231/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "231/20:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "231/21:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "231/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"12. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "231/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "231/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "231/25:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/26:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/27: genres\n",
      "231/28: genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "231/29:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/30:\n",
      "genres=dataset.loc[dataset['genres'].str.len()][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/31: dataset[\"genres\"]\n",
      "231/32:\n",
      "genres=dataset.loc[dataset['genres'].str.len() == 1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/33:\n",
      "genres=dataset.loc[['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/34:\n",
      "genres=dataset.loc[dataset][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/35: dataset['genres'].apply(pd.Series).stack()\n",
      "231/36:\n",
      "x = dataset['genres'].apply(pd.Series).stack()\n",
      "x.head(50)\n",
      "231/37:\n",
      "# x = dataset['genres'].apply(pd.Series).stack()\n",
      "# x.head(50)\n",
      "z = dataset.explode(\"genres\")\n",
      "z\n",
      "231/38:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "\n",
      "genres['genres']=dataset.explode(\"genres\").genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/39:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/40:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/41:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "genres\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/42:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "genres\n",
      "# genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "231/43:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "genres\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "231/44:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "genres\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/45:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/46:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/47:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/48:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "232/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "232/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "232/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "232/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "232/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "232/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "232/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "232/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "232/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "232/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "232/11:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "233/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "233/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "233/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "233/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "233/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "233/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "233/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "233/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "233/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "233/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "233/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "233/12:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "233/13:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres\n",
      "# genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "233/14:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"]=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "233/15:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "234/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "234/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "234/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "234/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "234/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "234/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "234/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "234/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "234/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "234/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "234/12:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/13:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/14:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/15:\n",
      "genre=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/16:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "235/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "235/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "235/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "235/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "235/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "235/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "235/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "235/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "235/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "235/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "235/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "235/12:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "235/13:\n",
      "dataset\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# # genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/14:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "235/15:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres\n",
      "# genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/16:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres\n",
      "# genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/17:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/18:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"] = genres[\"title\"]\n",
      "genres\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/19:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres.genres.name\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/20:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres.genres[\"name\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/21:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\", \"name\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/22:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"][\"name\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/23:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/24:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/25:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : x[\"name\"])\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/26:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : x.type())\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/27:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x))\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/28:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x)).distinct()\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/29:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x)).unique()\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/30:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x))\n",
      "genres[genres[\"genres\"] == \"float\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/31:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x))\n",
      "genres[genres[\"genres\"] == type(float)]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/32:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x))\n",
      "genres[genres[\"genres\"] == type(1.0)]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/33:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : isinstance(x, float))\n",
      "\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/34:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[genres[\"genres\"].apply(lambda x : isinstance(x, float))]\n",
      "\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/35:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "236/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "236/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "236/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "236/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "236/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "236/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "236/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "236/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "236/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "236/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "236/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "236/12:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "236/13: genres\n",
      "236/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"9. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "236/15:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "dataset\n",
      "# actorMovieDataset = dataset[['id','cast']].copy()\n",
      "# actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "# actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "# print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "# plt.figure(figsize=(20,12))\n",
      "# sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "# plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "# plt.xlabel(\"Actor name\")\n",
      "# plt.ylabel(\"Role number\")\n",
      "# plt.xticks(rotation=90)\n",
      "\n",
      "# plt.show()\n",
      "237/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "237/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "237/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "237/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "237/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "237/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "237/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "237/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "237/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "237/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "237/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "237/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "237/13: genres\n",
      "237/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "237/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,24))\n",
      "plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "237/16:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "237/17:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "237/18:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "237/19:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,24))\n",
      "plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "238/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "238/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "238/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "238/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "238/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "238/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "238/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "238/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "238/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "238/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "238/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "238/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "238/13: genres\n",
      "238/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "238/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,24))\n",
      "plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "238/16:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "238/17: oscars.head(10)\n",
      "238/18:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "238/19: dataset = oscar_movies\n",
      "238/20:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "238/21:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "238/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "238/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "238/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "238/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "238/26:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "238/27:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "240/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "240/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "240/3: import Bio\n",
      "240/4: pip3 install Bio\n",
      "241/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "241/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "242/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "242/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "243/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "243/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "244/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "244/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "245/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "245/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "246/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "246/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "247/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "247/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "248/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "248/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "249/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "249/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "250/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "250/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "251/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "251/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "252/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "252/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "253/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "253/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "254/1:\n",
      "\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "254/2:\n",
      "\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "254/3:\n",
      "\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "254/4:\n",
      "\n",
      "# jakość modelu\n",
      "clf.score(X_test, y_test)\n",
      "254/5:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "254/6:\n",
      "\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "254/7:\n",
      "\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "254/8:\n",
      "\n",
      "print(get_log())\n",
      "254/9:\n",
      "\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    \n",
      "    #TODO\n",
      "254/10:\n",
      "\n",
      "#TODO\n",
      "254/11:\n",
      "\n",
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(#TODO)\n",
      "254/12:\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "254/13:\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "254/14:\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "254/15:\n",
      "# jakość modelu\n",
      "clf.score(X_test, y_test)\n",
      "254/16:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "254/17:\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "254/18:\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "254/19: print(get_log())\n",
      "254/20: print(get_log())\n",
      "254/21: print(get_log())\n",
      "254/22: print(get_log())\n",
      "254/23: print(get_log())\n",
      "254/24:\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    comp_id = computers.index(triple[0])\n",
      "    server_id = sites.index(triple[1])\n",
      "    error_id = errors.index(triple[2])\n",
      "    CS_X.append((comp_id, server_id))\n",
      "    CS_y.append(error_id)\n",
      "254/25:\n",
      "print(CS_X)\n",
      "print(CS_y)\n",
      "254/26:\n",
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(CS_X, CS_y)\n",
      "254/27:\n",
      "features = [\"computers\", \"sites\"]\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf, filled=True, feature_names=features, class_names=errors)\n",
      "plt.show()\n",
      "254/28:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    feature_vector[comp_id] = 1\n",
      "    feature_vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(feature_vector)\n",
      "254/29:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    vector[comp_id] = 1\n",
      "    vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(vector)\n",
      "254/30: print(CS_X_onehot)\n",
      "254/31:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier()\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot,CS_y)\n",
      "254/32:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/33:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors, max_depth=2, min_samples_leaf=5)\n",
      "plt.show()\n",
      "254/34:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier()\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y, max_depth=2, min_samples_leaf=5)\n",
      "254/35:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/36:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/37:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=7, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/38:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/39:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier()\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/40:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/41:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=10)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/42:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/43:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/44:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/45:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/46:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/47:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/48:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/49:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "257/1:\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "257/2:\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "257/3:\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "257/4:\n",
      "# jakość modelu\n",
      "clf.score(X_test, y_test)\n",
      "257/5:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "257/6:\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "257/7:\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "257/8: print(get_log())\n",
      "257/9:\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    comp_id = computers.index(triple[0])\n",
      "    server_id = sites.index(triple[1])\n",
      "    error_id = errors.index(triple[2])\n",
      "    CS_X.append((comp_id, server_id))\n",
      "    CS_y.append(error_id)\n",
      "257/10:\n",
      "print(CS_X)\n",
      "print(CS_y)\n",
      "257/11:\n",
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(CS_X, CS_y)\n",
      "257/12:\n",
      "features = [\"computers\", \"sites\"]\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf, filled=True, feature_names=features, class_names=errors)\n",
      "plt.show()\n",
      "257/13:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    vector[comp_id] = 1\n",
      "    vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(vector)\n",
      "257/14: print(CS_X_onehot)\n",
      "257/15:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "257/16:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "258/1:\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "258/2:\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "258/3:\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "258/4:\n",
      "# jakość modelu\n",
      "clf.score(X_test, y_test)\n",
      "258/5:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "258/6:\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "258/7:\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "258/8: print(get_log())\n",
      "258/9:\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    comp_id = computers.index(triple[0])\n",
      "    server_id = sites.index(triple[1])\n",
      "    error_id = errors.index(triple[2])\n",
      "    CS_X.append((comp_id, server_id))\n",
      "    CS_y.append(error_id)\n",
      "258/10:\n",
      "print(CS_X)\n",
      "print(CS_y)\n",
      "258/11:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(CS_X, CS_y)\n",
      "258/12:\n",
      "features = [\"computers\", \"sites\"]\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf, filled=True, feature_names=features, class_names=errors)\n",
      "plt.show()\n",
      "258/13:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    vector[comp_id] = 1\n",
      "    vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(vector)\n",
      "258/14: print(CS_X_onehot)\n",
      "258/15:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "258/16:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "259/1:\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "259/2:\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "259/3:\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "259/4:\n",
      "# jakość modelu\n",
      "clf.score(X_test, y_test)\n",
      "259/5:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "259/6:\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "259/7:\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "259/8: print(get_log())\n",
      "259/9:\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    comp_id = computers.index(triple[0])\n",
      "    server_id = sites.index(triple[1])\n",
      "    error_id = errors.index(triple[2])\n",
      "    CS_X.append((comp_id, server_id))\n",
      "    CS_y.append(error_id)\n",
      "259/10:\n",
      "print(CS_X)\n",
      "print(CS_y)\n",
      "259/11:\n",
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(CS_X, CS_y)\n",
      "259/12:\n",
      "features = [\"computers\", \"sites\"]\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf, filled=True, feature_names=features, class_names=errors)\n",
      "plt.show()\n",
      "259/13:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    vector[comp_id] = 1\n",
      "    vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(vector)\n",
      "259/14: print(CS_X_onehot)\n",
      "259/15:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "259/16:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "260/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "260/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "261/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "261/2:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "261/3:\n",
      "\n",
      "dataset_original.head()\n",
      "261/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "261/5:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "261/6:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "261/7:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "261/8:\n",
      "\n",
      "dataset.head()\n",
      "261/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "261/10:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "261/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "261/12:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "261/13:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "261/14:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "261/15:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "261/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/17:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/18:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "261/19:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "261/20:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "261/21:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "261/22:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "261/23:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "261/24:\n",
      "\n",
      "oscars.head(50)\n",
      "261/25:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "261/26:\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "261/27:\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "261/28:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "\n",
      "genres['revenue']\n",
      "261/29:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "261/30:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "\n",
      "genres\n",
      "261/31:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "\n",
      "genres.groupby(genres.genres)\n",
      "261/32:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "\n",
      "genres.groupby(genres.genres).head(5)\n",
      "261/33:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "\n",
      "genres.groupby(genres.genres).head(1)\n",
      "261/34:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "261/35: grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"]))\n",
      "261/36:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"]))\n",
      "grouped\n",
      "261/37:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"])).reset_index(drop=True)\n",
      "grouped.head()\n",
      "261/38:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"]))#.reset_index(drop=True)\n",
      "grouped.head()\n",
      "261/39:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False))#.reset_index(drop=True)\n",
      "grouped.head()\n",
      "261/40:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False))#.reset_index(drop=True)\n",
      "grouped.head(10)\n",
      "261/41:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "grouped.head(10)\n",
      "261/42:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.head(10)\n",
      "genres\n",
      "261/43:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "grouped.nlargest(2)\n",
      "261/44:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(2)\n",
      "261/45:\n",
      "grouped = genres.groupby(genres.genres)#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/46:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/47:\n",
      "grouped = genres.groupby(genres.genres)#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/48:\n",
      "grouped = genres.groupby(genres.genres)[\"revenue\"]#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/49:\n",
      "grouped = genres.groupby(genres.genres)[\"revenue\"]#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10)\n",
      "261/50:\n",
      "grouped = genres.groupby(genres.genres)[\"title\", \"revenue\"]#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(2)\n",
      "261/51:\n",
      "grouped = genres.groupby(genres.genres)[\"title\", \"revenue\"]#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(2)\n",
      "261/52:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "genres.sort_values('revenue', ascending=False)\n",
      "        .groupby(genres.genres, sort=False).head(5)\n",
      "261/53:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(5)\n",
      "261/54:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "261/55: genres.sort_values('budget', ascending=False).head(10)\n",
      "261/56: genres.sort_values('popularity', ascending=False).head(10)\n",
      "261/57:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(r)\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "# loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "# plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/58:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False)#.groupby(genres.genres, sort=False).head(5)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(r)\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "# loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "# plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/59:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(x = r[\"title\"], y = r[\"revenue\"])\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "# loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "# plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/60:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "plt.figure(figsize=(20,12))\n",
      "r.plot(kind='bar',color='b',rot=0)\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "# loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "# plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/61:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "r.plot(kind='bar',color='b',rot=0)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/62:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "plot(r[\"title\"], r[\"revenue\"], kind='bar',color='b',rot=0)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/63:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "plt(r[\"title\"], r[\"revenue\"], kind='bar',color='b',rot=0)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/64:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "plt.plot(r[\"title\"], r[\"revenue\"], kind='bar',color='b',rot=0)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/65:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"],r[\"revenue\"])\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/66:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/67:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/68:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/69:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/70:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/71:\n",
      "r = genres.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/72:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/73:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/74:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/75:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "grouped.nlargest(10)\n",
      "261/76:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/77:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(10, \"revenue\")\n",
      "genres.sort_values('revenue', ascending=False)\n",
      "        .groupby(genres.genres, sort=False).head(5)\n",
      "261/78:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(10, \"revenue\")\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(5)\n",
      "261/79:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(10, \"revenue\")\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(10)\n",
      "261/80:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(10, \"revenue\")\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "261/81: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "261/82: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "261/83: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "261/84: from mlxtend.frequent_patterns import apriori\n",
      "261/85: from mlxtend.frequent_patterns import apriori\n",
      "261/86:\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "\n",
      "print frequent_itemsets\n",
      "261/87:\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/88:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/89:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "r\n",
      "261/90:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "r.map()\n",
      "261/91:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 30)\n",
      "r.map()\n",
      "261/92:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 30)\n",
      "r\n",
      "261/93:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 80)\n",
      "r\n",
      "261/94:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r\n",
      "261/95:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.map(\"name\")\n",
      "261/96:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r[\"name\"]\n",
      "261/97:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.keys\n",
      "261/98:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.values\n",
      "261/99:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r\n",
      "261/100:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: x[\"name\"], axis=1)\n",
      "261/101:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: x[\"name\"])\n",
      "261/102:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: x)\n",
      "261/103:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: x[0])\n",
      "261/104:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: type(x))\n",
      "261/105:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: len(x))\n",
      "261/106:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: map(lambda y: y[\"name\"] , x) )\n",
      "261/107:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: map(lambda y: y[\"name\"] , x))\n",
      "261/108:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: list(map(lambda y: y[\"name\"] , x))\n",
      "261/109:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: list(map(lambda y: y[\"name\"] , x)))\n",
      "261/110:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: list(x.map(lambda y: y[\"name\"])))\n",
      "261/111:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: list(map(lambda y: y[\"name\"], x)))\n",
      "261/112:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x)))\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/113:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/114:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/115:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: Series(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/116:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: pd.Series(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/117:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/118:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)[1]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/119:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)[]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/120:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/121:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r[0]\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/122:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/123:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "type(r)\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/124:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/125:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/126:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/127:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/128:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "te_ary\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/129:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "te_ary.any(True)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/130:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "np.any(te_ary)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/131:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/132:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/133:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/134:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/135:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/136:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/137:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/138:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, use_colnames=True)\n",
      "\n",
      "type(frequent_itemsets)\n",
      "261/139:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/140:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.6, use_colnames=True)\n",
      "261/141:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.1, use_colnames=True)\n",
      "261/142:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.001, use_colnames=True)\n",
      "262/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/ed'])\n",
      "263/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "263/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "263/3: dataset_original.head()\n",
      "263/4: dataset_original.describe(include='all')\n",
      "263/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "263/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "263/7: dataset.head()\n",
      "263/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "263/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "263/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "263/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "263/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "263/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "263/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "263/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "263/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "263/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "263/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "263/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "263/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "263/21:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "263/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "263/23: genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "263/24:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "263/25: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "263/26:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "263/27: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "263/28:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "263/29: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "263/30:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "263/31: oscars.head(50)\n",
      "263/32:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "263/33:\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "263/34:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.1, use_colnames=True)\n",
      "263/35:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "263/36:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.1, use_colnames=True)\n",
      "263/37:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.05, use_colnames=True)\n",
      "263/38:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/39:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.01, use_colnames=True)\n",
      "263/41:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.01, use_colnames=True)\n",
      "263/42:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.01, use_colnames=True)\n",
      "263/43:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.01, use_colnames=True)\n",
      "263/44:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/45:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(5000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/46:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/47:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/48:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/49:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/50:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/51:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/52:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/53:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Overview\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/54:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/55:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "264/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "264/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "264/3: dataset_original.head()\n",
      "264/4: dataset_original.describe(include='all')\n",
      "264/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "264/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "264/7: dataset.head()\n",
      "264/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "264/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "264/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "264/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "264/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "264/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "264/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "264/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "264/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "264/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "264/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "264/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "264/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "264/21:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "264/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "264/23: genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "264/24:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "264/25: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "264/26:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "264/27: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "264/28:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "264/29: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "264/30:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "264/31: oscars.head(50)\n",
      "264/32:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "264/33:\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "264/34:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "264/35:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "264/36:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "264/37:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "265/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "265/3: dataset_original.head()\n",
      "265/4: dataset_original.describe(include='all')\n",
      "265/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "265/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "265/7: dataset.head()\n",
      "265/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "265/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "265/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "265/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "265/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "265/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "265/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "265/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "265/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "265/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "265/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "265/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "265/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "265/21:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "265/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "265/23: genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "265/24:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "265/25: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "265/26:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "265/27: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "265/28:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "265/29: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "265/30:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "265/31: oscars.head(50)\n",
      "265/32:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "265/33:\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "265/34:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "265/35:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "265/36:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "265/37:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=2, use_colnames=True)\n",
      "265/38:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "265/39:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=2, use_colnames=True)\n",
      "265/40:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=1, use_colnames=True)\n",
      "265/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/42:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r.reset_index(drop=True).tolist()\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/43:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r.tolist()\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/44:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "map(lambda x: x.split(\" \"), r.tolist())\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/45:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/46:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=1, use_colnames=True)\n",
      "265/47:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/48:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: x !in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/49:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: x !in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/50:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: x not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/51:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: x[0] not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/52:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/53:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r\n",
      "# r = list(filter(lambda x: [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/54:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r\n",
      "# r = list(filter(lambda x: [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x], r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/55:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x], r))\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/56:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in r]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/57:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x for x in r]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/58:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x in r]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/59:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y for x in r]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/60:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for x for r for y in x]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/61:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for x in r for y in x]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/62:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y if y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for x in r for y in x]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/63:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort(\"support\")\n",
      "265/64:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/65:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/66:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/67:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/68:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x)).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/69:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/70:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "# r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x))).tolist()\n",
      "r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/71:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "# r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x))).tolist()\n",
      "r = r.split(\" \").tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/72:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "# r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x))).tolist()\n",
      "r = r.tolist().split(\" \")\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/73:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/74:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: x[\"name\"]).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/75:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: print(\"x\")).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/76:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: print(x)).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/77:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: print(x.split(\" \"))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/78:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(x not in [\"of\"], x.split(\" \"))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/79:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(x not in [\"of\"], x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/80:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y not in [\"of\"], x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/81:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y not in [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\"], x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/82:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/83:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/84:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/85:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"it's\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/86:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"it's\", \"at\", \"up\", \"they\", \"when\" \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/87:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\" \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/88:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/89:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"him\", \"after\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/90:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"him\", \"after\", \"out\", \"find\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/91:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/92:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/93:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/94:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/95:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/96:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/97:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.09, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/98:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/99:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/100:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"will\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/101:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/102:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/103:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/104:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/105:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/106:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/107:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "fp_growth(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/108:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fp_growth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/109:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/110:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "fp_growth(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/111:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "fpgrowth(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/112:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/113:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "# .sort_values('support', ascending=False)\n",
      "res[len(res[\"itemsets\"]) ==2]\n",
      "265/114: res\n",
      "265/115: res[\"itemsets\"]\n",
      "265/116: res[\"itemsets\"] == 2\n",
      "265/117: res[\"itemsets\"]\n",
      "265/118: res[\"itemsets\"].len()\n",
      "265/119: res[\"itemsets\"].length\n",
      "265/120: res[\"itemsets\"].length()\n",
      "265/121: res[\"itemsets\"].str.len()\n",
      "265/122: res[res[\"itemsets\"].str.len() == 2]\n",
      "265/123:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "# .sort_values('support', ascending=False)\n",
      "265/124:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/125:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 6].sort_values('support', ascending=False)\n",
      "265/126:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.006, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/127:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.007, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/128:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.007, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/129:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.007, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/130:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/131:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0016, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/132:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0016, max_len=10, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 10].sort_values('support', ascending=False)\n",
      "265/133:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.000016, max_len=10, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 10].sort_values('support', ascending=False)\n",
      "266/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "266/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "266/3: dataset_original.head()\n",
      "266/4: dataset_original.describe(include='all')\n",
      "266/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "266/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "266/7: dataset.head()\n",
      "266/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "266/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "266/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "266/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "266/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "266/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "266/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "266/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "266/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "266/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "266/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "266/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "266/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "266/21:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "266/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "266/23: genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "266/24:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "266/25: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "266/26:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "266/27: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "266/28:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/29: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "266/30:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "266/31: oscars.head(50)\n",
      "266/32:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "266/33:\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "266/34:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "266/35:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=3, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "266/36:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "266/37:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "266/38:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "266/39:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "266/40:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.002, max_len=3, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "266/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "266/42:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"], a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/43:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"].name, a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/44:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"].name(), a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/45:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"].str(), a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/46:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"].tolist(), a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/47:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/48:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([x.str for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/49:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([x.values() for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/50:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([list(x) for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/51:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([x for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/52:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([list(x)[0] for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/53:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([list(x)[0] for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/54:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([list(x) for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/55:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([\", \".join(list(x)) for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/56:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([\", \".join(list(x)) for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/57:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "266/58:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"Two actors\", a, dataset.len())\n",
      "266/59:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"Two actors\", a, len(dataset))\n",
      "266/60:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "266/61:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"Actors\", a, items)\n",
      "266/62:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"Crew\", a, items)\n",
      "266/63:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"Keywords\", a, items)\n",
      "266/64:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"Overview\", a, items)\n",
      "266/65:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "# plot_itemset(\"Two actors\", a, len(dataset))\n",
      "a\n",
      "266/66:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"Two actors\", a, len(dataset))\n",
      "267/1:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "267/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "267/3:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "267/4:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "267/5:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "267/6:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/7:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/10:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/11:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "267/12:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/13:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/14:\n",
      "\n",
      "genres\n",
      "267/15:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "267/16:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/17:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "267/18:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "267/19:\n",
      "\n",
      "oscars.head(10)\n",
      "267/20:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "267/21:\n",
      "\n",
      "dataset = oscar_movies\n",
      "267/22:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/24:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/25:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/26:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/27:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/28:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/29:\n",
      "\n",
      "Średnie największe przychody z filmów oscarowych przynoszą horrory przy stosunkowo niskich budżetach. \n",
      "\n",
      "Średnie największe budżety posiadają filmy animowane. \n",
      "\n",
      "Średnie najmniejsze przychody przynoszą filmy dokumentalne, posiadają one również średnio najmniejszy budżet.\n",
      "\n",
      "Średnio największą popularnością cieszą się filmy fantasy a najmniejszą filmy dokumentalne.\n",
      "\n",
      "Średnio najdłuższe są horrory a najkrótsze filmy dokumentalne.\n",
      "267/30:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "267/31:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "267/32:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "267/33:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "267/34:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/35:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/36:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/37:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/38:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/39:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "267/40:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/41:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/42:\n",
      "\n",
      "genres\n",
      "267/43:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "267/44:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/45:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "267/46:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "267/47:\n",
      "\n",
      "oscars.head(10)\n",
      "267/48:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "267/49:\n",
      "\n",
      "dataset = oscar_movies\n",
      "267/50:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/51:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/52:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/53:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/54:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/55:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/56:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/57:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/58:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/59:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/60:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "267/61:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "267/62:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "267/63:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "267/64:\n",
      "# Highest revenue movies in ieach genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/65:\n",
      "# Highest budget movies in ieach genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/66:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/67:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "267/68:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "267/69:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "267/70:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "267/71:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/72:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/73:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/74:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/75:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/76:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "267/77:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/78:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/79:\n",
      "\n",
      "genres\n",
      "267/80:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "267/81:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/82:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "267/83:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "267/84:\n",
      "\n",
      "oscars.head(10)\n",
      "267/85:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "267/86:\n",
      "\n",
      "dataset = oscar_movies\n",
      "267/87:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/88:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/89:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/90:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/91:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/92:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/93:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/94:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "267/95:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + items + \" highest revenue movies\", a, items)\n",
      "267/96:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "267/97:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "267/98:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "267/99:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "267/100:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/101:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/102:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/103:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/104:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/105:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "267/106:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/107:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/108:\n",
      "\n",
      "genres\n",
      "267/109:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "267/110:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/111:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "267/112:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "267/113:\n",
      "\n",
      "oscars.head(10)\n",
      "267/114:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "267/115:\n",
      "\n",
      "dataset = oscar_movies\n",
      "267/116:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/117:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/118:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/119:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/120:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/121:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/122:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/123:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "267/124:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/125:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/126:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "267/127:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "267/128:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "267/129:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "267/130:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/131:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/132:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/133:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/134:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "267/135:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + items + \" highest revenue movies\", a, items)\n",
      "267/136:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "267/137:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "269/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "270/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "270/2:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "270/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "270/4:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "270/5:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "270/6:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "270/7:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "270/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "270/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "270/10:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "270/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "270/12:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "270/13:\n",
      "\n",
      "genres\n",
      "270/14:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "270/15:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "270/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "270/17:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "270/18:\n",
      "\n",
      "oscars.head(10)\n",
      "270/19:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "270/20:\n",
      "\n",
      "dataset = oscar_movies\n",
      "270/21:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "270/22:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "270/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "270/24:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "270/25:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "270/26:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "270/27:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "270/28:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "270/29:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "270/30:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "270/31:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "270/32:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "270/33:\n",
      "\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "270/34:\n",
      "\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "270/35:\n",
      "\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "270/36:\n",
      "\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "270/37:\n",
      "\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "270/38:\n",
      "\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "270/39:\n",
      "\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "270/40:\n",
      "\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "270/41:\n",
      "\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "271/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "271/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "271/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "271/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "271/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "271/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "271/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "271/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "271/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "271/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "271/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "271/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "271/13: genres\n",
      "271/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "271/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "271/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "271/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "271/18: oscars.head(10)\n",
      "271/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "271/20: dataset_oscars = oscar_movies\n",
      "271/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "271/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "271/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "271/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "271/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "271/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "271/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "271/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "271/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "271/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "271/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "271/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "271/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "271/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "271/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "271/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "271/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "271/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "271/39:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "271/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "271/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "271/42:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "272/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "272/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "272/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "272/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "272/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "272/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "272/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "272/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "272/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "272/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "272/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "272/13: genres\n",
      "272/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "272/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "272/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "272/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "272/18: oscars.head(10)\n",
      "272/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "272/20: dataset_oscars = oscar_movies\n",
      "272/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "272/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "272/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "272/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "272/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "272/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "272/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "272/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "272/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "272/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "272/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "272/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "272/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "272/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "272/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "272/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "272/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "272/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "272/39:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "272/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "272/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/43:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/44:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/45:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/46:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "274/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "274/2:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "274/3:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "274/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "274/5:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "274/6:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "274/7:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "274/8:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "274/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "274/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "274/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "274/12:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "274/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "274/14:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "274/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "274/16:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "274/17:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "274/18:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "274/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "274/20:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "274/21: oscars.head(10)\n",
      "274/22:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "274/23: dataset_oscars = oscar_movies\n",
      "274/24:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "274/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "274/27:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "274/28:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "274/29:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "274/30:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "274/31:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "274/32:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "274/33:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"23. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "274/34:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "274/35:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "274/36:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "275/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "275/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "275/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "275/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "275/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "275/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "275/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "275/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "275/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "275/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "275/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "275/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "275/13: genres\n",
      "275/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "275/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "275/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "275/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "275/18: oscars.head(10)\n",
      "275/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "275/20: dataset_oscars = oscar_movies\n",
      "275/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "275/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "275/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "275/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "275/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "275/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "275/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "275/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "275/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "275/30:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"23. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "275/31:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "275/32:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "275/33:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "275/34:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "275/35:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "275/36:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "275/37:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "275/38:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "275/39:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"27. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "275/40:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "275/41:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "275/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "275/43:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "275/44:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "275/45:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/46:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors 2- and 3-itemsets in movies\", a, len(dataset))\n",
      "275/47:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "275/48:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/49:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/50:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"35. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/51:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"36. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "275/52:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/53:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"38. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/54:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('39. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "275/55:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"32. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/56:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/57:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/58:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"35. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "275/59:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"36. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/60:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/61:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('38. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "275/62:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"32. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/63:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/64:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/65:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/66:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"35. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "275/67:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"36. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/68:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/69:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('38. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "276/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "278/1:\n",
      "X_train = X_train.reshape(60000, 784)\n",
      "X_test = X_test.reshape(10000, 784)\n",
      "X_train = X_train.astype('float32')\n",
      "X_test = X_test.astype('float32')\n",
      "X_train /= #TODO\n",
      "X_test /= #TODO\n",
      "print(\"Training matrix shape\", X_train.shape)\n",
      "print(\"Testing matrix shape\", X_test.shape)\n",
      "281/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "282/1:\n",
      "\n",
      "import pandas as pd\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/2:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "282/3:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "y = train[:][0]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    print(\"Train Index: \", train_index, \"\\n\")\n",
      "    print(\"Test Index: \", test_index)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "285/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/eksploracja/lab-2'])\n",
      "282/4:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "y = train[:][1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    print(\"Train Index: \", train_index, \"\\n\")\n",
      "    print(\"Test Index: \", test_index)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "282/5:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "282/6:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "282/7:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/8:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/9:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/10:\n",
      "#4\n",
      "X = train\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/11:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/12:\n",
      "#4\n",
      "X = train[:][2:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/13:\n",
      "#4\n",
      "X = train[2:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/14:\n",
      "#4\n",
      "X = train[2:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/15:\n",
      "#4\n",
      "X = train[1:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/16:\n",
      "#4\n",
      "X = train[1:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/17:\n",
      "#4\n",
      "X = train[1:][2:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/18:\n",
      "#4\n",
      "X = train[:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/19:\n",
      "#4\n",
      "X = train[:,1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/20:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/21:\n",
      "#4\n",
      "X = (train.to_numpy)[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/22:\n",
      "#4\n",
      "X = train.to_numpy()[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/23:\n",
      "#4\n",
      "X = train.to_numpy()[:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/24:\n",
      "#4\n",
      "X = train.to_numpy()[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/25:\n",
      "#4\n",
      "X = train.to_numpy()[:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/26:\n",
      "#4\n",
      "X = train.to_numpy()[1:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/27:\n",
      "#4\n",
      "X = train.to_numpy()[:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/28:\n",
      "#4\n",
      "X = train.to_numpy()[:, 1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/29:\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/30:\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 2]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/31:\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "y\n",
      "282/32:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    print(\"Train Index: \", train_index, \"\\n\")\n",
      "    print(\"Test Index: \", test_index)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "282/33:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/34:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    print(\"Train Index: \", train_index, \"\\n\")\n",
      "    print(\"Test Index: \", test_index)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "282/35:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "282/36:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "scores.avg()\n",
      "282/37:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/38:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "depth_range = range(2, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X):\n",
      "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "        tree.fit(X_train, y_train)\n",
      "        scores.append(tree.score(X_test, y_test))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/39:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "depth_range = range(2, 21)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X):\n",
      "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "        tree.fit(X_train, y_train)\n",
      "        scores.append(tree.score(X_test, y_test))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/40:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 21)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X):\n",
      "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "        tree.fit(X_train, y_train)\n",
      "        scores.append(tree.score(X_test, y_test))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/41:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X):\n",
      "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "        tree.fit(X_train, y_train)\n",
      "        scores.append(tree.score(X_test, y_test))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/42:\n",
      "\n",
      "from sklearn import tree\n",
      "decision_tree = tree.DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "282/43:\n",
      "#5\n",
      "\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "282/44:\n",
      "#5\n",
      "test = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "decision_tree.predict(test)\n",
      "282/45:\n",
      "#5\n",
      "test = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "f = open(\"results.txt\",\"csv\")\n",
      "f.write(decision_tree.predict(test))\n",
      "282/46:\n",
      "#5\n",
      "test = test[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "f = open(\"results.txt\",\"csv\")\n",
      "f.write(decision_tree.predict(test))\n",
      "282/47:\n",
      "#5\n",
      "test = test[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "f = open(\"results.csv\",\"w+\")\n",
      "f.write(decision_tree.predict(test))\n",
      "282/48:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/49:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "282/50:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "282/51:\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        scores.append(tree.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/52:\n",
      "#5\n",
      "test_without_index = test[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "f = open(\"results.csv\",\"w+\")\n",
      "f.write(decision_tree.predict(test_without_index))\n",
      "282/53:\n",
      "#5\n",
      "test_without_index = test[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv',prediction, delimiter=',')\n",
      "282/54:\n",
      "#5\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv',prediction, delimiter=',')\n",
      "282/55:\n",
      "#6\n",
      "tree.plot_tree(decision_tree);\n",
      "282/56:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/57:\n",
      "#6\n",
      "tree.plot_tree(decision_tree);\n",
      "282/58:\n",
      "#6\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "282/59:\n",
      "#6\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/60:\n",
      "#6\n",
      "plt.figure(figsize=(12, 12))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/61:\n",
      "#6\n",
      "plt.figure(figsize=(18, 12))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/62:\n",
      "#6\n",
      "plt.figure(figsize=(18, 10))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/63:\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/64:\n",
      "#6\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.feature_names, class_names=train.target_names);\n",
      "282/65:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.feature_names, class_names=train.target_names)\n",
      "plt.show()\n",
      "282/66:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.column_names, class_names=train.target_names)\n",
      "plt.show()\n",
      "282/67:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns, class_names=train.target_names)\n",
      "plt.show()\n",
      "282/68:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns)\n",
      "plt.show()\n",
      "282/69:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:])\n",
      "plt.show()\n",
      "282/70:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Survived\"])\n",
      "plt.show()\n",
      "282/71:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=train.columns[1])\n",
      "plt.show()\n",
      "282/72:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"survived\", \"died\"])\n",
      "plt.show()\n",
      "282/73:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "286/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/2:\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "286/3:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "g\n",
      "# corr, _ = pearsonr(train[train[\"\"]], train)\n",
      "286/4:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "g.groups()\n",
      "# corr, _ = pearsonr(train[train[\"\"]], train)\n",
      "286/5:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "g.groups\n",
      "# corr, _ = pearsonr(train[train[\"\"]], train)\n",
      "286/6:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train[[7,16,50]]\n",
      "286/7:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train[7,16,50]\n",
      "286/8:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc([7,16,50])\n",
      "286/9:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc([7,16,50])\n",
      "286/10:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc([7,16,50])\n",
      "286/11:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc([[7,16,50]])\n",
      "286/12:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc(7,16,50)\n",
      "286/13:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc([7,16,50])\n",
      "286/14:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc[7,16,50]\n",
      "286/15:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc[7,16,50]\n",
      "286/16:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc[7,16,50]\n",
      "286/17:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc[[7,16,50]]\n",
      "286/18:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[[group]], train)\n",
      "train.iloc[[7,16,50], 1]\n",
      "286/19:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[[group]], train)\n",
      "train.iloc[[7,16,50], ]\n",
      "286/20:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[[group]], train)\n",
      "train.iloc[[7,16,50], 1]\n",
      "286/21:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(train[[group]], train[[group], 1])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/22:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(train[group], train[[group], 1])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/23:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/24:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    train[group.tolist()]\n",
      "    # corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "# names\n",
      "# correlations\n",
      "286/25:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    group.tolist()\n",
      "    # train[group.tolist()]\n",
      "    # corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "# names\n",
      "# correlations\n",
      "286/26:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names = group.tolist()\n",
      "    # train[group.tolist()]\n",
      "    # corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "names\n",
      "# correlations\n",
      "286/27:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Title\", \"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names = group.tolist()\n",
      "    # train[group.tolist()]\n",
      "    # corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "names\n",
      "# correlations\n",
      "286/28:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Title\", \"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/29:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Title\", \"Survived\"].groupby(train[\"Title\"])\n",
      "# names = []\n",
      "# correlations = []\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "#     names.append(name)\n",
      "#     correlations.append(corr)\n",
      "# names\n",
      "# correlations\n",
      "286/30:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "# names = []\n",
      "# correlations = []\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "#     names.append(name)\n",
      "#     correlations.append(corr)\n",
      "# names\n",
      "# correlations\n",
      "286/31:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/32:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "# correlations\n",
      "286/33:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "# correlations\n",
      "286/34:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/35:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations.append(group[\"Survived\"])\n",
      "names\n",
      "correlations\n",
      "286/36:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"]\n",
      "names\n",
      "correlations\n",
      "286/37:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\", 1]\n",
      "names\n",
      "correlations\n",
      "286/38:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"][1]\n",
      "names\n",
      "correlations\n",
      "286/39:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"]\n",
      "names\n",
      "correlations\n",
      "286/40:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"].tolist)_\n",
      "names\n",
      "correlations\n",
      "286/41:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"].tolist()\n",
      "names\n",
      "correlations\n",
      "286/42:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/43:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/44:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "286/45:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]]\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "286/46:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]]\n",
      "names = []\n",
      "# correlations = []\n",
      "# for name, group in g:\n",
      "#     # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "#     # names.append(name)\n",
      "#     # correlations.append(corr)\n",
      "#     names = group[\"Title\"].tolist()\n",
      "# names\n",
      "# correlations\n",
      "g\n",
      "286/47:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]]\n",
      "names = []\n",
      "correlations = []\n",
      "# for name, group in g:\n",
      "#     # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "#     # names.append(name)\n",
      "#     # correlations.append(corr)\n",
      "#     names = group[\"Title\"].tolist()\n",
      "# names\n",
      "g.astype(float).corr()\n",
      "# correlations\n",
      "286/48:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    names = group.astype(float).corr()\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "287/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/eksploracja/lab-2'])\n",
      "286/49:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    names = group[\"Title\"].corr(group[\"Survived\"])\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "286/50:\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "286/51:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/52:\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "286/53:\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        scores.append(tree.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "286/54:\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "286/55:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/56:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test)\n",
      "286/57:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap='viridis')\n",
      "286/58:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test)\n",
      "286/59:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues)\n",
      "286/60:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "286/61:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "286/62:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/63:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "accuracy_score(y_true, y_pred)\n",
      "286/64:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score: \", f1_score(y_true, y_pred))\n",
      "286/65:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/66:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score: \", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:: \", average_precision_score(y_true, y_pred))\n",
      "286/67:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score: \", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score: \", average_precision_score(y_true, y_pred))\n",
      "286/68:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "286/69:\n",
      "#8\n",
      "# kryterium podziału (gini vs. entropy), najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "286/70:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for scores from 1 to 20:\", scores)\n",
      "\n",
      "min_elem = range(1, 20)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for min samples in leaf from 1 to 20:\", scores)\n",
      "286/71:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 40)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for scores from 1 to 40:\", scores)\n",
      "plot(list(depth_range), scores)\n",
      "\n",
      "min_elem = range(1, 40)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for min samples in leaf from 1 to 40:\", scores)\n",
      "286/72:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 40)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for scores from 1 to 40:\", scores)\n",
      "plt.plot(list(depth_range), scores)\n",
      "\n",
      "min_elem = range(1, 40)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for min samples in leaf from 1 to 40:\", scores)\n",
      "286/73:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 40)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 40)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.show()\n",
      "286/74:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.show()\n",
      "288/1:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "288/2:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "288/3:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "288/4:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    # names = group[\"Title\"].corr(group[\"Survived\"])\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "288/5:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        scores.append(tree.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "288/6:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "288/7:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "289/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "289/2:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "289/3:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    # names = group[\"Title\"].corr(group[\"Survived\"])\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "289/4:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "289/5:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "289/6:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "289/7:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "289/8:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "289/9:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "289/10:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "289/11:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "    # names = group[\"Title\"].corr(group[\"Survived\"])\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "correlations\n",
      "289/12:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.show()\n",
      "289/13:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.show()\n",
      "289/14:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio\")\n",
      "plt.show()\n",
      "289/15:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survived among all with the same title\")\n",
      "plt.show()\n",
      "289/16:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "290/1:\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X, y = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y))\n",
      "290/2:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "290/3:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "290/4:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "290/5:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "290/6:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "290/7:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "290/8:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "290/9:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "290/10:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "290/11:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "290/12:\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X, y = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y))\n",
      "291/1:\n",
      "X_y_nyt = pd.read_csv('data/nyt-frame.csv')\n",
      "X_y_nyt.head(3)\n",
      "291/2:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "291/3:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "291/4:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "291/5:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "291/6:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "291/7:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "291/8:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/9:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "291/10:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "291/11:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "291/12:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X, y = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "291/13:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "291/14:\n",
      "X_y_nyt = pd.read_csv('data/nyt-frame.csv')\n",
      "X_y_nyt.head(3)\n",
      "291/15:\n",
      "X_y_nyt = pd.read_csv('nyt-frame.csv')\n",
      "X_y_nyt.head(3)\n",
      "291/16:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "291/17:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "291/18:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/19:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/20:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/21:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/22:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"taxi\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/23:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/24:\n",
      "\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/25:\n",
      "\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan')\n",
      "plt.show()\n",
      "291/26:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan')\n",
      "plt.show()\n",
      "291/27:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "291/28:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan metric\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "291/29:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "291/30:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "# 1- Dimension reduction of NYT\n",
      "dff = nyt._get_numeric_data()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "291/31:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "# 1- Dimension reduction of NYT\n",
      "dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "291/32:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "291/33:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "291/34:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "291/35:\n",
      "X_pca[0:5,:]\n",
      "variance = pca.explainedvariance\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/36:\n",
      "X_pca[0:5,:]\n",
      "variance = X_pca.explainedvariance\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/37:\n",
      "X_pca[0:5,:]\n",
      "variance = X_pca.explained_variance\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/38:\n",
      "X_pca[0:5,:]\n",
      "variance = PCA.explained_variance\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/39:\n",
      "X_pca[0:5,:]\n",
      "variance = X_pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/40:\n",
      "X_pca[0:5,:]\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/41:\n",
      "X_pca[0:5,:]\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance)\n",
      "plt.show()\n",
      "291/42:\n",
      "X_pca[0:5,:]\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/43:\n",
      "plt.plot(pca.explained_variance_, 'ro')\n",
      "plt.show()\n",
      "291/44:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "X_pca\n",
      "291/45:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "data.head(5)\n",
      "291/46:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "data.head(5)\n",
      "y_nyt = data.loc[:,'class.labels']\n",
      "reds = y_nyt == 'art'\n",
      "blues = y_nyt == 'music'\n",
      "reds\n",
      "291/47:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "data.head(5)\n",
      "# y_nyt = data.loc[:,'class.labels']\n",
      "# reds = y_nyt == 'art'\n",
      "# blues = y_nyt == 'music'\n",
      "# reds\n",
      "291/48:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "data.head(5)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "reds = y_nyt == 'art'\n",
      "blues = y_nyt == 'music'\n",
      "reds\n",
      "291/49:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "291/50:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "291/51:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "291/52:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "291/53:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "291/54:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "291/55:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "291/56:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/57:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "291/58:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "291/59:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "291/60:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "291/61:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "291/62:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "data.head(5)\n",
      "291/63:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "292/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "292/2:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "292/3:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "292/4:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "292/5:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "292/6:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "292/7:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "292/8:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "292/9:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "292/10:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "292/11:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "292/12:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "292/13:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "data.head(5)\n",
      "292/14:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "293/2:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "293/3:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "293/4:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "293/5:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "293/6:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "293/7:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "293/8:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "293/9:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "293/10:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "293/11:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "293/12:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "293/13:\n",
      "\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "data.head(5)\n",
      "293/14:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/15:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='euclidean with weights ')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/16:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/17:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/18:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/19:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "print(\"Euclidean with weights has the same values as Euclidean without them\")\n",
      "295/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "295/2:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "295/3:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "295/4:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "295/5:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "295/6:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "295/7:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordów w liściu oraz maksymalna głębokość drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "295/8:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "295/9:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "295/10:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "295/11:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "295/12:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "295/13:\n",
      "\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "data.head(5)\n",
      "295/14:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "print(\"Euclidean with weights has the same values as Euclidean without them\")\n",
      "296/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "296/2:\n",
      "\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "296/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "296/4:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "296/5:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "296/6:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "296/7:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "296/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "296/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "296/10:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "296/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "296/12:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "296/13:\n",
      "\n",
      "genres\n",
      "296/14:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "296/15:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "296/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "296/17:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "296/18:\n",
      "\n",
      "oscars.head(10)\n",
      "296/19:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "296/20:\n",
      "\n",
      "dataset_oscars = oscar_movies\n",
      "296/21:\n",
      "\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "296/22:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "296/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "296/24:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "296/25:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "296/26:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "296/27:\n",
      "\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "296/28:\n",
      "\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "296/29:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "296/30:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "296/31:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "296/32:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "296/33:\n",
      "\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "296/34:\n",
      "\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "296/35:\n",
      "\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "296/36:\n",
      "\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "296/37:\n",
      "\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "296/38:\n",
      "\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "296/39:\n",
      "\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "296/40:\n",
      "\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "296/41:\n",
      "\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "296/42:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "296/43:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "296/44:\n",
      "\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "296/45:\n",
      "\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "296/46:\n",
      "\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "297/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "297/2:\n",
      "\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "297/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "297/4:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "297/5:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "297/6:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "297/7:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "297/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "297/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "297/10:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "297/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "297/12:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "297/13:\n",
      "\n",
      "genres\n",
      "297/14:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "297/15:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "297/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "297/17:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "297/18:\n",
      "\n",
      "oscars.head(10)\n",
      "297/19:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "297/20:\n",
      "\n",
      "dataset_oscars = oscar_movies\n",
      "297/21:\n",
      "\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "297/22:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "297/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "297/24:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "297/25:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "297/26:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "297/27:\n",
      "\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "297/28:\n",
      "\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "297/29:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "297/30:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "297/31:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "297/32:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "297/33:\n",
      "\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "297/34:\n",
      "\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "297/35:\n",
      "\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "297/36:\n",
      "\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "297/37:\n",
      "\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "297/38:\n",
      "\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "297/39:\n",
      "\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "297/40:\n",
      "\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "297/41:\n",
      "\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "297/42:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "297/43:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "297/44:\n",
      "\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "297/45:\n",
      "\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "297/46:\n",
      "\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "298/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "298/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "298/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "298/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "298/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "298/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "298/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "298/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "298/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "298/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "298/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "298/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "298/13: genres\n",
      "298/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "298/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "298/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "298/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "298/18: oscars.head(10)\n",
      "298/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "298/20: dataset_oscars = oscar_movies\n",
      "298/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "298/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "298/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "298/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "298/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "298/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "298/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "298/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "298/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "298/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "298/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "298/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "298/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "298/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "298/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "298/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "298/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "298/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "298/39:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "298/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "298/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "298/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "298/43:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "298/44:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "298/45:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "298/46:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "299/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "299/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "299/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "299/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "299/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "299/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "299/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "299/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "299/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "299/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "299/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "299/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "299/13: genres\n",
      "299/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "299/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "299/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "299/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "299/18: oscars.head(10)\n",
      "299/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "299/20: dataset_oscars = oscar_movies\n",
      "299/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "299/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "299/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "299/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "299/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "299/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "299/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "299/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "299/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "299/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "299/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "299/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "299/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "299/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "299/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "299/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "299/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "299/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "299/39:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "299/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "299/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "299/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "299/43:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "299/44:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "299/45:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "299/46:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "299/47:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df))\n",
      "299/48:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), 90)\n",
      "300/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "300/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "300/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "300/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "300/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "300/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "300/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "300/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "300/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "300/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "300/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "300/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "300/13: genres\n",
      "300/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "300/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "300/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "300/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "300/18: oscars.head(10)\n",
      "300/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "300/20: dataset_oscars = oscar_movies\n",
      "300/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "300/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "300/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "300/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "300/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "300/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "300/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "300/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "300/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "300/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "300/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "300/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "300/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "300/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "300/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "300/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "300/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "300/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "300/39:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "300/40:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "300/41:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "300/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "300/43:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "300/44:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "300/45:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "300/46:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "300/47:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"35. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "300/48:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"36. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "300/49:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "300/50:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"38. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "300/51:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('39. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "300/52:\n",
      "# Required imports\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "from sklearn import preprocessing\n",
      "from sklearn.cluster import KMeans\n",
      "from wordcloud import WordCloud, STOPWORDS \n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from sklearn.cluster import DBSCAN\n",
      "import math\n",
      "300/53:\n",
      "crewDataset = oscar_movies[['id','title','crew']].copy()\n",
      "crewDataset['crew'] = crewDataset['crew'].apply(lambda x: ['\\\"'+i['name']+'\\\"-\\\"'+i['job']+'\\\"' for i in x] if x != {} else [])\n",
      "crewDataset = crewDataset['crew']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(crewDataset).transform(crewDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(4,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"40. Frequent itemsets for Oscars movies crew\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 40: Zbiory częste dla członków ekip filmowych dla filmów Oskarowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste jednowymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Wśród ekip filmowych dla filmów oskarowych pojawiają się grupy grające wspólnie. Najczęściej przewijającą się grupą są producenci wykonawczy: Bob Weinstein oraz Harvey Weinstein.\n",
      "#     - Grupy częste występują zazwyczaj w obrębie danej funkcji na planie\n",
      "#     - Wzrost min-support powoduje, że zaczynają dominować krótsze itemsety. Tendencja trudna do zaobserwowania ze względu na dużą różnorodność wśród filmów Oskarowych.\n",
      "300/54:\n",
      "# Film crem most frequent itemsets in films\n",
      "crewDataset = dataset[['id','title','crew']].copy()\n",
      "crewDataset['crew'] = crewDataset['crew'].apply(lambda x: ['\\\"'+i['name']+'\\\"-\\\"'+i['job']+'\\\"' for i in x] if x != {} else [])\n",
      "crewDataset = crewDataset['crew']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(crewDataset).transform(crewDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(10,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"41. Frequent itemsets for all movies crew\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 41: Zbiory częste dla członków ekip filmowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 10;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste jednowymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Wśród ekip filmowych pojawiają się grupy członków, którzy często współpracują ze sobą. Najczęściej przewijającą się grupą są osoby odpowiedzialne za castingi: Tricia Wood oraz Debora Aquila.\n",
      "#     - Częste zbiory występują najczęściej wśród castingowców\n",
      "300/55:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: [classify_movie_budget(x)])\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"42. Frequent itemsets for Oscar movies - parameters ['production_companies','budget']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 42: Zbiory częste dla wytwórni filmowych oraz budżetów filmów w kategorii filmy Oskarowe\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste dwuwymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Duże wytwórnie filmowe takie, jak Warner Bros, czy Universal Pictures tworzą głównie produkcje wysokobudżetowe.\n",
      "#     - Ciekawe, jest że dla wytwórni 20th Century Fox Film pojawiają się zbiory częste dla produkcji w różnych kategoriach budżetowych.\n",
      "300/56:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: [classify_movie_budget(x)])\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(20,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"43. Frequent itemsets for movies - parameters ['production_companies','budget']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 43: Zbiory częste dla wytwórni filmowych oraz budżetów filmów\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 20;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste dwuwymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Duże wytwórnie filmowe takie, jak Warner Bros, czy Universal Pictures tworzą głównie produkcje wysokobudżetowe.\n",
      "300/57:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"44. Frequent itemsets for Oscar movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 44: Zbiory częste dla wytwórni filmowych, obsady oraz słów kluczowych dla filmów Oskarowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Dozwolone zbiory częste dla wielu elementów w obrębie jednej cechy.\n",
      "300/58:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(11,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 45: Zbiory częste dla wytwórni filmowych, obsady oraz słów kluczowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 11;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Dozwolone zbiory częste dla wielu elementów w obrębie jednej cechy.\n",
      "# Wnioski: \n",
      "#    - Częste zbiory tworzą się zazwyczaj w obrębie jednego wymiaru. \n",
      "#    - Dla dużych wartości min_support dopinują krótke zbiory częste.\n",
      "300/59:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"46. Frequent itemsets for Oscras movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 46: Zbiory częste dla wytwórni filmowych, budżetów oraz słów kluczowych dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "300/60:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(8,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"47. Frequent itemsets for movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 47: Zbiory częste dla wytwórni filmowych, budżetów oraz słów kluczowych \n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 8;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Wśród częstych zbiorów pojawiają się wysokobudżetowe filmy Worner Bros ze słowem kluczowym dc comics\n",
      "#    - Wśród częstych zbiorów pojawiają się również wysokobudżetowe filmy wytwórni 20th Century Fox Film ze słowem kluczowym alien\n",
      "300/61:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"48. Frequent itemsets for Oscras movies - parameters ['cast', 'crew']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 48: Zbiory częste dla obsady i ekipy filmowej dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja co najmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Dla filmów Oskarowych wśród zbiorów częstych dla obsady i ekipy pojaiwają się grupy. Wskazuje, to na fakt istnienia zależności, że producenci/reżyserzy mają swoiuch ulubionych aktorów.\n",
      "300/62:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"49. Frequent itemsets for Oscars movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 49: Zbiory częste dla obsady, budżetu i wytwórni dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Liczba częstych zbiorów i ich support jest niewielki dla filmów oskarowych dla wektora cech: budżet, obsada, wytwórnia\n",
      "300/63:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(7,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"50. Frequent itemsets for movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 50: Zbiory częste dla obsady, budżetu i wytwórni filmowej\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 7;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "300/64:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(2,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"51. Frequent itemsets for Oscar movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 51: Zbiory częste dla obsady, reżysera i wytwórni dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 2;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "300/65:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"52. Frequent itemsets for movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 52: Zbiory częste dla obsady, reżysera i wytwórni dla wszystkich filmów\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "300/66:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['revenue','budget','popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 10]\n",
      "processedDataset = processedDataset[processedDataset['revenue'] < 2000000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] < 2500000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "fig = plt.figure(figsize=(12, 12))\n",
      "ax = Axes3D(fig)\n",
      "\n",
      "random_state = 170\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "ax.scatter(processedDataset['revenue'], processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"53. Movie clusters (KMeans k = 4 ) by revenue, budget and popularity\")\n",
      "plt.xlabel(\"Revenue\")\n",
      "plt.ylabel(\"Budget\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 53: Klastrowanie zbioru filmów dla cech revenue, budget, popularity\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 4;\n",
      "# Wnioski:\n",
      "#    - Trudno wyodrębnić spójne grupy. Dużą grupę stanowią filmy niskobudżetowe, które przyniosły niski dochód i nie były popularne.\n",
      "300/67:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"54. Movie clustering (KMeans) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 54: Klastrowanie zbioru filmów dla cech revenue, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 3;\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe i niskodochodowe.\n",
      "300/68:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"55. Movie clustering (DBSCAN) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 55: Klastrowanie zbioru filmów dla cech revenue, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe i niskodochodowe.\n",
      "300/69:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"56. Movie clustering (KMeans) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 56: Klastrowanie zbioru filmów dla cech popularity, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=3;\n",
      "# Uwagi: dokonano normalizacji danych. Odfiltrowano zerowe budżety.\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe, które wykazują popularnośc poniżej przeciętnej.\n",
      "300/70:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"57. Movie clustering (DBSCAN) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 57: Klastrowanie zbioru filmów dla cech popularity, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "300/71:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 100]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.06, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"58. Movie clustering (DBSCAN) by budget and popularity for Oscars movies\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 58: Klastrowanie zbioru filmów dla cech popularity, budget dla filmów Oskarowych\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps=0.06;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "300/72:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['runtime', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] < 200]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] > 50]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['runtime'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"59. Movie clustering (KMeans) by runtime and popularity\")\n",
      "plt.xlabel(\"Runtime\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 59: Klastrowanie zbioru filmów dla cech popularity, runtime\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=4;\n",
      "# Uwagi: dokonano normalizacji danych. Usunięto skrajne wartości dla czasu trwania dla lepszej wizualizacji.\n",
      "300/73:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(11,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 45: Zbiory częste dla wytwórni filmowych, obsady oraz słów kluczowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 11;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Dozwolone zbiory częste dla wielu elementów w obrębie jednej cechy.\n",
      "# Wnioski: \n",
      "#    - Częste zbiory tworzą się zazwyczaj w obrębie jednego wymiaru. \n",
      "#    - Dla dużych wartości min_support dopinują krótke zbiory częste.\n",
      "300/74:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"46. Frequent itemsets for Oscras movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 46: Zbiory częste dla wytwórni filmowych, budżetów oraz słów kluczowych dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "300/75:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(8,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"47. Frequent itemsets for movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 47: Zbiory częste dla wytwórni filmowych, budżetów oraz słów kluczowych \n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 8;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Wśród częstych zbiorów pojawiają się wysokobudżetowe filmy Worner Bros ze słowem kluczowym dc comics\n",
      "#    - Wśród częstych zbiorów pojawiają się również wysokobudżetowe filmy wytwórni 20th Century Fox Film ze słowem kluczowym alien\n",
      "300/76:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"48. Frequent itemsets for Oscras movies - parameters ['cast', 'crew']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 48: Zbiory częste dla obsady i ekipy filmowej dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja co najmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Dla filmów Oskarowych wśród zbiorów częstych dla obsady i ekipy pojaiwają się grupy. Wskazuje, to na fakt istnienia zależności, że producenci/reżyserzy mają swoiuch ulubionych aktorów.\n",
      "300/77:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"49. Frequent itemsets for Oscars movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 49: Zbiory częste dla obsady, budżetu i wytwórni dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Liczba częstych zbiorów i ich support jest niewielki dla filmów oskarowych dla wektora cech: budżet, obsada, wytwórnia\n",
      "300/78:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(7,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"50. Frequent itemsets for movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 50: Zbiory częste dla obsady, budżetu i wytwórni filmowej\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 7;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "300/79:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(2,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"51. Frequent itemsets for Oscar movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 51: Zbiory częste dla obsady, reżysera i wytwórni dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 2;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "300/80:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"52. Frequent itemsets for movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 52: Zbiory częste dla obsady, reżysera i wytwórni dla wszystkich filmów\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "300/81:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['revenue','budget','popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 10]\n",
      "processedDataset = processedDataset[processedDataset['revenue'] < 2000000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] < 2500000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "fig = plt.figure(figsize=(12, 12))\n",
      "ax = Axes3D(fig)\n",
      "\n",
      "random_state = 170\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "ax.scatter(processedDataset['revenue'], processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"53. Movie clusters (KMeans k = 4 ) by revenue, budget and popularity\")\n",
      "plt.xlabel(\"Revenue\")\n",
      "plt.ylabel(\"Budget\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 53: Klastrowanie zbioru filmów dla cech revenue, budget, popularity\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 4;\n",
      "# Wnioski:\n",
      "#    - Trudno wyodrębnić spójne grupy. Dużą grupę stanowią filmy niskobudżetowe, które przyniosły niski dochód i nie były popularne.\n",
      "300/82:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"54. Movie clustering (KMeans) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 54: Klastrowanie zbioru filmów dla cech revenue, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 3;\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe i niskodochodowe.\n",
      "300/83:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"55. Movie clustering (DBSCAN) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 55: Klastrowanie zbioru filmów dla cech revenue, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe i niskodochodowe.\n",
      "300/84:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"56. Movie clustering (KMeans) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 56: Klastrowanie zbioru filmów dla cech popularity, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=3;\n",
      "# Uwagi: dokonano normalizacji danych. Odfiltrowano zerowe budżety.\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe, które wykazują popularnośc poniżej przeciętnej.\n",
      "300/85:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"57. Movie clustering (DBSCAN) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 57: Klastrowanie zbioru filmów dla cech popularity, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "300/86:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 100]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.06, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"58. Movie clustering (DBSCAN) by budget and popularity for Oscars movies\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 58: Klastrowanie zbioru filmów dla cech popularity, budget dla filmów Oskarowych\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps=0.06;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "300/87:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['runtime', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] < 200]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] > 50]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['runtime'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"59. Movie clustering (KMeans) by runtime and popularity\")\n",
      "plt.xlabel(\"Runtime\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 59: Klastrowanie zbioru filmów dla cech popularity, runtime\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=4;\n",
      "# Uwagi: dokonano normalizacji danych. Usunięto skrajne wartości dla czasu trwania dla lepszej wizualizacji.\n",
      "301/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "301/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "301/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "301/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "301/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "301/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "301/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "301/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "301/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "301/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "301/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "301/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "301/13: genres\n",
      "301/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "301/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "301/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "301/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "301/18: oscars.head(10)\n",
      "301/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "301/20: dataset_oscars = oscar_movies\n",
      "301/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "301/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "301/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "301/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "301/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "301/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "301/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "301/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "301/30:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"23. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "301/31:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "301/32:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "301/33:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "301/34:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "301/35:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "301/36:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "301/37:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "301/38:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"27. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "301/39:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "301/40:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "301/41:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "301/42:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "301/43:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "301/44:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"32. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "301/45:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "301/46:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "301/47:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"35. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "301/48:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"36. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "301/49:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "301/50:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('38. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "301/51:\n",
      "# Required imports\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "from sklearn import preprocessing\n",
      "from sklearn.cluster import KMeans\n",
      "from wordcloud import WordCloud, STOPWORDS \n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from sklearn.cluster import DBSCAN\n",
      "import math\n",
      "301/52:\n",
      "crewDataset = oscar_movies[['id','title','crew']].copy()\n",
      "crewDataset['crew'] = crewDataset['crew'].apply(lambda x: ['\\\"'+i['name']+'\\\"-\\\"'+i['job']+'\\\"' for i in x] if x != {} else [])\n",
      "crewDataset = crewDataset['crew']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(crewDataset).transform(crewDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(4,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"39. Frequent itemsets for Oscars movies crew\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 39: Zbiory częste dla członków ekip filmowych dla filmów Oskarowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste jednowymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Wśród ekip filmowych dla filmów oskarowych pojawiają się grupy grające wspólnie. Najczęściej przewijającą się grupą są producenci wykonawczy: Bob Weinstein oraz Harvey Weinstein.\n",
      "#     - Grupy częste występują zazwyczaj w obrębie danej funkcji na planie\n",
      "#     - Wzrost min-support powoduje, że zaczynają dominować krótsze itemsety. Tendencja trudna do zaobserwowania ze względu na dużą różnorodność wśród filmów Oskarowych.\n",
      "301/53:\n",
      "# Film crem most frequent itemsets in films\n",
      "crewDataset = dataset[['id','title','crew']].copy()\n",
      "crewDataset['crew'] = crewDataset['crew'].apply(lambda x: ['\\\"'+i['name']+'\\\"-\\\"'+i['job']+'\\\"' for i in x] if x != {} else [])\n",
      "crewDataset = crewDataset['crew']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(crewDataset).transform(crewDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(10,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"40. Frequent itemsets for all movies crew\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 40: Zbiory częste dla członków ekip filmowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 10;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste jednowymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Wśród ekip filmowych pojawiają się grupy członków, którzy często współpracują ze sobą. Najczęściej przewijającą się grupą są osoby odpowiedzialne za castingi: Tricia Wood oraz Debora Aquila.\n",
      "#     - Częste zbiory występują najczęściej wśród castingowców\n",
      "301/54:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: [classify_movie_budget(x)])\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"41. Frequent itemsets for Oscar movies - parameters ['production_companies','budget']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 41: Zbiory częste dla wytwórni filmowych oraz budżetów filmów w kategorii filmy Oskarowe\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste dwuwymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Duże wytwórnie filmowe takie, jak Warner Bros, czy Universal Pictures tworzą głównie produkcje wysokobudżetowe.\n",
      "#     - Ciekawe, jest że dla wytwórni 20th Century Fox Film pojawiają się zbiory częste dla produkcji w różnych kategoriach budżetowych.\n",
      "301/55:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: [classify_movie_budget(x)])\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(20,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"42. Frequent itemsets for movies - parameters ['production_companies','budget']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 42: Zbiory częste dla wytwórni filmowych oraz budżetów filmów\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 20;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste dwuwymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Duże wytwórnie filmowe takie, jak Warner Bros, czy Universal Pictures tworzą głównie produkcje wysokobudżetowe.\n",
      "301/56:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"43. Frequent itemsets for Oscar movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 43: Zbiory częste dla wytwórni filmowych, obsady oraz słów kluczowych dla filmów Oskarowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Dozwolone zbiory częste dla wielu elementów w obrębie jednej cechy.\n",
      "301/57:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(11,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"44. Frequent itemsets for movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 44: Zbiory częste dla wytwórni filmowych, obsady oraz słów kluczowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 11;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Dozwolone zbiory częste dla wielu elementów w obrębie jednej cechy.\n",
      "# Wnioski: \n",
      "#    - Częste zbiory tworzą się zazwyczaj w obrębie jednego wymiaru. \n",
      "#    - Dla dużych wartości min_support dopinują krótke zbiory częste.\n",
      "301/58:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for Oscras movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 45: Zbiory częste dla wytwórni filmowych, budżetów oraz słów kluczowych dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "301/59:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(8,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"46. Frequent itemsets for movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 46: Zbiory częste dla wytwórni filmowych, budżetów oraz słów kluczowych \n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 8;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Wśród częstych zbiorów pojawiają się wysokobudżetowe filmy Worner Bros ze słowem kluczowym dc comics\n",
      "#    - Wśród częstych zbiorów pojawiają się również wysokobudżetowe filmy wytwórni 20th Century Fox Film ze słowem kluczowym alien\n",
      "301/60:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"47. Frequent itemsets for Oscras movies - parameters ['cast', 'crew']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 47: Zbiory częste dla obsady i ekipy filmowej dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja co najmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Dla filmów Oskarowych wśród zbiorów częstych dla obsady i ekipy pojaiwają się grupy. Wskazuje, to na fakt istnienia zależności, że producenci/reżyserzy mają swoiuch ulubionych aktorów.\n",
      "301/61:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"48. Frequent itemsets for Oscars movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 48: Zbiory częste dla obsady, budżetu i wytwórni dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Liczba częstych zbiorów i ich support jest niewielki dla filmów oskarowych dla wektora cech: budżet, obsada, wytwórnia\n",
      "301/62:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(7,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"49. Frequent itemsets for movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 49: Zbiory częste dla obsady, budżetu i wytwórni filmowej\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 7;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "301/63:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(2,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"50. Frequent itemsets for Oscar movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 50: Zbiory częste dla obsady, reżysera i wytwórni dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 2;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "301/64:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"51. Frequent itemsets for movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 51: Zbiory częste dla obsady, reżysera i wytwórni dla wszystkich filmów\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "301/65:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['revenue','budget','popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 10]\n",
      "processedDataset = processedDataset[processedDataset['revenue'] < 2000000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] < 2500000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "fig = plt.figure(figsize=(12, 12))\n",
      "ax = Axes3D(fig)\n",
      "\n",
      "random_state = 170\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "ax.scatter(processedDataset['revenue'], processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"52. Movie clusters (KMeans k = 4 ) by revenue, budget and popularity\")\n",
      "plt.xlabel(\"Revenue\")\n",
      "plt.ylabel(\"Budget\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 52: Klastrowanie zbioru filmów dla cech revenue, budget, popularity\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 4;\n",
      "# Wnioski:\n",
      "#    - Trudno wyodrębnić spójne grupy. Dużą grupę stanowią filmy niskobudżetowe, które przyniosły niski dochód i nie były popularne.\n",
      "301/66:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"53. Movie clustering (KMeans) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 53: Klastrowanie zbioru filmów dla cech revenue, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 3;\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe i niskodochodowe.\n",
      "301/67:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"54. Movie clustering (DBSCAN) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 54: Klastrowanie zbioru filmów dla cech revenue, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe i niskodochodowe.\n",
      "301/68:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"55. Movie clustering (KMeans) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 55: Klastrowanie zbioru filmów dla cech popularity, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=3;\n",
      "# Uwagi: dokonano normalizacji danych. Odfiltrowano zerowe budżety.\n",
      "# Wnioski:\n",
      "#  - Grupy nie są wyraźne. Największą grupę stanowią filmy niskobudżetowe, które wykazują popularnośc poniżej przeciętnej.\n",
      "301/69:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"56. Movie clustering (DBSCAN) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 56: Klastrowanie zbioru filmów dla cech popularity, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "301/70:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 100]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.06, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"57. Movie clustering (DBSCAN) by budget and popularity for Oscars movies\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 57: Klastrowanie zbioru filmów dla cech popularity, budget dla filmów Oskarowych\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps=0.06;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "301/71:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['runtime', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] < 200]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] > 50]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['runtime'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"58. Movie clustering (KMeans) by runtime and popularity\")\n",
      "plt.xlabel(\"Runtime\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 58: Klastrowanie zbioru filmów dla cech popularity, runtime\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=4;\n",
      "# Uwagi: dokonano normalizacji danych. Usunięto skrajne wartości dla czasu trwania dla lepszej wizualizacji.\n",
      "301/72:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for Oscars movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 45: Zbiory częste dla wytwórni filmowych, budżetów oraz słów kluczowych dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "301/73:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for Oscar movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 45: Zbiory częste dla wytwórni filmowych, budżetów oraz słów kluczowych dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "301/74:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"47. Frequent itemsets for Oscar movies - parameters ['cast', 'crew']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 47: Zbiory częste dla obsady i ekipy filmowej dla filmów Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja co najmniej dwuelementowych itemsetów. Zbióry częste wielowymiarowe. Zbiory częste dla wielu elementów w obrębie jednej cechy nie są dozwolone.\n",
      "# Wnioski: \n",
      "#    - Dla filmów Oskarowych wśród zbiorów częstych dla obsady i ekipy pojaiwają się grupy. Wskazuje, to na fakt istnienia zależności, że producenci/reżyserzy mają swoiuch ulubionych aktorów.\n",
      "302/1:\n",
      "import numpy as np\n",
      "from qiskit import *\n",
      "%matplotlib inline\n",
      "\n",
      "from qiskit import Aer\n",
      "from qiskit.visualization import plot_state_city\n",
      "from qiskit.visualization import plot_histogram\n",
      "\n",
      "from qiskit import IBMQ\n",
      "from qiskit.providers.ibmq import least_busy\n",
      "from qiskit.tools.monitor import job_monitor\n",
      "305/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "306/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "306/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "306/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "306/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "306/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/6:\n",
      "\n",
      "full_data_subset = dataset[[\"Budget\"]]\n",
      "306/7: full_data_subset = dataset[[\"Budget\"]]\n",
      "306/8: dataset[\"budget\"]\n",
      "306/9: dataset[\"budget\", \"revenue\"]\n",
      "306/10: dataset[[\"budget\", \"revenue\"]]\n",
      "306/11:\n",
      "dataset[[\"budget\", \"revenue\"]]\n",
      "cpi.update()\n",
      "306/12: dataset[[\"budget\", \"revenue\"]]\n",
      "306/13: dataset\n",
      "306/14:\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/15:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/16:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/17:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(df[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/18:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "307/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "307/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "307/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "307/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "307/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "307/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "307/7:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "307/8:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "307/9:\n",
      "dataset = dataset_original.copy()\n",
      "dataset = text_to_dict(dataset)\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "308/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "308/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "308/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "308/4:\n",
      "dataset = dataset_original.copy()\n",
      "dataset = text_to_dict(dataset)\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "308/5:\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "308/6:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/7:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/8:\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "308/9:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/10:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/11:\n",
      "def map_cast(cast_lists):\n",
      "    a = {}\n",
      "    for l in cast_lists:\n",
      "        for name in l:\n",
      "            if(name !in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "308/12:\n",
      "def map_cast(cast_lists):\n",
      "    a = {}\n",
      "    for l in cast_lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "308/13: dataset[\"cast\"]\n",
      "308/14: map_cast(dataset[\"cast\"])\n",
      "308/15:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/16:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "308/17:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/18:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/19:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/20:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/21:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/22:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/23:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/24:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/25:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/26:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/27:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/28:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/29: dataset[\"cast\"] = dataset.apply(lambda x: numpy.array(x[\"cast\"]) , axis=1)\n",
      "308/30: dataset[\"cast\"] = dataset.apply(lambda x: np.array(x[\"cast\"]) , axis=1)\n",
      "308/31:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/32:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/33: dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "308/34:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/35: dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "309/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "309/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "309/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "309/4:\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "309/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "309/7:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "309/8:\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "309/9: # dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "309/10:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/11:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/12:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]] #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]]\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/13:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/14:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/15: dataset.apply(lambda x: np.vectorize(x[\"cast\"]) , axis=1)\n",
      "309/16:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\"].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/17:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/18:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/19:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/20:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/21:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"title\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/22:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"title\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "prepare(X_train)\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/23:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "cast_names = cast_mapping.keys()\n",
      "cast_names\n",
      "# for g in top_cast_names:\n",
      "#     train['cast_name_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\n",
      "309/24:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "cast_names = cast_mapping.keys()\n",
      "cast_names\n",
      "for name in cast_names:\n",
      "    dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "309/25:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "309/26:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset\n",
      "309/27:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset\n",
      "309/28:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/29:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset.first()[\"crew\"]\n",
      "309/30:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset.first(0)[\"crew\"]\n",
      "309/31:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[0][\"crew\"]\n",
      "309/32:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0]\n",
      "309/33:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset[\"crew\"][0]\n",
      "309/34:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "dataset['cast'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset[\"crew\"][0]\n",
      "309/35:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "dataset['cast'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset[\"crew\"][0]\n",
      "309/36:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset[\"crew\"][0]\n",
      "309/37:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0]\n",
      "309/38:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0][\"job\"]\n",
      "309/39:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "type(dataset[\"crew\"][0])\n",
      "309/40:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0]\n",
      "309/41:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0][0]\n",
      "309/42:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0][1]\n",
      "309/43:\n",
      "## dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0]\n",
      "309/44:\n",
      "## dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/45:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in tqdm(json_cols + ['belongs_to_collection']) :\n",
      "    train[col] = train[col].apply(lambda x : get_dictionary(x))\n",
      "    test[col] = test[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "def get_json_dict(df) :\n",
      "    global json_cols\n",
      "    result = dict()\n",
      "    for e_col in json_cols :\n",
      "        d = dict()\n",
      "        rows = df[e_col].values\n",
      "        for row in rows :\n",
      "            if row is None : continue\n",
      "            for i in row :\n",
      "                if i['name'] not in d :\n",
      "                    d[i['name']] = 0\n",
      "                d[i['name']] += 1\n",
      "        result[e_col] = d\n",
      "    return result\n",
      "\n",
      "train_dict = get_json_dict(dataset)\n",
      "309/46:\n",
      "from tqdm import tqdm\n",
      "\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in tqdm(json_cols + ['belongs_to_collection']) :\n",
      "    train[col] = train[col].apply(lambda x : get_dictionary(x))\n",
      "    test[col] = test[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "def get_json_dict(df) :\n",
      "    global json_cols\n",
      "    result = dict()\n",
      "    for e_col in json_cols :\n",
      "        d = dict()\n",
      "        rows = df[e_col].values\n",
      "        for row in rows :\n",
      "            if row is None : continue\n",
      "            for i in row :\n",
      "                if i['name'] not in d :\n",
      "                    d[i['name']] = 0\n",
      "                d[i['name']] += 1\n",
      "        result[e_col] = d\n",
      "    return result\n",
      "\n",
      "train_dict = get_json_dict(dataset)\n",
      "309/47:\n",
      "from tqdm import tqdm\n",
      "\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in tqdm(json_cols + ['belongs_to_collection']) :\n",
      "    train[col] = train[col].apply(lambda x : get_dictionary(x))\n",
      "    test[col] = test[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "def get_json_dict(df) :\n",
      "    global json_cols\n",
      "    result = dict()\n",
      "    for e_col in json_cols :\n",
      "        d = dict()\n",
      "        rows = df[e_col].values\n",
      "        for row in rows :\n",
      "            if row is None : continue\n",
      "            for i in row :\n",
      "                if i['name'] not in d :\n",
      "                    d[i['name']] = 0\n",
      "                d[i['name']] += 1\n",
      "        result[e_col] = d\n",
      "    return result\n",
      "\n",
      "train_dict = get_json_dict(dataset)\n",
      "309/48:\n",
      "from tqdm import tqdm\n",
      "\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in tqdm(json_cols + ['belongs_to_collection']) :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "def get_json_dict(df) :\n",
      "    global json_cols\n",
      "    result = dict()\n",
      "    for e_col in json_cols :\n",
      "        d = dict()\n",
      "        rows = df[e_col].values\n",
      "        for row in rows :\n",
      "            if row is None : continue\n",
      "            for i in row :\n",
      "                if i['name'] not in d :\n",
      "                    d[i['name']] = 0\n",
      "                d[i['name']] += 1\n",
      "        result[e_col] = d\n",
      "    return result\n",
      "\n",
      "train_dict = get_json_dict(dataset)\n",
      "309/49: train_dict\n",
      "309/50: dataset\n",
      "309/51: dataset[\"production_companies\"]\n",
      "309/52: dataset[\"production_companies\"][0]\n",
      "309/53: dataset[\"production_companies\"][0][0]\n",
      "309/54: dataset[\"production_companies\"][0][0][\"name\"]\n",
      "309/55: dataset[\"crew\"][0][0][\"name\"]\n",
      "309/56: dataset[\"crew\"][0]\n",
      "309/57: dataset[\"crew\"][0].index(lambda x: x[\"job\"] == \"Director\")\n",
      "309/58: next(x for x in dataset[\"crew\"][0] if x[\"job\"] == \"Director\")\n",
      "309/59: next(x[\"name\"] for x in dataset[\"crew\"][0] if x[\"job\"] == \"Director\")\n",
      "309/60: next(x[\"name\"] for x in dataset[\"crew\"][1] if x[\"job\"] == \"Director\")\n",
      "309/61:\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: next(y[\"name\"] for y in x[\"crew\"] if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/62:\n",
      "# dataset['director'] = dataset['crew'].apply(lambda x: next(y[\"name\"] for y in x[\"crew\"] if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/63:\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: next(y[\"name\"] for y in x[\"crew\"] if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/64:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: get_director(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "310/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "310/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "310/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "310/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "310/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "310/7:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "310/8:\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "310/9:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in json_cols + ['belongs_to_collection'] :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "310/10:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: get_director(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/11:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/12:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset['crew'].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/13:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/14:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "# dataset[\"crew\"].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset\n",
      "310/15:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset\n",
      "310/16:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"]#.apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset\n",
      "310/17:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"]#.apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/18:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: get_director(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/19:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [get_director(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/20:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: x[\"job\"])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/21:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: x[\"job\"][0])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/22:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(y[\"name\"] for y in data if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/23:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(y[\"name\"] for y in x if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/24:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(y[\"name\"] for y in x if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/25:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [y[\"name\"] for y in x if y[\"job\"] == \"Director\"])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/26:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(y[\"name\"] for y in x if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/27:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/28:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [y[\"name\"] for y in x if y[\"job\"] == \"Director\"])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/29:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [y[\"name\"] for y in x if y[\"job\"] == \"Director\"][0])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/30:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [y[\"name\"] for y in x if y[\"job\"] == \"Director\"])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/31:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/32: dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "310/33:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "310/34:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"director\"],prefix='director',drop_first=True)\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "310/35:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reżyser, scenariusz, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"director\"],prefix='director',drop_first=True)\n",
      "# X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "311/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "311/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "311/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "311/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "311/7:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "311/8:\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "311/9:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in json_cols + ['belongs_to_collection'] :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "311/10: dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "311/11:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset\n",
      "311/12: dataset\n",
      "311/13:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"director\"], prefix='director', drop_first=True)\n",
      "# X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/14:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "onehot_encoder = OneHotEncoder(sparse=False)\n",
      "onehot_encoded = onehot_encoder.fit_transform(dataset[\"director\"])\n",
      "dataset\n",
      "311/15:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "onehot_encoder = OneHotEncoder(sparse=False)\n",
      "onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "dataset\n",
      "311/16:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd\n",
      "311/17:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[\"director\"]\n",
      "311/18:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 9]\n",
      "311/19:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 10]\n",
      "311/20:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 11]\n",
      "311/21:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 12]\n",
      "311/22:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 13]\n",
      "311/23:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 14]\n",
      "311/24:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 20]\n",
      "311/25:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 25]\n",
      "311/26:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 23]\n",
      "311/27:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "onehot_encoder = OneHotEncoder(sparse=False)\n",
      "onehot_encoded = onehot_encoder.fit_transform(asd[:, 23])\n",
      "dataset\n",
      "311/28:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "onehot_encoder = OneHotEncoder(sparse=False)\n",
      "onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "dataset\n",
      "311/29:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "311/30:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "311/31:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "311/32:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset\n",
      "311/33:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/34:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "311/35:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/36:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director_*\"]].to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/37:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.* budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/38:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.* budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/39:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/40:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/41:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/42:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "y_train\n",
      "# X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/43:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -3))\n",
      "311/44:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "y_train\n",
      "# X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/45:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/46:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        s = classifier.score(xtest, ytest)\n",
      "        scores.append(s)\n",
      "    print(\"Score: \", s, \" for depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/47:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/48:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/49:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/50: pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "311/51:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset\n",
      "311/52:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset[\"cast\"]\n",
      "312/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "312/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "312/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "312/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "312/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "312/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "312/7:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "312/8:\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# # crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "312/9:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in json_cols + ['belongs_to_collection'] :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "312/10: dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "312/11:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset\n",
      "312/12:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -3))\n",
      "312/13:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "# depth_range = range(1, 15)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "312/14: pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "312/15:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset\n",
      "312/16:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset[\"cast\"]\n",
      "312/17:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset\n",
      "312/18:\n",
      "pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset\n",
      "312/19:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "dataset.cast.apply(ast.literal_eval)\n",
      "\n",
      "# depth_range = range(1, 15)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "312/20:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset\n",
      "dataset.cast.str.join('|').str.get_dummies().add_prefix('dummy_name_')\n",
      "312/21:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset\n",
      "# dataset.cast.str.join('|').str.get_dummies(drop_first=True).add_prefix('actor_')\n",
      "\n",
      "dataset.explode(\"cast\")\n",
      "312/22:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset\n",
      "# dataset.cast.str.join('|').str.get_dummies(drop_first=True).add_prefix('actor_')\n",
      "\n",
      "dataset.explode(\"cast\")[\"cast\"]\n",
      "312/23:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -5))\n",
      "dataset = dataset.explode(\"cast\")\n",
      "312/24:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget|cast\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "312/25:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "# dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -5))\n",
      "# dataset = dataset.explode(\"cast\")\n",
      "dataset = pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "312/26:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget|actor_.*\")).to_numpy() #aktorzy, reżyser, budżet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "313/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "313/2:\n",
      "\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "313/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "313/4:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "313/5:\n",
      "\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "313/6:\n",
      "\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "313/7:\n",
      "\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "313/8:\n",
      "\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "313/9:\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "313/10:\n",
      "\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "cast_names = cast_mapping.keys()\n",
      "cast_names\n",
      "for name in cast_names:\n",
      "    dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "313/11: %history -g\n",
      "313/12: %history\n",
      "313/13: %history -g\n",
      "   1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "   2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "   3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "   4:\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "   5:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "   6:\n",
      "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "   7: # dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "   8:\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "   9:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in json_cols + ['belongs_to_collection'] :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "\n",
      "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -5))\n",
      "# dataset.cast.str.join('|').str.get_dummies(drop_first=True).add_prefix('actor_')\n",
      "\n",
      "dataset = dataset.explode(\"cast\")\n",
      "dataset = pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "  10: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "X_train = dataset.filter(regex=(\"director_.*|budget|actor_.*\")).to_numpy() #aktorzy, reżyser, budżet\n",
    "y_train = dataset[[\"revenue\"]].to_numpy()\n",
    "\n",
    "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
    "X_train\n",
    "depth_range = range(1, 15)\n",
    "best_depth = 1\n",
    "max_score = 0\n",
    "for depth in depth_range:\n",
    "    scores = []\n",
    "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
    "    cv = KFold(n_splits=10)\n",
    "    for train_index, test_index in cv.split(X_train):\n",
    "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "        classifier.fit(xtrain, ytrain)\n",
    "        scores.append(classifier.score(xtest, ytest))\n",
    "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
    "    if np.mean(scores) > max_score:\n",
    "        max_score = np.mean(scores)\n",
    "        best_depth = depth\n",
    "print(\"Max score: \", max_score, \" for depth: \", best_depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}