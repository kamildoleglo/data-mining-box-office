{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import statsmodels\n",
    "from collections import Counter\n",
    "import cpi\n",
    "import ast\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_dict(df):\n",
    "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
    "    for column in dict_columns:\n",
    "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df\n",
    "\n",
    "def classify_movie_budget(budget):\n",
    "    if budget == 0:\n",
    "        return 'no data'\n",
    "    elif budget < 400000:\n",
    "        return 'micro-budget'\n",
    "    elif budget < 2000000:\n",
    "        return 'low-budget'\n",
    "    elif budget < 10000000:\n",
    "        return 'middle-budget'\n",
    "    else:\n",
    "        return 'high-budget'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     3000 non-null   int64  \n",
      " 1   belongs_to_collection  604 non-null    object \n",
      " 2   budget                 3000 non-null   int64  \n",
      " 3   genres                 2993 non-null   object \n",
      " 4   homepage               946 non-null    object \n",
      " 5   imdb_id                3000 non-null   object \n",
      " 6   original_language      3000 non-null   object \n",
      " 7   original_title         3000 non-null   object \n",
      " 8   overview               2992 non-null   object \n",
      " 9   popularity             3000 non-null   float64\n",
      " 10  poster_path            2999 non-null   object \n",
      " 11  production_companies   2844 non-null   object \n",
      " 12  production_countries   2945 non-null   object \n",
      " 13  release_date           3000 non-null   object \n",
      " 14  runtime                2998 non-null   float64\n",
      " 15  spoken_languages       2980 non-null   object \n",
      " 16  status                 3000 non-null   object \n",
      " 17  tagline                2403 non-null   object \n",
      " 18  title                  3000 non-null   object \n",
      " 19  Keywords               2724 non-null   object \n",
      " 20  cast                   2987 non-null   object \n",
      " 21  crew                   2984 non-null   object \n",
      " 22  revenue                3000 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(18)\n",
      "memory usage: 539.2+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
    "dataset_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset_original.copy()\n",
    "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
    "\n",
    "\n",
    "def fix_date(date):\n",
    "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
    "    if x.year > 2020:\n",
    "        year = x.year - 100\n",
    "    else:\n",
    "        year = x.year\n",
    "    return datetime.datetime(year,x.month,x.day)\n",
    "\n",
    "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
    "\n",
    "def adjust_price_to_inflation(price, date):\n",
    "    return int(cpi.inflate(price, date.year))\n",
    "\n",
    "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
    "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def map_list_to_integer(lists):\n",
    "    a = {}\n",
    "    for l in lists:\n",
    "        for name in l:\n",
    "            if(name not in a):\n",
    "                a[name] = len(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
    "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
    "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cast_names = cast_mapping.keys()\n",
    "# cast_names\n",
    "# for name in cast_names:\n",
    "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
    "\n",
    "def get_dictionary(s):\n",
    "    try:\n",
    "        d = eval(s)\n",
    "    except:\n",
    "        d = {}\n",
    "    return d\n",
    "\n",
    "for col in json_cols + ['belongs_to_collection'] :\n",
    "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
    "    \n",
    "dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
    "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
    "\n",
    "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -5))\n",
    "# dataset.cast.str.join('|').str.get_dummies(drop_first=True).add_prefix('actor_')\n",
    "\n",
    "dataset = dataset.explode(\"cast\")\n",
    "dataset = pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      " 1/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      " 1/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      " 1/4:\n",
      "train = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train.info()\n",
      " 1/5: train.head()\n",
      " 1/6: train.describe(include='all')\n",
      " 1/7: train.describe(include='all')\n",
      " 1/8:\n",
      "# Missing values \n",
      "train.isna().sum()\n",
      " 1/9:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "1/10:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "1/11:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "1/12:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/13:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"r\")\n",
      "plt.show()\n",
      "1/14:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"y\")\n",
      "plt.show()\n",
      "1/15:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"p\")\n",
      "plt.show()\n",
      "1/16:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"k\")\n",
      "plt.show()\n",
      "1/17:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"m\")\n",
      "plt.show()\n",
      "1/18:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/19:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", color=\"b\")\n",
      "plt.show()\n",
      "1/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"hex\", color=\"b\")\n",
      "plt.show()\n",
      "1/21:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", color=\"b\")\n",
      "plt.show()\n",
      "1/22:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", logistic=True, color=\"b\")\n",
      "plt.show()\n",
      "1/23:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "1/24:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "1/25:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", logistic=True, color=\"b\")\n",
      "plt.show()\n",
      "1/26:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", x_estimator=train.mean, color=\"b\")\n",
      "plt.show()\n",
      "1/27:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", x_estimator=train.info().mean, color=\"b\")\n",
      "plt.show()\n",
      "1/28:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", x_estimator=train.describe().mean, color=\"b\")\n",
      "plt.show()\n",
      "1/29:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, kind=\"reg\", color=\"b\")\n",
      "plt.show()\n",
      "1/30:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/31:\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/32:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, color=\"b\")\n",
      "plt.show()\n",
      "1/33:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, ratio=4, color=\"b\")\n",
      "plt.show()\n",
      "1/34:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "1/35:\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      " 2/1:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      " 2/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      " 2/3:\n",
      "\n",
      "train = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train.info()\n",
      " 2/4:\n",
      "\n",
      "train.head()\n",
      " 2/5:\n",
      "\n",
      "train.describe(include='all')\n",
      " 2/6:\n",
      "\n",
      "# Missing values \n",
      "train.isna().sum()\n",
      " 2/7:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      " 2/8:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      " 2/9:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "2/10:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "2/11:\n",
      "\n",
      "sns.distplot(train.revenue, height=11)\n",
      "2/12:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "1/36:\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "1/37:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/13:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "2/14:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/15:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "2/16:\n",
      "\n",
      "train = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train.info()\n",
      "2/17:\n",
      "\n",
      "train.head()\n",
      "2/18:\n",
      "\n",
      "train.describe(include='all')\n",
      "2/19:\n",
      "\n",
      "# Missing values \n",
      "train.isna().sum()\n",
      "2/20:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/21:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/22:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "2/23:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      " 3/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/ed'])\n",
      " 4/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 5/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 5/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      " 6/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 6/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      " 7/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 7/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      " 8/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 8/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      " 9/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      " 9/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "10/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "10/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "11/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "11/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "12/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "12/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "13/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "13/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "14/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "14/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "15/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "15/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "16/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "16/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "17/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "17/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "18/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "18/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "18/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(np.diff(r_peaks))\n",
      "print(np.average(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(np.diff(r_peaks))\n",
      "print(np.average(r_peaks/100))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(np.diff(r_peaks))\n",
      "print(np.average(r_peaks)/100)\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(np.diff(r_peaks))\n",
      "print(np.average(np.diff(r_peaks)))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \" + np.average(np.diff(r_peaks)))\n",
      "print(\"Heart rate:\" + np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "18/8:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Heart rate:\", np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "19/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "19/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "20/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "20/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "20/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "plt.cla()\n",
      "plt.plot(r_peaks, np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "20/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks = []\n",
      "for peak in peaksAll:\n",
      "    if ecg[peak] > 0.3:\n",
      "        r_peaks.append(peak)\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "plt.cla()\n",
      "plt.plot(r_peaks.drop(1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "20/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "# r_peaks = []\n",
      "# for peak in peaksAll:\n",
      "#     if ecg[peak] > 0.3:\n",
      "#         r_peaks.append(peak)\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "plt.cla()\n",
      "plt.plot(r_peaks.drop(1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "21/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "21/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "22/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "22/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "23/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "23/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "24/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "24/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "25/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "25/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "26/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "26/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "26/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "26/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "26/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks), \"rx\")\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "26/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "26/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", np.diff(r_peaks))\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "# todo: find PRT peaks\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "27/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "27/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "28/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "28/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "29/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "29/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "30/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "30/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "31/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "31/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "32/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "32/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "33/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "33/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "34/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "34/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "35/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "35/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "36/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "36/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "36/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", 60/np.diff(r_peaks)*1000)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "36/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "print(\"Avg heart rate: \", np.average(np.diff(r_peaks)))\n",
      "print(\"Inter-beat heart rate:\", 60/np.diff(r_peaks)*100)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), np.diff(r_peaks))\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "37/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "37/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(r_peaks, hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "# prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "# \n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "# \n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/8:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "# \n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "# \n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "prt_peaks, _ = find_peaks(ecg, height=[-0.3, 0.3])\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/9:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def moving_average(a, n=3):\n",
      "    ret = np.cumsum(a, dtype=float)\n",
      "    ret[n:] = ret[n:] - ret[:-n]\n",
      "    return ret[n - 1:] / n\n",
      "\n",
      "\n",
      "prt_peaks, _ = moving_average(ecg)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/10:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def moving_average(a, n=3):\n",
      "    ret = np.cumsum(a, dtype=float)\n",
      "    ret[n:] = ret[n:] - ret[:-n]\n",
      "    return ret[n - 1:] / n\n",
      "\n",
      "\n",
      "prt_peaks, _ = moving_average(ecg, 3)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/11:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks, _ = running_mean(ecg, 3)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/12:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/13:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "plot(prt_peaks)\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "37/14:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/2:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, prt_peaks, \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "prt_peaks = running_mean(ecg, 3)\n",
      "print(prt_peaks)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(prt_peaks, ecg[prt_peaks], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT = find_peaks(ecg_filt, height=0.2)\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/8:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      " 4/9:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/10:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/11:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/12:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/13:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 50)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/14:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 30)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/15:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 10)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/16:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/17:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 8)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/18:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 7)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/19:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/20:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/21:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 1)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/22:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 0.1)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/23:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 1)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/24:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/25:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 1)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/26:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.2)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/27:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.1)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/28:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.02)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/29:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=-0.02)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/30:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.02)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/31:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=0.02)\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/32:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/33:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "# r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "#\n",
      "# fig = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "#\n",
      "# hr = 60/np.diff(r_peaks)*100\n",
      "# print(\"Avg heart rate: \", np.average(hr))\n",
      "# print(\"Inter-beat heart rate:\", hr)\n",
      "# fig2 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "# plt.figure(figsize=(15,8))\n",
      "# plt.plot(hr)\n",
      "# plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "# plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "# plt.plot(ecg_filt, 'g')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/34:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 2)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/35:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 3)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/36:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/37:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "fig3 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/38:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/39:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.06, 0.2])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/40:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.1, 0.2])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/41:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.2, 0.3])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/42:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.2, 0.3])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "# todo: plot PRT peaks\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/43:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.2, 0.3])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "# todo: plot PRT peaks\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/44:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.2])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "# todo: plot PRT peaks\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "4/45:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "# filter\n",
      "# todo: filter with moving average\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60/np.diff(r_peaks)*100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# fig3 = plt.figure(figsize=(15, 8))\n",
      "# plt.plot(ecg, '#999999')\n",
      "# plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "\n",
      "# plot heart rate\n",
      "# todo: plot heart rate\n",
      "plt.figure(figsize=(15,8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "# todo: plot PRT peaks\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "38/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "38/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "38/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : DICOM\n",
      "\n",
      "import pydicom\n",
      "from tkinter import *\n",
      "from PIL import Image, ImageTk\n",
      "\n",
      "class MainWindow():\n",
      "\n",
      "    ds = pydicom.dcmread(\"./data/head.dcm\")\n",
      "    data = ds.pixel_array\n",
      "\n",
      "    def __init__(self, main):\n",
      "        # print patient name\n",
      "        print(self.ds.PatientName)\n",
      "\n",
      "        # prepare canvas\n",
      "        self.canvas = Canvas(main, width=512, height=512)\n",
      "        self.canvas.grid(row=0, column=0)\n",
      "        self.canvas.bind(\"<Button-1>\", self.initWindow)\n",
      "        self.canvas.bind(\"<B1-Motion>\", self.updateWindow)\n",
      "        self.canvas.bind(\"<Button-3>\", self.initMeasure)\n",
      "        self.canvas.bind(\"<B3-Motion>\", self.updateMeasure)\n",
      "        self.canvas.bind(\"<ButtonRelease-3>\", self.finishMeasure)\n",
      "\n",
      "        # load image\n",
      "        # todo: apply transform\n",
      "        #self.array = self.transformData(self.data, self.winWidth, self.winCenter)\n",
      "        self.array = self.data\n",
      "        self.image = Image.fromarray(self.array)\n",
      "        self.image = self.image.resize((512, 512), Image.ANTIALIAS)\n",
      "        self.img = ImageTk.PhotoImage(image=self.image, master=root)\n",
      "        self.image_on_canvas = self.canvas.create_image(0, 0, anchor = NW, image = self.img)\n",
      "\n",
      "\n",
      "    def transformData(self, data, window, level):\n",
      "        # todo: transform data\n",
      "        return data\n",
      "\n",
      "\n",
      "    def initWindow(self, event):\n",
      "        # todo: save mouse position\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def updateWindow(self, event):\n",
      "        # todo: modify window width and center\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "        #self.array2 = self.transformData(self.data, self.winWidth, self.winCenter)\n",
      "        #self.image2 = Image.fromarray(self.array2)\n",
      "        #self.image2 = self.image2.resize((512, 512), Image.ANTIALIAS)\n",
      "        #self.img2 = ImageTk.PhotoImage(image=self.image2, master=root)\n",
      "        #self.canvas.itemconfig(self.image_on_canvas, image = self.img2)\n",
      "\n",
      "\n",
      "    def initMeasure(self, event):\n",
      "        # todo: save mouse position\n",
      "        # todo: create line\n",
      "        # hint: self.canvas.create_line(...)\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def updateMeasure(self, event):\n",
      "        # todo: update line\n",
      "        # hint: self.canvas.coords(...)\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def finishMeasure(self, event):\n",
      "        # todo: print measured length in mm\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "\n",
      "\n",
      "#----------------------------------------------------------------------\n",
      "\n",
      "root = Tk()\n",
      "MainWindow(root)\n",
      "root.mainloop()\n",
      "39/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "39/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_ecg.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "40/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "40/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "41/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "41/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "42/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "42/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "43/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "43/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "44/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "44/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "45/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "45/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "46/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "46/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "47/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "47/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "48/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "48/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "49/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "49/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "50/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "50/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "51/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "51/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "52/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "52/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "53/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "53/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "54/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "54/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "55/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "56/1:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      " %matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "56/2:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/3:\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      " %matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "56/4:\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "56/5:\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/6:\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/7:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "56/8:\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/9:\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "56/10:\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "56/11:\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/12:\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/13:\n",
      "# NumPy - przegldanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "56/14:\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem rwnym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "56/15:\n",
      "filename = 'iris.csv'\n",
      "dataset = csv.read\n",
      "57/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "56/16:\n",
      "filename = 'iris.csv'\n",
      "dataset = csv.reader(filename, delimiter=',')\n",
      "56/17:\n",
      "filename = 'iris.csv'\n",
      "dataset = pd.read_csv(filename, delimiter=',')\n",
      "56/18:\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "56/19: print(dataset.size)\n",
      "56/20: dataset.head(20)\n",
      "56/21:\n",
      "# descriptions\n",
      "print(dataset.description())\n",
      "56/22:\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "56/23:\n",
      "# class distribution\n",
      "print(dataset.groupby(variety).size())\n",
      "56/24:\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "56/25:\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "56/26:\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "56/27:\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "56/28:\n",
      "array = dataset.values\n",
      "X = array[0]\n",
      "Y = array[1]\n",
      "56/29:\n",
      "array = dataset.values\n",
      "X = array[0]\n",
      "Y = array[1]\n",
      "X\n",
      "56/30:\n",
      "array = dataset.values\n",
      "X = array[:,0]\n",
      "Y = array[1]\n",
      "X\n",
      "56/31:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[1]\n",
      "X\n",
      "56/32:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[1]\n",
      "array\n",
      "56/33:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[1]\n",
      "X\n",
      "56/34:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "Y\n",
      "56/35:\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "56/36:\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "56/37:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = x[0.8:, :]\n",
      "56/38:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[0.8:, :]\n",
      "56/39:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[0.8:]\n",
      "56/40:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[x[:80,:], x[80:,:]]\n",
      "56/41:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:]\n",
      "56/42:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80,:]. Y[80:,:]\n",
      "56/43:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80]. Y[80:]\n",
      "56/44:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "56/45:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "X_train.size()\n",
      "56/46:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "X_train\n",
      "56/47:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "X_train.size\n",
      "56/48:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:80,:], X[80:,:], Y[:80], Y[80:]\n",
      "X_test.size\n",
      "56/49:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "56/50:\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "56/51:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "Y_train.size\n",
      "56/52:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "Y_test.size\n",
      "56/53:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "56/54:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "Y.size\n",
      "56/55:\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = X[:120,:], X[120:,:], Y[:120], Y[120:]\n",
      "56/56:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "56/57:\n",
      "\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/58:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/59:\n",
      "\n",
      "# NumPy - przegldanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "56/60:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem rwnym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "56/61:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "56/62:\n",
      "\n",
      "print(dataset.size)\n",
      "56/63:\n",
      "\n",
      "dataset.head(20)\n",
      "56/64:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "56/65:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "56/66:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "56/67:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "56/68:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "56/69:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "56/70:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "56/71:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_seed()\n",
      "56/72:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "56/73:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "56/74:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "56/75:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = getattr(name)()\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "names = [\"M1\", \"M2\", \"M3\"]\n",
      "results = [0.5, 0.6, 0.7]\n",
      "56/76:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = getattr(self,name)()\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "names = [\"M1\", \"M2\", \"M3\"]\n",
      "results = [0.5, 0.6, 0.7]\n",
      "56/77:\n",
      "\n",
      "names = [LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = name\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "names = [\"M1\", \"M2\", \"M3\"]\n",
      "results = [0.5, 0.6, 0.7]\n",
      "56/78:\n",
      "\n",
      "names = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = name\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "names = [\"M1\", \"M2\", \"M3\"]\n",
      "results = [0.5, 0.6, 0.7]\n",
      "56/79:\n",
      "\n",
      "names = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for name in names:\n",
      "    klr = name\n",
      "    klr.fit(X_train, Y_train)\n",
      "    predictions = klr.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "print(results)\n",
      "56/80:\n",
      "\n",
      "names = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in names:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "# print(classification_report(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "56/81:\n",
      "\n",
      "names = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in names:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "56/82:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "56/83:\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "56/84:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "56/85:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "56/86:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "56/87:\n",
      "\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "56/88:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "56/89:\n",
      "\n",
      "# NumPy - przegldanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "56/90:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem rwnym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "56/91:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "56/92:\n",
      "\n",
      "print(dataset.size)\n",
      "56/93:\n",
      "\n",
      "dataset.head(20)\n",
      "56/94:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "56/95:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "56/96:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "56/97:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "56/98:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "56/99:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "56/100:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "56/101:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "56/102:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "56/103:\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "56/104:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "56/105:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "58/1:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "58/2:\n",
      "\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "58/3:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "58/4:\n",
      "\n",
      "# NumPy - przegldanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "58/5:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem rwnym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "58/6:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "58/7:\n",
      "\n",
      "print(dataset.size)\n",
      "58/8:\n",
      "\n",
      "dataset.head(20)\n",
      "58/9:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "58/10:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "58/11:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "58/12:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "58/13:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "58/14:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "58/15:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "58/16:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "58/17:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "58/18:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "58/19:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "58/20:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "58/21:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "60/1:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "60/2:\n",
      "\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "60/3:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "60/4:\n",
      "\n",
      "# NumPy - przegldanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "60/5:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem rwnym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "60/6:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "60/7:\n",
      "\n",
      "print(dataset.size)\n",
      "60/8:\n",
      "\n",
      "dataset.head(20)\n",
      "60/9:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "60/10:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "60/11:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "60/12:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "60/13:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "60/14:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "60/15:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "60/16:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "60/17:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "60/18:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "60/19:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "60/20:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/1:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "61/2:\n",
      "\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "61/3:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "61/4:\n",
      "\n",
      "# NumPy - przegldanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "61/5:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem rwnym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "61/6:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "61/7:\n",
      "\n",
      "print(dataset.size)\n",
      "61/8:\n",
      "\n",
      "dataset.head(20)\n",
      "61/9:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "61/10:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "61/11:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "61/12:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "61/13:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "61/14:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "61/15:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "61/16:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "61/17:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/18:\n",
      "\n",
      "names = [\"LogisticRegression\", \"KNeighborsClassifier\", \"DecisionTreeClassifier\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/19:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/20:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "62/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/Lab0-student'])\n",
      "61/21:\n",
      "\n",
      "names = [\"LR\", \"KNeighbors\", \"DecisionTree\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/22:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/23:\n",
      "\n",
      "# Konfiguracja interakcji z wykresami pakietu matplotlib\n",
      "\n",
      "# Standardowe rozwizanie\n",
      "%matplotlib inline\n",
      "\n",
      "# Interaktywne wykresy - moe powodowa bdy\n",
      "# %matplotlib notebook\n",
      "61/24:\n",
      "\n",
      "# Zaaduj biblioteki\n",
      "from pandas import read_csv\n",
      "from matplotlib import pyplot\n",
      "from pandas.plotting import scatter_matrix\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "import numpy as np\n",
      "61/25:\n",
      "\n",
      "# NumPy - tworzenie tablic\n",
      "\n",
      "a = np.array([1,2,3])\n",
      "b = np.array([(1.5,2,3), (4,5,6)], dtype = float)\n",
      "c = np.array([[(1.5,2,3), (4,5,6)], [(3,2,1), (4,5,6)]], dtype = float)\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "print(c)\n",
      "61/26:\n",
      "\n",
      "# NumPy - przegldanie tablic\n",
      "\n",
      "print(a.shape)\n",
      "print(len(a))\n",
      "print(b.ndim)\n",
      "print(c.size)\n",
      "print(b.dtype)\n",
      "print(b.dtype.name)\n",
      "print(b.astype(int))\n",
      "61/27:\n",
      "\n",
      "# NumPy - subsetting\n",
      "\n",
      "print(a[2]) #wybierz element z indeksem rwnym 2\n",
      "print(b[1,2]) # wybierz element z pierwszego wiersza i drugiej kolumny\n",
      "\n",
      "# NumPy - slicing\n",
      "print(a[0:2])   #wybierz elementy o indeksach 0 i 1\n",
      "print(b[0:2,1]) #wybierz elementy z wiersza 0 i 1 oraz kolumny 1\n",
      "\n",
      "print(b[:1])    #wybierz elementy z wiersza 0 \n",
      "print(b[0:1,:]) #wybierz elementy z wiersza 0\n",
      "61/28:\n",
      "\n",
      "filename = 'iris.csv'\n",
      "dataset = read_csv(filename, delimiter=',')\n",
      "61/29:\n",
      "\n",
      "print(dataset.size)\n",
      "61/30:\n",
      "\n",
      "dataset.head(20)\n",
      "61/31:\n",
      "\n",
      "# descriptions\n",
      "print(dataset.describe(include='all'))\n",
      "61/32:\n",
      "\n",
      "# class distribution\n",
      "print(dataset.groupby(dataset.variety).size())\n",
      "61/33:\n",
      "\n",
      "dataset.hist()\n",
      "pyplot.show()\n",
      "61/34:\n",
      "\n",
      "dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
      "pyplot.show()\n",
      "61/35:\n",
      "\n",
      "scatter_matrix(dataset)\n",
      "pyplot.show()\n",
      "61/36:\n",
      "\n",
      "array = dataset.values\n",
      "X = array[:,0:4]\n",
      "Y = array[:,4]\n",
      "61/37:\n",
      "\n",
      "print(X.shape)\n",
      "print(X)\n",
      "\n",
      "print(Y.shape)\n",
      "print(Y)\n",
      "61/38:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "61/39:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/40:\n",
      "\n",
      "names = [\"LR\", \"KNeighbors\", \"DecisionTree\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/41:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/42:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/43:\n",
      "\n",
      "validation_size = 0.20\n",
      "seed = 7\n",
      "\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
      "61/44:\n",
      "\n",
      "klr = LogisticRegression(max_iter=2000)\n",
      "klr.fit(X_train, Y_train)\n",
      "predictions = klr.predict(X_test)\n",
      "\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "61/45:\n",
      "\n",
      "names = [\"LR\", \"KNeighbors\", \"DecisionTree\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/46:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/47:\n",
      "\n",
      "names = [\"LR\", \"KNeighbors\", \"DecisionTree\", \"SVC\", \"GaussianNB\"]\n",
      "classifiers = [LogisticRegression(max_iter=2000), KNeighborsClassifier(), DecisionTreeClassifier(), SVC(), GaussianNB()]\n",
      "results = []\n",
      "\n",
      "for cls in classifiers:\n",
      "    cls.fit(X_train, Y_train)\n",
      "    predictions = cls.predict(X_test)\n",
      "    results.append(accuracy_score(Y_test, predictions))\n",
      "\n",
      "#TODO zaimplementuj uczenie i ocenianie wybranych algorytmw\n",
      "#TODO wyniki zawrzyj w tablicach names i results\n",
      "\n",
      "print(names)\n",
      "print(results)\n",
      "61/48:\n",
      "\n",
      "pyplot.bar(names,results)\n",
      "61/49:\n",
      "\n",
      "best = KNeighborsClassifier()\n",
      "best.fit(X_train, Y_train)\n",
      "predictions = best.predict(X_test)\n",
      "\n",
      "#TODO accuracy_score\n",
      "#TODO confusion_matrix\n",
      "#TODO classification_report\n",
      "print(accuracy_score(Y_test, predictions))\n",
      "print(confusion_matrix(Y_test, predictions))\n",
      "print(classification_report(Y_test, predictions))\n",
      "63/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "63/2:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/3:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/4:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/5:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/6:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/7:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/8:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 5.5)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/9:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 4)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/10:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "fig2 = plt.figure(figsize=(15, 8))\n",
      "plt.plot(np.resize(r_peaks, r_peaks.size - 1), hr)\n",
      "plt.show()\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 6)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "63/11:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : ECG (Electrocardiography)\n",
      "# Data sampling frequency : 100 Hz\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.signal import find_peaks\n",
      "\n",
      "# params\n",
      "max_samples = 1000\n",
      "\n",
      "# read data\n",
      "ecg = np.zeros(max_samples)\n",
      "with open('./data/ecg.data') as csv_file:\n",
      "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
      "    idx = 0\n",
      "    for row in csv_reader:\n",
      "        if 0 < idx < max_samples:\n",
      "            ecg[idx] = float(row[0])\n",
      "        idx = idx + 1\n",
      "\n",
      "\n",
      "# filter\n",
      "def running_mean(x, N):\n",
      "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
      "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
      "\n",
      "\n",
      "# find peaks\n",
      "peaksAll, _ = find_peaks(ecg)\n",
      "print(peaksAll)\n",
      "\n",
      "# heart rate\n",
      "r_peaks, _ = find_peaks(ecg, height=0.3)\n",
      "\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(r_peaks, ecg[r_peaks], \"rx\")\n",
      "plt.show()\n",
      "\n",
      "hr = 60 / np.diff(r_peaks) * 100\n",
      "print(\"Avg heart rate: \", np.average(hr))\n",
      "print(\"Inter-beat heart rate:\", hr)\n",
      "\n",
      "# PRT peaks\n",
      "ecg_filt = running_mean(ecg, 6)\n",
      "peaksPRT, _ = find_peaks(ecg_filt, height=[0.02, 0.3])\n",
      "\n",
      "# plot heart rate\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(hr)\n",
      "plt.show()\n",
      "\n",
      "# # plot ECG\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(peaksAll, ecg[peaksAll], \"rx\")\n",
      "plt.show()\n",
      "plt.figure(figsize=(15, 8))\n",
      "plt.plot(ecg, '#999999')\n",
      "plt.plot(ecg_filt, 'g')\n",
      "plt.plot(peaksPRT, ecg_filt[peaksPRT], \"rx\")\n",
      "plt.show()\n",
      "64/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "64/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "65/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "65/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "66/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "66/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "67/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "67/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "68/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "68/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "69/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "69/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "70/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "70/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "71/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "71/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "72/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "72/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "73/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "73/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "74/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "75/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "75/2:\n",
      "# AGH UST Medical Informatics 03.2020\n",
      "# Lab 1 : DICOM\n",
      "\n",
      "import pydicom\n",
      "from tkinter import *\n",
      "from PIL import Image, ImageTk\n",
      "\n",
      "\n",
      "class MainWindow():\n",
      "    ds = pydicom.dcmread(\"./data/head.dcm\")\n",
      "    data = ds.pixel_array\n",
      "    winWidth = ds.WindowWidth\n",
      "    winCenter = ds.WindowCenter\n",
      "    fromX = 0\n",
      "    fromY = 0\n",
      "    measurementFromX = 0\n",
      "    measurementFromY = 0\n",
      "\n",
      "    def __init__(self, main):\n",
      "        # print patient name\n",
      "        print(self.ds.PatientName)\n",
      "\n",
      "        # prepare canvas\n",
      "        self.canvas = Canvas(main, width=512, height=512)\n",
      "        self.canvas.grid(row=0, column=0)\n",
      "        self.canvas.bind(\"<Button-1>\", self.initWindow)\n",
      "        self.canvas.bind(\"<B1-Motion>\", self.updateWindow)\n",
      "        self.canvas.bind(\"<Button-3>\", self.initMeasure)\n",
      "        self.canvas.bind(\"<B3-Motion>\", self.updateMeasure)\n",
      "        self.canvas.bind(\"<ButtonRelease-3>\", self.finishMeasure)\n",
      "        self.canvas.bind(\"\")\n",
      "\n",
      "        # load image\n",
      "        self.array = self.transformData(self.data, self.winWidth, self.winCenter)\n",
      "        self.image = Image.fromarray(self.array)\n",
      "        self.image = self.image.resize((512, 512), Image.ANTIALIAS)\n",
      "        self.img = ImageTk.PhotoImage(image=self.image, master=root)\n",
      "        self.image_on_canvas = self.canvas.create_image(0, 0, anchor=NW, image=self.img)\n",
      "\n",
      "    def transformData(self, data, window, level):\n",
      "        start = level - window / 2\n",
      "        return (data - start) / window * 255\n",
      "\n",
      "    def initWindow(self, event):\n",
      "        self.fromX = event.x\n",
      "        self.fromY = event.y\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def updateWindow(self, event):\n",
      "        self.winCenter -= event.x - self.fromX\n",
      "        self.winWidth -= event.y - self.fromY\n",
      "        self.winCenter = max(self.winCenter, 0)\n",
      "        self.fromX = event.x\n",
      "        self.fromY = event.y\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "        self.array2 = self.transformData(self.data, self.winWidth, self.winCenter)\n",
      "        self.image2 = Image.fromarray(self.array2)\n",
      "        self.image2 = self.image2.resize((512, 512), Image.ANTIALIAS)\n",
      "        self.img2 = ImageTk.PhotoImage(image=self.image2, master=root)\n",
      "        self.canvas.itemconfig(self.image_on_canvas, image = self.img2)\n",
      "\n",
      "    def initMeasure(self, event):\n",
      "        # todo: save mouse position\n",
      "        self.measurementFromX = event.x\n",
      "        self.measurementFromY = event.y\n",
      "        # todo: create line\n",
      "        self.canvas.create_line(event.x, event.y)\n",
      "        # hint: self.canvas.create_line(...)\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def updateMeasure(self, event):\n",
      "        # todo: update line\n",
      "        self.canvas.create_line(self.measurementFromX, self.measurementFromX, event.x, event.y)\n",
      "        # hint: self.canvas.coords(...)\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "    def finishMeasure(self, event):\n",
      "        # todo: print measured length in mm\n",
      "        print(\"x: \" + str(event.x) + \" y: \" + str(event.y))\n",
      "\n",
      "\n",
      "# ----------------------------------------------------------------------\n",
      "\n",
      "root = Tk()\n",
      "MainWindow(root)\n",
      "root.mainloop()\n",
      "76/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "77/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "77/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "78/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "78/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "79/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "79/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "80/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "80/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "81/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "81/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "82/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "82/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "83/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "83/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "84/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "84/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "85/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "85/2: runfile('/Users/kamildoleglo/Downloads/im_lab_1/lab1_dicom.py', wdir='/Users/kamildoleglo/Downloads/im_lab_1')\n",
      "86/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab_1'])\n",
      "87/1:\n",
      "\n",
      "print(__doc__)\n",
      "87/2:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "87/3:\n",
      "\n",
      "df = pd.read_csv('./nyt-frame.csv', header = 0)\n",
      "87/4:\n",
      "\n",
      "print(__doc__)\n",
      "87/5:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "87/6:\n",
      "\n",
      "df = pd.read_csv('./nyt-frame.csv', header = 0)\n",
      "87/7:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "87/8:\n",
      "\n",
      "print(original_headers)\n",
      "87/9:\n",
      "\n",
      "df\n",
      "87/10:\n",
      "\n",
      "df.loc[:,'class.labels']\n",
      "87/11:\n",
      "\n",
      "df.size\n",
      "87/12:\n",
      "\n",
      "df.shape\n",
      "87/13:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "87/14:\n",
      "\n",
      "df.shape\n",
      "87/15:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "87/16:\n",
      "\n",
      "array = data.as_matrix()\n",
      "87/17:\n",
      "\n",
      "array = data.values\n",
      "87/18:\n",
      "\n",
      "array.shape\n",
      "87/19:\n",
      "\n",
      "print(__doc__)\n",
      "87/20:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "87/21:\n",
      "\n",
      "df = pd.read_csv('./nyt-frame.csv', header = 0)\n",
      "87/22:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "87/23:\n",
      "\n",
      "print(original_headers)\n",
      "87/24:\n",
      "\n",
      "df\n",
      "87/25:\n",
      "\n",
      "df.loc[:,'class.labels']\n",
      "87/26:\n",
      "\n",
      "df.size\n",
      "87/27:\n",
      "\n",
      "df.shape\n",
      "87/28:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "87/29:\n",
      "\n",
      "df.shape\n",
      "87/30:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "87/31:\n",
      "\n",
      "array = data.values\n",
      "87/32:\n",
      "\n",
      "array.shape\n",
      "87/33:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "87/34:\n",
      "\n",
      "pca = PCA()\n",
      "87/35:\n",
      "\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "87/36:\n",
      "\n",
      "y = df.loc[:,'class.labels']\n",
      "reds = y == 'art'\n",
      "blues = y == 'music'\n",
      "87/37:\n",
      "\n",
      "header = list(df.columns.values[9:])\n",
      "sample_word = np.random.choice(header, 20, replace=False)\n",
      "print(sample_word)\n",
      "87/38:\n",
      "\n",
      "indices = np.random.choice(array.shape[0], 2, replace=False)\n",
      "sample = array[indices,:]\n",
      "87/39:\n",
      "\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(reds), 0], X_pca[np.array(reds), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.scatter(X_pca[np.array(blues), 0], X_pca[np.array(blues), 1], c=\"blue\",s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "87/40:\n",
      "\n",
      "k = 15\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "sorted = list(np.argsort(pc2))\n",
      "print('----------PC2-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "87/41:\n",
      "\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "87/42:\n",
      "\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:10]))\n",
      "88/1:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/2:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "88/3:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/4:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "88/5:\n",
      "\n",
      "print(original_headers)\n",
      "88/6:\n",
      "\n",
      "df\n",
      "88/7:\n",
      "\n",
      "df.loc[:,'class.labels']\n",
      "88/8:\n",
      "\n",
      "print(__doc__)\n",
      "88/9:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "88/10:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/11:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "88/12:\n",
      "\n",
      "print(original_headers)\n",
      "88/13:\n",
      "\n",
      "df\n",
      "88/14:\n",
      "\n",
      "df.size\n",
      "88/15:\n",
      "\n",
      "df.shape\n",
      "88/16:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "88/17:\n",
      "\n",
      "df.shape\n",
      "88/18:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "88/19:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "data_scaled.mean(axis=0)\n",
      "88/20:\n",
      "\n",
      "array = data_scaled.values\n",
      "88/21: array = data_scaled\n",
      "88/22: array = data_scaled\n",
      "88/23: array.shape\n",
      "88/24:\n",
      "\n",
      "print(__doc__)\n",
      "88/25:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "88/26:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/27:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "88/28:\n",
      "\n",
      "print(original_headers)\n",
      "88/29:\n",
      "\n",
      "df\n",
      "88/30:\n",
      "\n",
      "df.size\n",
      "88/31:\n",
      "\n",
      "df.shape\n",
      "88/32:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "88/33:\n",
      "\n",
      "df.shape\n",
      "88/34:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "88/35:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "data_scaled.mean(axis=0)\n",
      "88/36:\n",
      "\n",
      "array = data_scaled.values\n",
      "88/37:\n",
      "\n",
      "print(__doc__)\n",
      "88/38:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "88/39:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "88/40:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "88/41:\n",
      "\n",
      "print(original_headers)\n",
      "88/42:\n",
      "\n",
      "df\n",
      "88/43:\n",
      "\n",
      "df.size\n",
      "88/44:\n",
      "\n",
      "df.shape\n",
      "88/45:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "88/46:\n",
      "\n",
      "df.shape\n",
      "88/47:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "88/48:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "data_scaled.mean(axis=0)\n",
      "88/49:\n",
      "\n",
      "array = data_scaled\n",
      "88/50:\n",
      "\n",
      "array.shape\n",
      "88/51:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "88/52:\n",
      "\n",
      "pca = PCA()\n",
      "88/53:\n",
      "\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "88/54:\n",
      "\n",
      "y = df.loc[:,'class.labels']\n",
      "reds = y == 'art'\n",
      "blues = y == 'music'\n",
      "88/55:\n",
      "header = list(df.columns.values[9:])\n",
      "sample_word = np.random.choice(header, 20, replace=False)\n",
      "print(sample_word)\n",
      "88/56: data_scaled.mean(axis=0)\n",
      "88/57: data_scaled.variance(axis=0)\n",
      "88/58: data_scaled.var(axis=0)\n",
      "88/59:\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "88/60:\n",
      "\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "88/61:\n",
      "\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:10]))\n",
      "88/62: data_scaled.var(axis=0)\n",
      "88/63: data_scaled.var(axis=0)\n",
      "88/64: array = data_scaled\n",
      "88/65: array.shape\n",
      "88/66:\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "88/67:\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "88/68:\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:10]))\n",
      "88/69:\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:2]))\n",
      "print(sum(variance_ratio[0:3]))\n",
      "88/70:\n",
      "k = 15\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/71:\n",
      "k = 10\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/72:\n",
      "k = 11\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/73:\n",
      "k = 12\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/74:\n",
      "k = 11\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/75:\n",
      "k = 11\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "88/76:\n",
      "pc1 = pca.components_[0]\n",
      "3\n",
      "pc2 = pca.components_[1]\n",
      "print(Attribute, PC1, PC2)\n",
      "for i in range(0,pc1.shape[0]):\n",
      "print(attributes[i] + : + repr(pc1[i]) + : + repr(pc2[i]))\n",
      "88/77:\n",
      "pc1 = pca.components_[0]\n",
      "3\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/78:\n",
      "pc1 = pca.components_[0]\n",
      "3\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/79:\n",
      "k = 2\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "sorted = list(np.argsort(pc2))\n",
      "print('----------PC2-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "88/80:\n",
      "k = 3\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "sorted = list(np.argsort(pc2))\n",
      "print('----------PC2-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "88/81:\n",
      "pc1 = pca.components_[0]\n",
      "3\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/82:\n",
      "pc1 = pca.components_[0]\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/83: y = df.loc[:,'Vehicle Name']\n",
      "89/1:\n",
      "\n",
      "print(__doc__)\n",
      "89/2:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "89/3:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "89/4:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "89/5:\n",
      "\n",
      "print(original_headers)\n",
      "89/6:\n",
      "\n",
      "df\n",
      "89/7:\n",
      "\n",
      "df.size\n",
      "89/8:\n",
      "\n",
      "df.shape\n",
      "89/9:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "89/10:\n",
      "\n",
      "df.shape\n",
      "89/11:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "89/12:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "89/13:\n",
      "\n",
      "data_scaled.mean(axis=0)\n",
      "89/14:\n",
      "\n",
      "data_scaled.var(axis=0)\n",
      "89/15:\n",
      "\n",
      "array = data_scaled\n",
      "89/16:\n",
      "\n",
      "array.shape\n",
      "89/17:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "89/18:\n",
      "\n",
      "pca = PCA()\n",
      "89/19:\n",
      "\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "89/20:\n",
      "\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "89/21:\n",
      "\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:2]))\n",
      "print(sum(variance_ratio[0:3]))\n",
      "89/22:\n",
      "\n",
      "pc1 = pca.components_[0]\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/84: y = df.loc[:,'Vehicle Name']\n",
      "88/85: y = df.loc[:,'Vehicle Name']\n",
      "88/86:\n",
      "y = df.loc[:,'Vehicle Name']\n",
      "print(y)\n",
      "88/87: y = df.loc[:,'Vehicle Name']\n",
      "88/88:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(y), 0], X_pca[np.array(y), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/89:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(true), 0], X_pca[np.array(y), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/90:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(True), 0], X_pca[np.array(y), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/91:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(True), 0], X_pca[np.array(True), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "89/23:\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "88/92:\n",
      "x = X_pca[:,0]\n",
      "y = X_pca[:,1]\n",
      "88/93:\n",
      "plt.figure()\n",
      "plt.scatter(X_pca[np.array(True), 0], X_pca[np.array(y), 1], c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/94:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "plt.show()\n",
      "88/95:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "\n",
      "for i, txt in enumerate(n):\n",
      "    ax.annotate(txt, (z[i], y[i]))\n",
      "\n",
      "plt.show()\n",
      "88/96:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(z, y)\n",
      "for i, txt in enumerate(n):\n",
      "    ax.annotate(txt, (z[i], y[i]))\n",
      "\n",
      "plt.show()\n",
      "88/97:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(z, y)\n",
      "for i, txt in enumerate(labels):\n",
      "    ax.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "88/98:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "for i, txt in enumerate(labels):\n",
      "    ax.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "88/99:\n",
      "plt.figure()\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "for i, txt in enumerate(labels):\n",
      "    plt.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "88/100:\n",
      "plt.figure(figsize=(30, 20))\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "for i, txt in enumerate(labels):\n",
      "    plt.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "88/101:\n",
      "def biplot(score,coeff,pcax,pcay,labels=None):\n",
      "    pca1=pcax-1\n",
      "    pca2=pcay-1\n",
      "    xs = score[:,pca1]\n",
      "    ys = score[:,pca2]\n",
      "    n=score.shape[1]\n",
      "    scalex = 1.0/(xs.max()- xs.min())\n",
      "    scaley = 1.0/(ys.max()- ys.min())\n",
      "    plt.scatter(xs*scalex,ys*scaley)\n",
      "    for i in range(n):\n",
      "        plt.arrow(0, 0, coeff[i,pca1], coeff[i,pca2],color='r',alpha=0.5) \n",
      "        if labels is None:\n",
      "            plt.text(coeff[i,pca1]* 1.15, coeff[i,pca2] * 1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
      "        else:\n",
      "            plt.text(coeff[i,pca1]* 1.15, coeff[i,pca2] * 1.15, labels[i], color='g', ha='center', va='center')\n",
      "    plt.xlim(-1,1)\n",
      "    plt.ylim(-1,1)\n",
      "    plt.xlabel(\"PC{}\".format(pcax))\n",
      "    plt.ylabel(\"PC{}\".format(pcay))\n",
      "    plt.grid()\n",
      "88/102: biplot(score,pca.components_,1,2,labels=categories)\n",
      "88/103: biplot(y,X_pca.components_,1,2,labels=categories)\n",
      "88/104: biplot(x, y,1,2,labels=labels)\n",
      "88/105: biplot(X_pca,x,1,2,labels=labels)\n",
      "88/106: biplot(X_pca,y,1,2,labels=labels)\n",
      "88/107: biplot(X_pca,X_pca,1,2,labels=labels)\n",
      "88/108: biplot(data,pca.components_,1,2,labels=labels)\n",
      "88/109:\n",
      "def myplot(score,coeff,labels=None):\n",
      "    plt.figure(figsize=(15,10))\n",
      "    plt.xlabel(\"PC{}\".format(1))\n",
      "    plt.ylabel(\"PC{}\".format(2))\n",
      "    xs = score[:,0]\n",
      "    ys = score[:,1]\n",
      "    n = coeff.shape[0]\n",
      "    scalex = 1.0/(xs.max() - xs.min())\n",
      "    scaley = 1.0/(ys.max() - ys.min())\n",
      "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
      "    for i in range(n):\n",
      "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
      "        if labels is None:\n",
      "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
      "        else:\n",
      "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
      "\n",
      "print(pca.components_[0:2, :])\n",
      "myplot(X_pca[:,0:2],np.transpose(pca.components_[0:2, :]),attributes)\n",
      "axes = plt.gca()\n",
      "axes.set_ylim([-1.0,1.0])\n",
      "plt.show()\n",
      "89/24:\n",
      "\n",
      "print(__doc__)\n",
      "89/25:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "import pandas as pd\n",
      "89/26:\n",
      "\n",
      "df = pd.read_csv('./04cars-data.csv', header = 0)\n",
      "89/27:\n",
      "\n",
      "original_headers = list(df.columns.values)\n",
      "89/28:\n",
      "\n",
      "print(original_headers)\n",
      "89/29:\n",
      "\n",
      "df\n",
      "89/30:\n",
      "\n",
      "df.size\n",
      "89/31:\n",
      "\n",
      "df.shape\n",
      "89/32:\n",
      "\n",
      "dff = df._get_numeric_data()\n",
      "89/33:\n",
      "\n",
      "df.shape\n",
      "89/34:\n",
      "\n",
      "data = df.iloc[:,9:]\n",
      "89/35:\n",
      "\n",
      "from sklearn import preprocessing\n",
      "data_scaled = preprocessing.scale(data)\n",
      "89/36:\n",
      "\n",
      "data_scaled.mean(axis=0)\n",
      "89/37:\n",
      "\n",
      "data_scaled.var(axis=0)\n",
      "89/38:\n",
      "\n",
      "array = data_scaled\n",
      "89/39:\n",
      "\n",
      "array.shape\n",
      "89/40:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "89/41:\n",
      "\n",
      "pca = PCA()\n",
      "89/42:\n",
      "\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "89/43:\n",
      "\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "89/44:\n",
      "\n",
      "variance_ratio = pca.explained_variance_ratio_\n",
      "plt.plot(variance_ratio, 'ro')\n",
      "plt.show()\n",
      "print(sum(variance_ratio[0:2]))\n",
      "print(sum(variance_ratio[0:3]))\n",
      "89/45:\n",
      "\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "pc2 = pca.components_[1]\n",
      "print('Attribute, PC1, PC2')\n",
      "for i in range(0,pc1.shape[0]):\n",
      "    print(attributes[i] + ':' + repr(pc1[i]) + \":\" + repr(pc2[i]))\n",
      "89/46:\n",
      "\n",
      "k = 3\n",
      "attributes = df.columns.values[9:]\n",
      "pc1 = pca.components_[0]\n",
      "sorted = list(np.argsort(pc1))\n",
      "print('----------PC1-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc1[sorted[i]]))\n",
      "pc2 = pca.components_[1]\n",
      "sorted = list(np.argsort(pc2))\n",
      "print('----------PC2-------------')\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "print('--------------------------')\n",
      "sorted.reverse()\n",
      "for i in range(0, k): \n",
      "    print(attributes[sorted[i]] + ':' + repr(pc2[sorted[i]]))\n",
      "89/47:\n",
      "\n",
      "x = X_pca[:,0]\n",
      "y = X_pca[:,1]\n",
      "89/48:\n",
      "\n",
      "plt.figure(figsize=(30, 20))\n",
      "plt.scatter(x, y, c=\"red\", s=20, edgecolor='k')\n",
      "plt.title(\"Projection by PCA\")\n",
      "plt.xlabel(\"1st component\")\n",
      "plt.ylabel(\"2nd component\")\n",
      "labels = list(df.iloc[:,0])\n",
      "for i, txt in enumerate(labels):\n",
      "    plt.annotate(txt, (x[i], y[i]), ha=\"center\")\n",
      "\n",
      "plt.show()\n",
      "89/49:\n",
      "\n",
      "def myplot(score,coeff,labels=None):\n",
      "    plt.figure(figsize=(15,10))\n",
      "    plt.xlabel(\"PC{}\".format(1))\n",
      "    plt.ylabel(\"PC{}\".format(2))\n",
      "    xs = score[:,0]\n",
      "    ys = score[:,1]\n",
      "    n = coeff.shape[0]\n",
      "    scalex = 1.0/(xs.max() - xs.min())\n",
      "    scaley = 1.0/(ys.max() - ys.min())\n",
      "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
      "    for i in range(n):\n",
      "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
      "        if labels is None:\n",
      "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
      "        else:\n",
      "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
      "\n",
      "print(pca.components_[0:2, :])\n",
      "myplot(X_pca[:,0:2],np.transpose(pca.components_[0:2, :]),attributes)\n",
      "axes = plt.gca()\n",
      "axes.set_ylim([-1.0,1.0])\n",
      "plt.show()\n",
      "90/1:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "90/2:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "90/3:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "90/4:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "90/5:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "90/6:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "90/7:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "90/8:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    \n",
      "    #TODO\n",
      "90/9:\n",
      "\n",
      "plt.scatter(#TODO)\n",
      "plt.show()\n",
      "91/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "91/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "91/3:\n",
      "\n",
      "train\n",
      "91/4:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "91/5:\n",
      "\n",
      "train = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train.info()\n",
      "91/6:\n",
      "\n",
      "train.head()\n",
      "91/7:\n",
      "\n",
      "train.describe(include='all')\n",
      "91/8:\n",
      "\n",
      "# Missing values \n",
      "train.isna().sum()\n",
      "91/9:\n",
      "\n",
      "train\n",
      "91/10:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/11:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/12:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/13:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "91/14:\n",
      "\n",
      "train[\"budget\"]\n",
      "91/15:\n",
      "\n",
      "train[\"budget\"]\n",
      "cpi(train[\"budget\"], train[\"release_date\"])\n",
      "91/16:\n",
      "\n",
      "for movie in train :\n",
      "    cpi(movie[\"budget\"], movie[\"release_date\"])\n",
      "91/17:\n",
      "\n",
      "for movie in train :\n",
      "    cpi(movie[\"budget\"], movie[\"release_date\"])\n",
      "91/18:\n",
      "\n",
      "for movie in train :\n",
      "    print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/19:\n",
      "\n",
      "for movie in train :\n",
      "    movie\n",
      "    # print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/20:\n",
      "\n",
      "for movie in train :\n",
      "    print(movie)\n",
      "    # print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/21:\n",
      "\n",
      "for movie in train.rows :\n",
      "    print(movie)\n",
      "    # print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/22:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    print(movie)\n",
      "    # print(cpi(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/23:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/24:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[\"budget\"])\n",
      "91/25:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[0])\n",
      "91/26:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/27:\n",
      "\n",
      "for movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/28:\n",
      "\n",
      "for movie in train.columns() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/29:\n",
      "\n",
      "for movie in train.iteritems :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/30:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/31:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie)\n",
      "91/32:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[\"budget\"])\n",
      "91/33:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/34:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/35:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/36:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[0])\n",
      "91/37:\n",
      "\n",
      "for movie in train.iteritems() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/38:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[1])\n",
      "91/39:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie)\n",
      "91/40:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[0])\n",
      "91/41:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[3])\n",
      "91/42:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[\"budget\"])\n",
      "91/43:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[2])\n",
      "91/44:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[3])\n",
      "91/45:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[4])\n",
      "91/46:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[5])\n",
      "91/47:\n",
      "\n",
      "for movie in train.itertuples() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[6])\n",
      "91/48:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie)\n",
      "91/49:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))    \n",
      "    print(movie[\"budget\"])\n",
      "91/50:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"]))\n",
      "91/51:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))\n",
      "91/52:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(movie[\"release_date\"].year)\n",
      "91/53:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(movie[\"release_date\"].year())\n",
      "91/54:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/55:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "from datetime import date\n",
      "91/56:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(date.strftime(movie[\"release_date\"], \"%m/%d/%Y\") )\n",
      "91/57:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%Y\") )\n",
      "91/58:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\") )\n",
      "91/59:\n",
      "\n",
      "for i, movie in train.iterrows() :\n",
      "    # print(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year))    \n",
      "    print(datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year )\n",
      "91/60:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    print(cpi.inflate(movie[\"budget\"], year))\n",
      "91/61:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    print(cpi.inflate(movie[\"budget\"], year).to_int)\n",
      "91/62:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/63:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    print(movie[\"release_date\"])\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/64:\n",
      "\n",
      "pd.to_datetime(train[\"release_date\"])\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/65:\n",
      "\n",
      "print(pd.to_datetime(train[\"release_date\"]))\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/66:\n",
      "\n",
      "pd.to_datetime(train[\"release_date\"])\n",
      "def fix_date(x):\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train[\"release_date\"].apply(fix_date)\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/67:\n",
      "\n",
      "def fix_date(x):\n",
      "    pd.to_datetime(x, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train[\"release_date\"].apply(fix_date)\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/68:\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train[\"release_date\"].apply(fix_date)\n",
      "# for i, movie in train.iterrows() :    \n",
      "#     print(pd.to_datetime(movie[\"release_date\"] ))\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/69:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    print(movie[\"release_date\"] )\n",
      "    # year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    # print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/70:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%m/%d/%y\").year\n",
      "    print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/71:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    year = datetime.datetime.strptime(movie[\"release_date\"], \"%Y-%m-%d\").year\n",
      "    print(int(cpi.inflate(movie[\"budget\"], year)))\n",
      "91/72:\n",
      "\n",
      "for i, movie in train.iterrows() :    \n",
      "    print(int(cpi.inflate(movie[\"budget\"], movie[\"release_date\"].year)))\n",
      "91/73:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "    \n",
      "train[\"budget\"] = train_original[\"budget\", \"release_date\"].apply(adjust_price_to_inflation)\n",
      "91/74:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/75:\n",
      "\n",
      "train_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train_original.info()\n",
      "91/76:\n",
      "\n",
      "train_original.head()\n",
      "91/77:\n",
      "\n",
      "train_original.describe(include='all')\n",
      "91/78:\n",
      "\n",
      "# Missing values \n",
      "train_original.isna().sum()\n",
      "91/79:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "    \n",
      "train[\"budget\"] = train_original[\"budget\", \"release_date\"].apply(adjust_price_to_inflation)\n",
      "91/80:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "# def adjust_price_to_inflation(price, date):\n",
      "#     return int(cpi.inflate(price, date.year))\n",
      "#     \n",
      "# train[\"budget\"] = train_original[\"budget\", \"release_date\"].apply(adjust_price_to_inflation)\n",
      "    \n",
      "print(train_original[\"budget\", \"release_date\"])\n",
      "91/81:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "# def adjust_price_to_inflation(price, date):\n",
      "#     return int(cpi.inflate(price, date.year))\n",
      "#     \n",
      "# train[\"budget\"] = train_original[\"budget\", \"release_date\"].apply(adjust_price_to_inflation)\n",
      "    \n",
      "print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/82:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train_original[[\"budget\", \"release_date\"]].apply(adjust_price_to_inflation)\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/83:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = adjust_price_to_inflation(train_original[\"budget\"], train_original[\"release_date\"])\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/84:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x['budget'], x['release_date']))\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/85:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]))\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/86:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "    \n",
      "# print(train_original[[\"budget\", \"release_date\"]])\n",
      "91/87:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/88:\n",
      "\n",
      "train_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train_original.info()\n",
      "91/89:\n",
      "\n",
      "train_original.head()\n",
      "91/90:\n",
      "\n",
      "train_original.describe(include='all')\n",
      "91/91:\n",
      "\n",
      "# Missing values \n",
      "train_original.isna().sum()\n",
      "91/92:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "91/93:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/94:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/95:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/96:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "91/97:\n",
      "\n",
      "print(train)\n",
      "91/98:\n",
      "\n",
      "train.head()\n",
      "91/99:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "91/100:\n",
      "\n",
      "train.head()\n",
      "91/101:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "91/102:\n",
      "\n",
      "train.head()\n",
      "91/103:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "91/104:\n",
      "\n",
      "train.head()\n",
      "91/105:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "train[\"revenue\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/106:\n",
      "\n",
      "train.head()\n",
      "91/107:\n",
      "\n",
      "train.head()\n",
      "91/108:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/109:\n",
      "\n",
      "train_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train_original.info()\n",
      "91/110:\n",
      "\n",
      "train_original.head()\n",
      "91/111:\n",
      "\n",
      "train_original.describe(include='all')\n",
      "91/112:\n",
      "\n",
      "# Missing values \n",
      "train_original.isna().sum()\n",
      "91/113:\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "train[\"revenue\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/114:\n",
      "\n",
      "train.head()\n",
      "91/115:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/116:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/117:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11)\n",
      "plt.show()\n",
      "91/118:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "91/119:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "91/120:\n",
      "\n",
      "train_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "train_original.info()\n",
      "91/121:\n",
      "\n",
      "train_original.head()\n",
      "91/122:\n",
      "\n",
      "train_original.describe(include='all')\n",
      "91/123:\n",
      "\n",
      "# Missing values \n",
      "train_original.isna().sum()\n",
      "91/124:\n",
      "\n",
      "train = train_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "train[\"release_date\"] = train_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "train[\"budget\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "train[\"revenue\"] = train.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/125:\n",
      "\n",
      "train.head()\n",
      "91/126:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/127:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/128:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=train, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/129:\n",
      "\n",
      "sns.distplot(train.revenue)\n",
      "92/1:\n",
      "\n",
      "# loading recquired libraries \n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# evironment configuration\n",
      "np.set_printoptions(linewidth=2000)\n",
      "np.set_printoptions(threshold=2000)\n",
      "92/2:\n",
      "\n",
      "# data reading\n",
      "data_file_path = \"./tmdb-box-office-prediction/complete_movie_dataset.csv\"\n",
      "dataset = pd.read_csv(data_file_path, encoding=\"ISO-8859-1\")\n",
      "92/3:\n",
      "\n",
      "# print basic informations obout dataset\n",
      "print(\"Dataset size:\", dataset.size)\n",
      "print(\"Dataset shape:\", dataset.shape)\n",
      "print(\"\")\n",
      "print(dataset.info())\n",
      "print(\"\")\n",
      "print(\"Example records: \")\n",
      "dataset.head(2) #uncomment to see example record\n",
      "92/4:\n",
      "\n",
      "# Decription of numerical column\n",
      "dataset.describe(include = [np.number])\n",
      "92/5:\n",
      "\n",
      "# Description of objectvalue columns\n",
      "dataset.describe(include = ['O'])\n",
      "92/6:\n",
      "\n",
      "# All column description\n",
      "dataset.describe(include='all')\n",
      "92/7:\n",
      "\n",
      "# Missing values \n",
      "dataset.isna().sum()\n",
      "91/130:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/131:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/132:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/133:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/134:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/135:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/136:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/137:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/138:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/139:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/140:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/141:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv')\n",
      "dataset_original.info()\n",
      "91/142:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv', encoding=\"ISO-8859-1\")\n",
      "dataset_original.info()\n",
      "91/143:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/144:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/complete_movie_dataset.csv', encoding=\"ISO-8859-1\")\n",
      "dataset_original.info()\n",
      "91/145:\n",
      "\n",
      "dataset_original.head()\n",
      "91/146:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "91/147:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "91/148:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/149:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/150:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/151:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "91/152:\n",
      "\n",
      "dataset_original.head()\n",
      "91/153:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "91/154:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "91/155:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.date(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/156:\n",
      "\n",
      "dataset.head()\n",
      "91/157:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/158:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/159:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/160:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "91/161:\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/162:\n",
      "\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].describe\n",
      "91/163:\n",
      "\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "dataset.describe\n",
      "91/164:\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "dataset.describe(include=\"release_date\")\n",
      "91/165:\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].describe()\n",
      "91/166:\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].describe(include=\"all\")\n",
      "91/167:\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].info()\n",
      "91/168:\n",
      "\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "dataset[\"release_date\"].min()\n",
      "dataset[\"release_date\"].max()\n",
      "91/169:\n",
      "\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "print(dataset[\"release_date\"].min())\n",
      "print(dataset[\"release_date\"].max())\n",
      "91/170:\n",
      "\n",
      "#Okrelenie cech zbiorw wartoci (np. w kontekcie dat minimalna, maksymalna warto, w kontekcie sw kluczowych, czy pochodz ze zbioru predefiniowanego, itd.).\n",
      "print(\"Oldest: \",dataset[\"release_date\"].min())\n",
      "print(\"Newest: \",dataset[\"release_date\"].max())\n",
      "91/171:\n",
      "dataset.groupby(dataset[\"release_date\"].year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/172:\n",
      "dataset.groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/173:\n",
      "dataset.groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/174:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "91/175:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "91/176:\n",
      "\n",
      "dataset_original.head()\n",
      "91/177:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "91/178:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "91/179:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "91/180:\n",
      "\n",
      "dataset.head()\n",
      "91/181:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/182:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/183:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/184:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "91/185:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "91/186:\n",
      "dataset.groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/187:\n",
      "dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/188:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped)\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/189:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.distplot(grouped)\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/190:\n",
      "dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.distplot(grouped)\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/191:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "fig, ax = plt.subplots(1,1)\n",
      "\n",
      "# pass ax here\n",
      "sns.distplot(grouped, ax=ax)\n",
      "plt.show()\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/192:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped.transpose())\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/193:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped.transpose())\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/194:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/195:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped[\"release_date\"]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/196:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped[0]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/197:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.info()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/198:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.describe()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/199:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/200:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.hist()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/201:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/202:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/203:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.first()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/204:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/205:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[0]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/206:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[1]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/207:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[3]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/208:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[10]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/209:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[200]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/210:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[100]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/211:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.iloc[50]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/212:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.columns\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/213:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.columns()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/214:\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.columns()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/215:\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/216:\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "grouped.distplot()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/217:\n",
      "\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/218:\n",
      "\n",
      "grouped = dataset[[\"release_date\", \"id\"]].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/219:\n",
      "\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/220:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/221:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=dataset, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/222:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/223:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.release_date\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/224:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped[\"release_date\"]\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/225:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/226:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.info()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/227:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.describe()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/228:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.columns()\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/229:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.head(10)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/230:\n",
      "grouped = dataset[\"release_date\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "grouped.head(10)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/231:\n",
      "grouped = dataset[\"release_date\"].groupby(dataset[\"release_date\"].dt.year).agg('count')\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/232:\n",
      "grouped = dataset[\"release_date\"].groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/233:\n",
      "grouped = dataset[\"release_date\"].groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/234:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/235:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/236:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/237:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/238:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/239:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/240:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/241:\n",
      "grouped = dataset.groupby(\"release_date\".dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/242:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/243:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/244:\n",
      ".count()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/245:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False)\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/246:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).groups\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/247:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).sum()\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/248:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(\"count\")\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/249:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False)\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/250:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({\"count\": \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/251:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({\"n\": \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/252:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({\"n\": \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/253:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({count: \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/254:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({id: \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/255:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg({number: \"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/256:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(number: \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/257:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(count: \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/258:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(count= \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/259:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(total= \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/260:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg((\"id\", \"count\"))\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/261:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg((\"release_date\", \"count\"))\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/262:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(total= \"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/263:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False)\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/264:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).groups\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/265:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/266:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year, as_index=False).agg(\"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/267:\n",
      "grouped = dataset[[\"release_date\",\"id\"]].groupby(dataset[\"release_date\"].dt.year).agg(\"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/268:\n",
      "grouped = dataset[[\"id\"]].groupby(dataset[\"release_date\"].dt.year).agg(\"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/269:\n",
      "grouped = dataset[[\"id\"]].groupby(dataset[\"release_date\"].dt.year).agg(id=\"count\")\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/270:\n",
      "grouped = dataset[[\"id\"]].groupby(dataset[\"release_date\"].dt.year).agg({\"count\":\"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/271:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg({\"count\":\"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/272:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year).agg({\"total\":\"count\"})\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/273:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year)[\"anount\"]\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/274:\n",
      "grouped = dataset[\"id\"].groupby(dataset[\"release_date\"].dt.year)[\"release_date\"]\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/275:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"]\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/276:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].groups\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/277:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/278:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year, as_index=False)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/279:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/280:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/281:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/282:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[[\"release_date\", \"id\"]].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/283:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[[\"release_date\", \"id\"]].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/284:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[[\"release_date\", \"id\"]].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/285:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"release_date\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/286:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/287:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/288:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).value_counts()\n",
      "\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/289:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/290:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.columns()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/291:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/292:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/293:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].value_counts()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/294:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).agg(['sum', 'mean', 'max'])\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/295:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).agg(['count', 'mean', 'max'])\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/296:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year).count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/297:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/298:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.index\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/299:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.add_suffix('_Count').reset_index()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/300:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped.reset_index()\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/301:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "\n",
      "sns.distplot(grouped.reset_index())\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/302:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "# sns.distplot(grouped)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/303:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.id)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/304:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/305:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.release_date, bins=grouped.id)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/306:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.id, bins=grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/307:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.distplot(grouped.release_date)\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/308:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index()\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/309:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={'id':'count'})\n",
      "\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/310:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":'count'})\n",
      "\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/311:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":'count'})\n",
      "\n",
      "\n",
      "# sns.jointplot(x=\"release_date\", y=\"id\", data=grouped, height=11, color=\"b\")\n",
      "# grouped.head(10)\n",
      "grouped\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/312:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "91/313:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "91/314:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "91/315:\n",
      "\n",
      "sns.jointplot(x=\"category\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/1:\n",
      "\n",
      "sns.jointplot(x=\"category\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/4:\n",
      "\n",
      "dataset_original.head()\n",
      "93/5:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/6:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "93/7:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/8:\n",
      "\n",
      "dataset.head()\n",
      "93/9:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/10:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/11:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/12:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/13:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/14:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/15:\n",
      "\n",
      "sns.jointplot(x=\"category\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/316: cpi.update()\n",
      "93/16:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/17:\n",
      "\n",
      "cpi.update()\n",
      "91/317:\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "94/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/ed'])\n",
      "94/2: import cpi\n",
      "94/3: cpi\n",
      "94/4: cpi.update()\n",
      "93/18:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/19:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/20:\n",
      "\n",
      "dataset_original.head()\n",
      "93/21:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/22:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "93/23:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/24:\n",
      "\n",
      "dataset.head()\n",
      "93/25:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/26:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/27:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/28:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/29:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/30:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/31:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/32:\n",
      "sns.catplot(x='release_date_weekday', y='revenue', data=dataset)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "93/33:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "\n",
      "sns.catplot(x='release_date', y='revenue', data=dataset)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "93/34:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "\n",
      "sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "91/318:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"id\"].reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/319:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"]\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/320:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/321:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].groups\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/322:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].avg().reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/323:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].agg(\"avg\").reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/324:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].mean().reset_index()\n",
      "grouped\n",
      "# sns.catplot(x='release_date', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/325:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].mean().reset_index().rename(columns=\"release_date\", \"weekday\")\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/326:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].mean().reset_index().rename(columns={\"release_date\", \"weekday\"})\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/327:\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.weekday)[\"revenue\"].mean().reset_index().rename(columns={\"release_date\": \"weekday\"})\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/328:\n",
      "dataset['weekday'] = dataset[['release_date']].apply(lambda x: dt.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/329:\n",
      "dataset['weekday'] = dataset[['release_date']].apply(lambda x: dataset.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/330:\n",
      "dataset['weekday'] = dataset[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/331:\n",
      "temp = dataset[(dataset.revenue != 0).any()]\n",
      "temp\n",
      "# dataset['weekday'] = dataset[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/332:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "\n",
      "temp\n",
      "# dataset['weekday'] = dataset[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/333:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset[\"weekday\"] = temp[\"release_date\"].apply(lambda x: datetime.datetime.strftime(x[\"release_date\"], \"%A\"), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/334:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset[\"weekday\"] = temp[\"release_date\"].apply(lambda x: datetime.datetime.strftime(x[\"release_date\"], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/335:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/336:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "# plt.title('Revenue on different days of week of release')\n",
      "91/337:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "91/338:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped, height=11)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "91/339:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Revenue on different days of week of release')\n",
      "91/340:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.histplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Mean revenue on different days of week of release')\n",
      "91/341:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.plot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Mean revenue on different days of week of release')\n",
      "91/342:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.distplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Mean revenue on different days of week of release')\n",
      "91/343:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "plt.title('Mean revenue on different days of week of release')\n",
      "91/344:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "91/345:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=dataset, height=8)\n",
      "91/346:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=dataset, height=8, kind = \"box\")\n",
      "91/347:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=dataset, height=8, kind = \"boxen\")\n",
      "91/348:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "dataset['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=dataset, height=8)\n",
      "91/349:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "91/350:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "91/351:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "91/352:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "91/353:\n",
      "temp = dataset[dataset.revenue != 0 && dataset.revenue != 1]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "91/354:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp = temp[temp.revenue != 1]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "91/355:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "93/35:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/36:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/37:\n",
      "\n",
      "dataset_original.head()\n",
      "93/38:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/39:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "93/40:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/41:\n",
      "\n",
      "dataset.head()\n",
      "93/42:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/43:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/44:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/45:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/46:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/47:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/48:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "93/49:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/50:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/51:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/52:\n",
      "\n",
      "dataset_original.head()\n",
      "93/53:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/54:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "93/55:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/56:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "93/57:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/58:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/59:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/60:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/61:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/62:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/63:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "93/64:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/65:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "93/66:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "93/67:\n",
      "\n",
      "dataset_original.head()\n",
      "93/68:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "93/69:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "print(dataset_original.revenue.min())\n",
      "93/70:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "93/71:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "93/72:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/73:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/74:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "93/75:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "93/76:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "93/77:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "93/78:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "93/79:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/1:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "print(dataset_original.revenue.min())\n",
      "96/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "96/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "96/4:\n",
      "\n",
      "dataset_original.head()\n",
      "96/5:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "96/6:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "print(dataset_original.revenue.min())\n",
      "96/7:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "96/8:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "96/9:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/10:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/11:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/12:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "96/13:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "96/14:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "96/15:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "# temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "# grouped = dataset.groupby(\"weekday\")[\"revenue\"].mean().reset_index()\n",
      "# sns.catplot(x='weekday', y='revenue', data=grouped, height=8)\n",
      "# plt.title('Mean revenue on different days of week of release')\n",
      "# sns.catplot(x='weekday', y='revenue', data=temp, height=8, kind=\"box\")\n",
      "temp.revenue.min()\n",
      "96/16:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/17:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "91/356:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "# \n",
      "dataset[dataset.revenue == 1]\n",
      "96/18:\n",
      "\n",
      "genres = dataset['genres'].apply(lambda x: [i['genres'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in lang for i in j]).most_common(5)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/19:\n",
      "\n",
      "genre=df_train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/20:\n",
      "\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/21:\n",
      "\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/22:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "96/23:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "96/24:\n",
      "\n",
      "dataset_original.head()\n",
      "96/25:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "96/26:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "96/27:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "96/28:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "96/29:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/30:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/31:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/32:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "96/33:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "96/34:\n",
      "\n",
      "grouped = dataset.groupby(dataset[\"release_date\"].dt.year)[\"id\"].count().reset_index().rename(columns={\"id\":\"count\"})\n",
      "sns.jointplot(x=\"release_date\", y=\"count\", data=grouped, height=11, color=\"b\")\n",
      "96/35:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "96/36:\n",
      "\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/37:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/38:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/39:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/40:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/41:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Total movies released on Day Of Week\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/357:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by day\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/358:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/359:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(train['release_year'].sort_values())\n",
      "plt.title(\"Movie Release count by Year\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "91/360:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie Release count by Year\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "91/361:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "plt.show()\n",
      "91/362:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "91/363:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "91/364:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "91/365:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "\n",
      "train.groupby('release_month').agg('mean')['revenue'].plot(kind='bar',color='navy',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "91/366:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "\n",
      "dataset.groupby('release_month').agg('mean')['revenue'].plot(kind='bar',color='navy',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "91/367:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='navy',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "91/368:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "91/369:\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "91/370:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(train['release_month'].sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/371:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_month'].sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/372:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, labels = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/373:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "91/374:\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "91/375:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "        \n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/42:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "96/43:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "96/44:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "96/45:\n",
      "\n",
      "dataset_original.head()\n",
      "96/46:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "96/47:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "96/48:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "96/49:\n",
      "\n",
      "dataset.head()\n",
      "print(dataset.revenue.min())\n",
      "96/50:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/51:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/52:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/53:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "96/54:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "96/55:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "96/56:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "96/57:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/58:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/59:\n",
      "\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "96/60:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "        \n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(10)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/61:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "96/62:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/63:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "        \n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/64:\n",
      "\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "96/65:\n",
      "\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "96/66:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "96/67:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "96/68:\n",
      "\n",
      "dataset_original.head()\n",
      "96/69:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "96/70:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "96/71:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "96/72:\n",
      "\n",
      "dataset.head()\n",
      "96/73:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/74:\n",
      "\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/75:\n",
      "\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "96/76:\n",
      "\n",
      "sns.distplot(dataset.revenue)\n",
      "96/77:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "96/78:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "96/79:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "96/80:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/81:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "96/82:\n",
      "\n",
      "fig = plt.figure(figsize=(10,10))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "96/83:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "96/84:\n",
      "\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "96/85:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "96/86:\n",
      "\n",
      "sns.jointplot(x=\"genres\", y=\"budget\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "91/376:\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count], height=11)\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "91/377:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "91/378:\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count], height=11)\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "91/379:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "91/380:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "# sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "# plt.title(\"Movies by category\")\n",
      "# plt.show()\n",
      "count\n",
      "91/381:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "# sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "# plt.title(\"Movies by category\")\n",
      "# plt.show()\n",
      "genre\n",
      "91/382:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.show()\n",
      "91/383:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "91/384:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(15,10))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "91/385:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(15,10))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "96/87:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "96/88:\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "97/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "98/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "98/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "99/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "99/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "100/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "100/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "101/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "101/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "102/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "102/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "103/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "103/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "104/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "104/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "105/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "105/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "106/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "106/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "107/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "107/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "108/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "108/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "109/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "109/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "110/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "110/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "111/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "111/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "112/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "112/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "113/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "113/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "114/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "114/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "115/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "115/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "116/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "116/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "117/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "117/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "118/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "118/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "119/1: import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "120/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "120/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "121/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "121/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "122/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "122/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "123/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "123/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "124/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "124/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "125/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "125/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "126/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "126/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "127/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "127/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "128/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "128/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "129/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "130/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "130/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "131/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "131/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "132/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "132/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "133/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "133/2:\n",
      "def test(x):\n",
      "    return x*5\n",
      "133/3: x = 2\n",
      "133/4: test(x)\n",
      "133/5: x\n",
      "134/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "135/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "135/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "136/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "136/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "137/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "137/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "138/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "138/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "139/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "139/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "140/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "140/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "141/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "141/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "142/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "142/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "143/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "143/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "144/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "144/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "145/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "145/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "146/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "146/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "147/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "147/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "148/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "148/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "149/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "149/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "150/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "150/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "151/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "151/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "152/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "152/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "153/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "153/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "154/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "154/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "155/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "155/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "156/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "156/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "157/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "157/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "158/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "158/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "159/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "159/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "160/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "160/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "161/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "161/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "162/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "162/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "163/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "163/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "164/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "164/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "165/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "165/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "166/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "166/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "167/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "167/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "168/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "168/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "169/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "169/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "170/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "170/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "171/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "171/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "172/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "172/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "173/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "173/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "174/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "174/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "175/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "175/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "176/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "176/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "177/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "177/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "178/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "178/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "179/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "179/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "180/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "180/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "181/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "181/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "182/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "182/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "183/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "183/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "184/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "184/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "185/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "185/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "186/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "186/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "187/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "187/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "188/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "188/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "189/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "189/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "190/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "190/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "191/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "191/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "192/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "192/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "193/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "193/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "194/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "194/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "195/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "195/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "196/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "196/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "197/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "197/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "198/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "198/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "199/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "199/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "200/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "200/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "201/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "201/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "202/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "202/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "203/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "204/1:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/2:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "204/3:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "204/4:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "204/5:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "204/6:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "204/7:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "204/8:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    \n",
      "    #TODO\n",
      "204/9:\n",
      "\n",
      "plt.scatter(#TODO)\n",
      "plt.show()\n",
      "204/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/11:\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "204/12:\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "204/13:\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "204/14:\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "204/15:\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "204/16:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/17:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "204/18:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "204/19:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.show()\n",
      "204/20:\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "204/21:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/22:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = np.array(training_X).reshape(-1,1)\n",
      "training_y = np.array(training_y).reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/23:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = np.array(training_X).reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/24:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/25:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/26:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred)\n",
      "\n",
      "plt.show()\n",
      "204/27:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred, color=\"red\")\n",
      "\n",
      "plt.show()\n",
      "204/28:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred, color=\"green\")\n",
      "\n",
      "plt.show()\n",
      "204/29:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(n)\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/30:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict([n])\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/31:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict([n])\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/32:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1, 1))\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/33:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1, 1))[0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/34:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1, 1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/35:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1, -1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/36:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/37:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/38:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/39:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200)\n",
      "204/40:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(1000)\n",
      "204/41:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(1000)\n",
      "204/42:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(1000)\n",
      "204/43:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(2000)\n",
      "204/44:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(2000)\n",
      "204/45:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(4000)\n",
      "204/46:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(4000)\n",
      "204/47:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/48:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/49:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(10)\n",
      "204/50:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(1)\n",
      "204/51:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(100)\n",
      "204/52:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred, color=\"green\")\n",
      "\n",
      "plt.show()\n",
      "204/53:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/54:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/55:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array([[n]]))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/56:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/57:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(-1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/58:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/59:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/60:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/61:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/62:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "204/63:\n",
      "\n",
      "coef = regr.coef_[0][0]\n",
      "intercept = regr.intercept_[0]\n",
      "\n",
      "print('Coefficient: \\n', coef)\n",
      "print('Intercept: \\n', intercept)\n",
      "204/64:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "204/65:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "204/66:\n",
      "\n",
      "print(\"Test 1: \",run_series_cpu(4032003),\" Predicted:\", run_series_cpu_predicted_time(4032003),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(4032003))\n",
      "print(\"Test 2: \",run_series_cpu(54300),\" Predicted:\", run_series_cpu_predicted_time(54300),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(54300))\n",
      "print(\"Test 3: \",run_series_cpu(765403),\" Predicted:\", run_series_cpu_predicted_time(765403),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(765403))\n",
      "print(\"Test 4: \",run_series_cpu(84673),\" Predicted:\", run_series_cpu_predicted_time(84673),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(84673))\n",
      "print(\"Test 5: \",run_series_cpu(5436574),\" Predicted:\", run_series_cpu_predicted_time(5436574),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(5436574))\n",
      "204/67:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/68:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "204/69:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "204/70:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "204/71:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "204/72:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "204/73:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "204/74:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "204/75:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "204/76:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "204/77:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "204/78:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "204/79:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "204/80:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "204/81:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "204/82:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "204/83:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "204/84:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "204/85:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "204/86:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/1:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "205/2:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "205/3:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "205/4:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "205/5:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "205/6:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "205/7:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "205/8:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "205/9:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "205/10:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/11:\n",
      "\n",
      "def predicted_time(n):\n",
      " return regr.intercept_[0][0] + regr.coef_[0] * n\n",
      "205/12:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "205/13:\n",
      "\n",
      "print(\"Test 1: \",run_series_cpu(4032003),\" Predicted:\", run_series_cpu_predicted_time(4032003),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(4032003))\n",
      "print(\"Test 2: \",run_series_cpu(54300),\" Predicted:\", run_series_cpu_predicted_time(54300),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(54300))\n",
      "print(\"Test 3: \",run_series_cpu(765403),\" Predicted:\", run_series_cpu_predicted_time(765403),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(765403))\n",
      "print(\"Test 4: \",run_series_cpu(84673),\" Predicted:\", run_series_cpu_predicted_time(84673),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(84673))\n",
      "print(\"Test 5: \",run_series_cpu(5436574),\" Predicted:\", run_series_cpu_predicted_time(5436574),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(5436574))\n",
      "205/14:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "205/15:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "205/16:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "205/17:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "205/18:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "205/19:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "205/20:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations=10):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "205/21:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "ITERATIONS = 1\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N, ITERATIONS))\n",
      "205/22:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "205/23:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/24:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "205/25:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "205/26:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "205/27:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "205/28:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "205/29:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "205/30:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations=10):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "205/31:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N))\n",
      "205/32:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "205/33:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = training_X.reshape(-1,1)\n",
      "training_y = training_y.reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/34:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = np.array(training_X).reshape(-1,1)\n",
      "training_y = np.array(training_y).reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/35:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "205/36:\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "205/37:\n",
      "\n",
      "# Use only one feature\n",
      "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
      "\n",
      "# Split the data into training/testing sets\n",
      "diabetes_X_train = diabetes_X[:-20]\n",
      "diabetes_X_test = diabetes_X[-20:]\n",
      "\n",
      "# Split the targets into training/testing sets\n",
      "diabetes_y_train = diabetes.target[:-20]\n",
      "diabetes_y_test = diabetes.target[-20:]\n",
      "205/38:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "regr.fit(diabetes_X_train, diabetes_y_train)\n",
      "205/39:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
      "205/40:\n",
      "\n",
      "# Plot outputs\n",
      "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
      "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
      "\n",
      "#plt.xticks(())\n",
      "#plt.yticks(())\n",
      "\n",
      "plt.show()\n",
      "205/41:\n",
      "\n",
      "import random\n",
      "import string\n",
      "import time\n",
      "\n",
      "def io_heavy(text):\n",
      "    f = open('temp_file.txt', 'wt', encoding='utf-8')\n",
      "    f.write(text)\n",
      "    f.close()\n",
      "    return\n",
      "\n",
      "def cpu_heavy(n):\n",
      "    start = time.time()\n",
      "    count = 0\n",
      "    for i in range(n):\n",
      "        count += i\n",
      "    stop = time.time()\n",
      "    return\n",
      "\n",
      "def run_series_cpu(n, iterations=10):\n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        cpu_heavy(n)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "\n",
      "def run_series(text_size, iterations):\n",
      "    TEXT = ''.join(random.choice(string.ascii_lowercase) for i in range(text_size))\n",
      "    \n",
      "    start = time.time()\n",
      "    for i in range(iterations):\n",
      "        io_heavy(TEXT)\n",
      "    stop = time.time()\n",
      "    \n",
      "    return stop - start\n",
      "205/42:\n",
      "\n",
      "CPU_MAX_N = 1000000\n",
      "\n",
      "TRAINING_N = 50\n",
      "\n",
      "training_X = []\n",
      "training_y = []\n",
      "\n",
      "for i in range(TRAINING_N):\n",
      "    CPU_N = random.randrange(CPU_MAX_N)\n",
      "    training_X.append(CPU_N)\n",
      "    training_y.append(run_series_cpu(CPU_N))\n",
      "205/43:\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.xlabel(\"Iterations\")\n",
      "plt.ylabel(\"Time [s]\")\n",
      "plt.show()\n",
      "205/44:\n",
      "\n",
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "training_X = np.array(training_X).reshape(-1,1)\n",
      "training_y = np.array(training_y).reshape(-1,1)\n",
      "# Train the model using the training sets\n",
      "regr.fit(training_X, training_y)\n",
      "205/45:\n",
      "\n",
      "# Make predictions using the testing set\n",
      "training_y_pred = regr.predict(training_X)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: \\n', regr.coef_)\n",
      "print('Intercept: \\n', regr.intercept_)\n",
      "# The mean squared error\n",
      "print(\"Mean squared error: %.2f\"\n",
      "      % mean_squared_error(training_y, training_y_pred))\n",
      "# Explained variance score: 1 is perfect prediction\n",
      "print('Variance score: %.2f' % r2_score(training_y, training_y_pred))\n",
      "\n",
      "plt.scatter(training_X, training_y)\n",
      "plt.plot(training_X, training_y_pred, color=\"green\")\n",
      "\n",
      "plt.show()\n",
      "205/46:\n",
      "\n",
      "#testing data\n",
      "\n",
      "def run_series_cpu_predicted_time(n):\n",
      "    return regr.predict(np.array(n).reshape(1,1))[0,0]\n",
      "\n",
      "run_series_cpu_predicted_time(200000)\n",
      "205/47:\n",
      "\n",
      "coef = regr.coef_[0][0]\n",
      "intercept = regr.intercept_[0]\n",
      "\n",
      "print('Coefficient: \\n', coef)\n",
      "print('Intercept: \\n', intercept)\n",
      "205/48:\n",
      "\n",
      "def predicted_time(n):\n",
      " return regr.intercept_[0][0] + regr.coef_[0] * n\n",
      "205/49:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "205/50:\n",
      "\n",
      "print(\"Test 1: \",run_series_cpu(4032003),\" Predicted:\", run_series_cpu_predicted_time(4032003),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(4032003))\n",
      "print(\"Test 2: \",run_series_cpu(54300),\" Predicted:\", run_series_cpu_predicted_time(54300),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(54300))\n",
      "print(\"Test 3: \",run_series_cpu(765403),\" Predicted:\", run_series_cpu_predicted_time(765403),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(765403))\n",
      "print(\"Test 4: \",run_series_cpu(84673),\" Predicted:\", run_series_cpu_predicted_time(84673),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(84673))\n",
      "print(\"Test 5: \",run_series_cpu(5436574),\" Predicted:\", run_series_cpu_predicted_time(5436574),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(5436574))\n",
      "205/51:\n",
      "\n",
      "def predicted_time(n):\n",
      " return regr.intercept_[0] + regr.coef_[0][0] * n\n",
      "205/52:\n",
      "\n",
      "def run_series_cpu_predicted_time_linear_function(n):\n",
      "    return predicted_time(n)\n",
      "205/53:\n",
      "\n",
      "print(\"Test 1: \",run_series_cpu(4032003),\" Predicted:\", run_series_cpu_predicted_time(4032003),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(4032003))\n",
      "print(\"Test 2: \",run_series_cpu(54300),\" Predicted:\", run_series_cpu_predicted_time(54300),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(54300))\n",
      "print(\"Test 3: \",run_series_cpu(765403),\" Predicted:\", run_series_cpu_predicted_time(765403),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(765403))\n",
      "print(\"Test 4: \",run_series_cpu(84673),\" Predicted:\", run_series_cpu_predicted_time(84673),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(84673))\n",
      "print(\"Test 5: \",run_series_cpu(5436574),\" Predicted:\", run_series_cpu_predicted_time(5436574),\" Predicted2:\", run_series_cpu_predicted_time_linear_function(5436574))\n",
      "205/54:\n",
      "\n",
      "# Podsumowanie\n",
      "\n",
      "* Predykcja na podstawie modelu `LinearRegression` polega na obliczeniu wartoci funkcji liniowej regresji \n",
      "* Oszacowane czasy wykonywania oblicze s bliskie rzeczywistym czasom wykonania\n",
      "205/55:\n",
      "# Podsumowanie\n",
      "\n",
      "* Predykcja na podstawie modelu `LinearRegression` polega na obliczeniu wartoci funkcji liniowej regresji \n",
      "* Oszacowane czasy wykonywania oblicze s bliskie rzeczywistym czasom wykonania\n",
      "206/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "207/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "207/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "208/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "208/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "209/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "209/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_segmentation.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "210/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "210/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "211/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "211/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "212/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "212/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "213/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "213/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "214/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "214/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "215/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "215/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "216/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "216/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "217/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "217/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "218/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "218/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "206/2: (.8 * (1 - .8))\n",
      "219/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "219/2: runfile('/Users/kamildoleglo/Downloads/im_lab3/lab3_classification.py', wdir='/Users/kamildoleglo/Downloads/im_lab3')\n",
      "220/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab3'])\n",
      "221/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "221/2:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "221/3:\n",
      "\n",
      "dataset_original.head()\n",
      "221/4:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "221/5:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "221/6:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "221/7:\n",
      "\n",
      "dataset.head()\n",
      "221/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "221/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "221/10:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "221/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "221/12:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "221/13:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "221/14:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "221/15:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "221/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "221/17:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "221/18:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "221/19:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "221/20:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "221/21:\n",
      "\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "221/22:\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "221/23:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "222/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "222/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "222/3: dataset_original.head()\n",
      "222/4: dataset_original.describe(include='all')\n",
      "222/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "222/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "222/7: dataset.head()\n",
      "222/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "222/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "222/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "222/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "222/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "222/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "222/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "222/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "222/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "222/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "222/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "222/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "222/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "222/21:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "222/22:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "223/1:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "224/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "224/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "224/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "224/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "224/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "224/6:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"2. Revenue by release weekday\")\n",
      "plt.show()\n",
      "224/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "224/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "224/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "224/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "224/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "224/12:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "224/13:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "224/14:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "225/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "225/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "225/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "225/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "225/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "225/6:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"2. Revenue by release weekday\")\n",
      "plt.show()\n",
      "225/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "225/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "225/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "225/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "225/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "225/12:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "225/13:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "225/14:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "225/15:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "225/16:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "226/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "226/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "226/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "226/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "226/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "226/6:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"2. Revenue by release weekday\")\n",
      "plt.show()\n",
      "226/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "226/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "226/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "226/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "226/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "226/12:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "226/13:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "226/14:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "226/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movies releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "226/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movies releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "226/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movies releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "226/18:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movies releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "226/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "226/20:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "226/21:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "226/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "226/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Revenue by month\")\n",
      "plt.show()\n",
      "226/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "226/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "226/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "226/27:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Movies by category\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "226/28:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "226/29: dataset[\"TV\" in dataset['genre'].str]\n",
      "226/30: dataset[\"TV\" in dataset['genres'].str]\n",
      "226/31: dataset['genres'].str\n",
      "226/32: dataset['genres']\n",
      "226/33: dataset['genres'].name\n",
      "226/34: dataset['genres']['name']\n",
      "226/35: dataset[\"TV\" in dataset['genres']]\n",
      "226/36: \"TV\" in dataset['genres']\n",
      "226/37: dataset['genres']\n",
      "226/38: dataset['genres'].str.contains(\"TV\")\n",
      "226/39: dataset[dataset['genres'].str.contains(\"TV\")]\n",
      "226/40: dataset[dataset['genres'].str.contains(\"TV\", na=False)]\n",
      "226/41: Najwicej wydanych zostao dramatw a najmniej westernw\n",
      "226/42: dataset[dataset[\"genres\"].str.contains(\"Family\", no=False)]\n",
      "226/43: dataset[dataset[\"genres\"].str.contains(\"Family\", na=False)]\n",
      "226/44: dataset[dataset[\"genres\"].str.contains(\"family\", na=False)]\n",
      "226/45: dataset[dataset[\"genres\"].str.contains(\".*Family.*\", na=False)]\n",
      "226/46: dataset[dataset[\"genres\"].str.contains(\"Family\", regex=False, na=False)]\n",
      "226/47: dataset[dataset[\"genres\"].str.contains(\"Family\", regex=False, na=False)]\n",
      "226/48: dataset[dataset[\"genres\"].str.contains(\"Family\", regex=False, na=False)]\n",
      "226/49: genres\n",
      "226/50: genres[\"Family\"]\n",
      "226/51: genres\n",
      "226/52: genres\n",
      "227/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "227/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "227/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "227/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "227/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "227/11:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "227/12: genres\n",
      "227/13:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 50\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"5. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "plt.show()\n",
      "222/23:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "dataset_original.info()\n",
      "222/24:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "222/25: oscars.head()\n",
      "222/26: oscars.head(50)\n",
      "222/27: oscars[oscars[\"winner\"] ==True]\n",
      "222/28: oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "222/29: oscars_winners = oscars[oscars[\"winner\"] == True and !oscars[\"film\"].isnan()]\n",
      "222/30: oscars_winners = oscars[oscars[\"winner\"] == True and not oscars[\"film\"].isnan()]\n",
      "222/31: oscars_winners = oscars[oscars[\"winner\"] == True and not oscars[\"film\"] != NaN]\n",
      "222/32: oscars_winners = oscars[oscars[\"winner\"] == True and oscars[\"film\"].str != \"NaN\"]\n",
      "222/33: oscars_winners = oscars[oscars[\"winner\"] == True and not oscars[\"film\"].str.match(\"NaN\")]\n",
      "222/34: oscars_winners = oscars[oscars[\"winner\"] == True and not oscars[\"film\"].str.match(\"NaN\")]\n",
      "222/35:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "oscars_winners = oscars_winners[~oscars_winners[\"film\"].str.match(\"NaN\")]\n",
      "222/36:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "oscars_winners = oscars_winners[\"film\"].notnull()\n",
      "222/37:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "222/38:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "222/39:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True]\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "222/40:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "222/41:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "222/42:\n",
      "\n",
      "merged_inner = pd.merge(left=dataset, right=oscars_winners, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "222/43:\n",
      "\n",
      "df_dataset_lower['title'] = dataset['title'].str.lower()\n",
      "df_oscars_lower['film'] = oscars_winners['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=df_dataset_lower, right=df_oscars_lower, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "222/44:\n",
      "\n",
      "dataset['title'] = dataset['title'].str.lower()\n",
      "oscars_winners['film'] = oscars_winners['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=df_dataset_lower, right=df_oscars_lower, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "222/45:\n",
      "\n",
      "dataset['title'] = dataset['title'].str.lower()\n",
      "oscars_winners['film'] = oscars_winners['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset, right=oscars_winners, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "222/46:\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "227/14:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "227/15:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "227/16:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "227/17:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/18:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/19:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/20:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/21:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/22:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "227/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "227/24:\n",
      "\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "227/25:\n",
      "\n",
      "genres\n",
      "227/26:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "227/27: oscars.head(10)\n",
      "227/28:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "oscar_movies\n",
      "227/29:\n",
      "\n",
      "dataset = oscar_movies\n",
      "227/30:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/31:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/32:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/33:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/34:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/35:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "227/36:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/37:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/38:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/39: Najwicej filmw oscarowych wydanych zostao w grudniu a najmniej w styczniu, lutym i lipcu\n",
      "227/40:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/41:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates()\n",
      "oscar_movies\n",
      "227/42:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "oscar_movies.drop_duplicates()\n",
      "oscar_movies\n",
      "227/43:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"imdb_id\")\n",
      "oscar_movies\n",
      "227/44:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies\n",
      "227/45:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\").reset)index()\n",
      "oscar_movies\n",
      "227/46:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\").reset_index()\n",
      "oscar_movies\n",
      "227/47:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies\n",
      "227/48:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/49: dataset = oscar_movies\n",
      "227/50: dataset = oscar_movies\n",
      "227/51:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "227/52:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "227/53:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/54:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "227/55:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/56:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"12. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "227/57:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "227/58:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies.drop(columns=[\"film\"])\n",
      "oscar_movies\n",
      "227/59:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies.drop(columns=[\"film\"])\n",
      "oscar_movies\n",
      "227/60:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "oscar_movies\n",
      "227/61:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "oscar_movies\n",
      "227/62:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "# dataset_lc = dataset.copy()\n",
      "# oscars_winners_lc = oscars_winners.copy()\n",
      "# dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "# oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "# oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "# oscar_movies\n",
      "227/63:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscars_winners_lc\n",
      "# oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "# oscar_movies\n",
      "227/64:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "oscar_movies\n",
      "227/65:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "dataset_lc\n",
      "# oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "# oscar_movies\n",
      "228/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "228/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "228/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "228/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "228/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "228/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "228/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "228/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "228/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "228/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "228/11:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "228/12: genres\n",
      "228/13:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "228/14: oscars.head(10)\n",
      "228/15:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film_x\", \"film_y\"])\n",
      "oscar_movies\n",
      "228/16:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies\n",
      "228/17:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(\"film\")\n",
      "228/18:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\"])\n",
      "228/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()].rename(columns={\"category\": \"oscar_for\", \"name\": \"oscar_for_person\"})\n",
      "oscars_winners\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\"])\n",
      "oscar_movies\n",
      "228/20: dataset = oscar_movies\n",
      "228/21:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "228/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "228/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "228/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "228/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"12. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "228/26:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "229/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "229/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "229/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "229/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "229/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "229/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "229/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "229/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "229/11:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "229/12: genres\n",
      "229/13:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "229/14: oscars.head(10)\n",
      "229/15:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\"])\n",
      "oscar_movies\n",
      "229/16: dataset = oscar_movies\n",
      "229/17:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "229/18:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "229/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "229/20:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "229/21:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"12. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "229/22:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/23:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "229/24:\n",
      "import ast\n",
      "dict_columns = ['genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/25:\n",
      "import ast\n",
      "dict_columns = ['genres']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/26:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"]).replace({{}: np.NaN})\n",
      "oscar_movies\n",
      "229/27:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"]).replace({'{}': np.NaN})\n",
      "oscar_movies\n",
      "229/28:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "229/29:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) or x == {} else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/30:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if x == {} or pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "229/31:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "dataset_lc\n",
      "# oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "# oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "# oscar_movies\n",
      "229/32:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "230/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "230/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "230/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "230/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "230/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "230/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "230/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "230/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "230/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "temp = dataset.copy()\n",
      "temp = text_to_dict(temp)\n",
      "230/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "230/11:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "231/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "231/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "231/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "231/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "231/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "231/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "231/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "231/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "231/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "231/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "231/12:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/13: genres\n",
      "231/14:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "231/15: oscars.head(10)\n",
      "231/16:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "231/17: dataset = oscar_movies\n",
      "231/18:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "231/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"9. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "231/20:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"10. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "231/21:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"11. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "231/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"12. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "231/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "231/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "231/25:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/26:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/27: genres\n",
      "231/28: genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "231/29:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/30:\n",
      "genres=dataset.loc[dataset['genres'].str.len()][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/31: dataset[\"genres\"]\n",
      "231/32:\n",
      "genres=dataset.loc[dataset['genres'].str.len() == 1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/33:\n",
      "genres=dataset.loc[['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/34:\n",
      "genres=dataset.loc[dataset][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres\n",
      "231/35: dataset['genres'].apply(pd.Series).stack()\n",
      "231/36:\n",
      "x = dataset['genres'].apply(pd.Series).stack()\n",
      "x.head(50)\n",
      "231/37:\n",
      "# x = dataset['genres'].apply(pd.Series).stack()\n",
      "# x.head(50)\n",
      "z = dataset.explode(\"genres\")\n",
      "z\n",
      "231/38:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "\n",
      "genres['genres']=dataset.explode(\"genres\").genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/39:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/40:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/41:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "genres\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/42:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "genres\n",
      "# genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "231/43:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "genres\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "231/44:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\")\n",
      "genres\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/45:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/46:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x[0]['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/47:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "231/48:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "232/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "232/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "232/3:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "232/4:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "232/5:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "232/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "232/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "232/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "232/9:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "232/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "232/11:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "233/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "233/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "233/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "233/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "233/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "233/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "233/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "233/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "233/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "233/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "233/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "233/12:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "233/13:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres\n",
      "# genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "233/14:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"]=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "233/15:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "234/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "234/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "234/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "234/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "234/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "234/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "234/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "234/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "234/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "234/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "234/12:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/13:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/14:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/15:\n",
      "genre=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genre[\"genres\"]=genre.genres.apply(lambda x : x['name'])\n",
      "genres=genre.groupby(genre.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "234/16:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "235/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "235/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "235/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "235/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "235/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "235/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "235/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "235/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "235/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "235/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "235/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "235/12:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "235/13:\n",
      "dataset\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# # genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/14:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "235/15:\n",
      "genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "# genre = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres\n",
      "# genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/16:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres\n",
      "# genres[\"genres\"]=genres.genres.apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/17:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/18:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres[\"genres\"] = genres[\"title\"]\n",
      "genres\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/19:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres.genres.name\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/20:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres.genres[\"name\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/21:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\", \"name\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/22:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"][\"name\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/23:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/24:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/25:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : x[\"name\"])\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/26:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : x.type())\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/27:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x))\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/28:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x)).distinct()\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/29:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x)).unique()\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/30:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x))\n",
      "genres[genres[\"genres\"] == \"float\"]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/31:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x))\n",
      "genres[genres[\"genres\"] == type(float)]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/32:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : type(x))\n",
      "genres[genres[\"genres\"] == type(1.0)]\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/33:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[\"genres\"].apply(lambda x : isinstance(x, float))\n",
      "\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/34:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "# genres[\"genres\"] = genres\n",
      "genres[genres[\"genres\"].apply(lambda x : isinstance(x, float))]\n",
      "\n",
      "# genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "# genres=genres.groupby(genres.genres).agg('mean')\n",
      "# plt.figure(figsize=(20,12))\n",
      "# plt.subplot(2,2,1)\n",
      "# plt.title(\"5. Mean revenue and genre\")\n",
      "# sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,2)\n",
      "# plt.title(\"6. Mean budget and genre\")\n",
      "# sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,3)\n",
      "# plt.title(\"7. Mean popularity and genre\")\n",
      "# sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "# plt.subplot(2,2,4)\n",
      "# plt.title(\"8. Mean runtime and genre\")\n",
      "# sns.barplot(genres['runtime'],genres.index)\n",
      "# plt.show()\n",
      "235/35:\n",
      "# genres=dataset.loc[dataset['genres'].str.len()==1][['genres','revenue','budget','popularity','runtime']].reset_index(drop=True)\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "236/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "236/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "236/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "236/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "236/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "236/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "236/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "236/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "236/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "236/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "236/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"4. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "236/12:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"5. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"6. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"7. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"8. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "236/13: genres\n",
      "236/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"9. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "236/15:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "dataset\n",
      "# actorMovieDataset = dataset[['id','cast']].copy()\n",
      "# actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "# actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "# print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "# plt.figure(figsize=(20,12))\n",
      "# sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "# plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "# plt.xlabel(\"Actor name\")\n",
      "# plt.ylabel(\"Role number\")\n",
      "# plt.xticks(rotation=90)\n",
      "\n",
      "# plt.show()\n",
      "237/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "237/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "237/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "237/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "237/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "237/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "237/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "237/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "237/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "237/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "237/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "237/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "237/13: genres\n",
      "237/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "237/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,24))\n",
      "plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "237/16:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "237/17:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "237/18:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "237/19:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,24))\n",
      "plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "238/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "238/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "238/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "238/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "238/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "238/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "238/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "238/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "238/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "238/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "238/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "238/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "238/13: genres\n",
      "238/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "238/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,24))\n",
      "plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "238/16:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "238/17: oscars.head(10)\n",
      "238/18:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "238/19: dataset = oscar_movies\n",
      "238/20:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "238/21:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "238/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "238/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "238/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "238/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "238/26:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "238/27:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "240/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "240/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "240/3: import Bio\n",
      "240/4: pip3 install Bio\n",
      "241/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "241/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "242/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "242/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "243/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "243/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "244/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "244/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "245/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "245/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "246/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "246/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "247/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "247/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "248/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "248/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "249/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "249/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "250/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "250/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "251/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "251/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "252/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "252/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "253/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "253/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "254/1:\n",
      "\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "254/2:\n",
      "\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "254/3:\n",
      "\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "254/4:\n",
      "\n",
      "# jako modelu\n",
      "clf.score(X_test, y_test)\n",
      "254/5:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "254/6:\n",
      "\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "254/7:\n",
      "\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "254/8:\n",
      "\n",
      "print(get_log())\n",
      "254/9:\n",
      "\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    \n",
      "    #TODO\n",
      "254/10:\n",
      "\n",
      "#TODO\n",
      "254/11:\n",
      "\n",
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(#TODO)\n",
      "254/12:\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "254/13:\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "254/14:\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "254/15:\n",
      "# jako modelu\n",
      "clf.score(X_test, y_test)\n",
      "254/16:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "254/17:\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "254/18:\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "254/19: print(get_log())\n",
      "254/20: print(get_log())\n",
      "254/21: print(get_log())\n",
      "254/22: print(get_log())\n",
      "254/23: print(get_log())\n",
      "254/24:\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    comp_id = computers.index(triple[0])\n",
      "    server_id = sites.index(triple[1])\n",
      "    error_id = errors.index(triple[2])\n",
      "    CS_X.append((comp_id, server_id))\n",
      "    CS_y.append(error_id)\n",
      "254/25:\n",
      "print(CS_X)\n",
      "print(CS_y)\n",
      "254/26:\n",
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(CS_X, CS_y)\n",
      "254/27:\n",
      "features = [\"computers\", \"sites\"]\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf, filled=True, feature_names=features, class_names=errors)\n",
      "plt.show()\n",
      "254/28:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    feature_vector[comp_id] = 1\n",
      "    feature_vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(feature_vector)\n",
      "254/29:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    vector[comp_id] = 1\n",
      "    vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(vector)\n",
      "254/30: print(CS_X_onehot)\n",
      "254/31:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier()\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot,CS_y)\n",
      "254/32:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/33:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors, max_depth=2, min_samples_leaf=5)\n",
      "plt.show()\n",
      "254/34:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier()\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y, max_depth=2, min_samples_leaf=5)\n",
      "254/35:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/36:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/37:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=7, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/38:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/39:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier()\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/40:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/41:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=10)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/42:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/43:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/44:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/45:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/46:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/47:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "254/48:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "254/49:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "257/1:\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "257/2:\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "257/3:\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "257/4:\n",
      "# jako modelu\n",
      "clf.score(X_test, y_test)\n",
      "257/5:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "257/6:\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "257/7:\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "257/8: print(get_log())\n",
      "257/9:\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    comp_id = computers.index(triple[0])\n",
      "    server_id = sites.index(triple[1])\n",
      "    error_id = errors.index(triple[2])\n",
      "    CS_X.append((comp_id, server_id))\n",
      "    CS_y.append(error_id)\n",
      "257/10:\n",
      "print(CS_X)\n",
      "print(CS_y)\n",
      "257/11:\n",
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(CS_X, CS_y)\n",
      "257/12:\n",
      "features = [\"computers\", \"sites\"]\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf, filled=True, feature_names=features, class_names=errors)\n",
      "plt.show()\n",
      "257/13:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    vector[comp_id] = 1\n",
      "    vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(vector)\n",
      "257/14: print(CS_X_onehot)\n",
      "257/15:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "257/16:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "258/1:\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "258/2:\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "258/3:\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "258/4:\n",
      "# jako modelu\n",
      "clf.score(X_test, y_test)\n",
      "258/5:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "258/6:\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "258/7:\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "258/8: print(get_log())\n",
      "258/9:\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    comp_id = computers.index(triple[0])\n",
      "    server_id = sites.index(triple[1])\n",
      "    error_id = errors.index(triple[2])\n",
      "    CS_X.append((comp_id, server_id))\n",
      "    CS_y.append(error_id)\n",
      "258/10:\n",
      "print(CS_X)\n",
      "print(CS_y)\n",
      "258/11:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(CS_X, CS_y)\n",
      "258/12:\n",
      "features = [\"computers\", \"sites\"]\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf, filled=True, feature_names=features, class_names=errors)\n",
      "plt.show()\n",
      "258/13:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    vector[comp_id] = 1\n",
      "    vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(vector)\n",
      "258/14: print(CS_X_onehot)\n",
      "258/15:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "258/16:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "259/1:\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn import tree\n",
      "import numpy as np\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "iris = load_iris()\n",
      "\n",
      "X, y = load_iris(return_X_y=True)\n",
      "\n",
      "print('Class labels:', np.unique(y))\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "259/2:\n",
      "# przygotowanie danych treningowych\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
      "\n",
      "print('Labels counts in y:', np.bincount(y))\n",
      "print('Labels counts in y_train:', np.bincount(y_train))\n",
      "print('Labels counts in y_test:', np.bincount(y_test))\n",
      "259/3:\n",
      "# uczenie modelu\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(X_train, y_train)\n",
      "259/4:\n",
      "# jako modelu\n",
      "clf.score(X_test, y_test)\n",
      "259/5:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
      "plt.show()\n",
      "259/6:\n",
      "import random\n",
      "\n",
      "computers = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
      "sites = [\"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "errors = [\"200\", \"418\"]\n",
      "259/7:\n",
      "#model systemu komputerowego\n",
      "\n",
      "def get_log():\n",
      "    c = random.randrange(len(computers))\n",
      "    s = random.randrange(len(sites))\n",
      "    \n",
      "    if ( (0.2983*c*c*c - 1.195*c*c + 0.8967*c + 0.9) * (-0.125*s*s*s + 0.445*s*s - 0.44*s + 0.99)) < random.random():\n",
      "        e=1\n",
      "    else:\n",
      "        e=0\n",
      "        \n",
      "    return (computers[c], sites[s], errors[e])\n",
      "259/8: print(get_log())\n",
      "259/9:\n",
      "N = 50\n",
      "\n",
      "CS_X = []\n",
      "CS_y = []\n",
      "\n",
      "for n in range(N):\n",
      "    triple = get_log()\n",
      "    comp_id = computers.index(triple[0])\n",
      "    server_id = sites.index(triple[1])\n",
      "    error_id = errors.index(triple[2])\n",
      "    CS_X.append((comp_id, server_id))\n",
      "    CS_y.append(error_id)\n",
      "259/10:\n",
      "print(CS_X)\n",
      "print(CS_y)\n",
      "259/11:\n",
      "cs_clf = tree.DecisionTreeClassifier()\n",
      "cs_clf = cs_clf.fit(CS_X, CS_y)\n",
      "259/12:\n",
      "features = [\"computers\", \"sites\"]\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf, filled=True, feature_names=features, class_names=errors)\n",
      "plt.show()\n",
      "259/13:\n",
      "#features_onehot = [\"C1\", \"C2\", \"C3\", \"C4\", \"WWW1\", \"WWW2\", \"WWW3\", \"WWW4\"]\n",
      "\n",
      "features_onehot = computers + sites\n",
      "\n",
      "CS_X_onehot=[]\n",
      "\n",
      "for (comp_id, server_id) in CS_X:\n",
      "    vector = [0 for _ in range(len(features_onehot))]\n",
      "    vector[comp_id] = 1\n",
      "    vector[len(computers) + server_id] = 1\n",
      "    CS_X_onehot.append(vector)\n",
      "259/14: print(CS_X_onehot)\n",
      "259/15:\n",
      "cs_clf_onehot = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5)\n",
      "cs_clf_onehot = cs_clf_onehot.fit(CS_X_onehot, CS_y)\n",
      "259/16:\n",
      "features_onehot = computers + sites\n",
      "\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(cs_clf_onehot, filled=True, feature_names=features_onehot, class_names=errors)\n",
      "plt.show()\n",
      "260/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "260/2: runfile('/Users/kamildoleglo/Downloads/im_lab4/lab4_seq_align.py', wdir='/Users/kamildoleglo/Downloads/im_lab4')\n",
      "261/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "261/2:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "261/3:\n",
      "\n",
      "dataset_original.head()\n",
      "261/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "261/5:\n",
      "\n",
      "dataset_original.describe(include='all')\n",
      "261/6:\n",
      "\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "261/7:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "261/8:\n",
      "\n",
      "dataset.head()\n",
      "261/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "261/10:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "261/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "261/12:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "261/13:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "261/14:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "261/15:\n",
      "\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "261/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/17:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/18:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "261/19:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "261/20:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "261/21:\n",
      "\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "261/22:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "261/23:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "261/24:\n",
      "\n",
      "oscars.head(50)\n",
      "261/25:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "261/26:\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "261/27:\n",
      "\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "261/28:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "\n",
      "genres['revenue']\n",
      "261/29:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "261/30:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "\n",
      "genres\n",
      "261/31:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "\n",
      "genres.groupby(genres.genres)\n",
      "261/32:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "\n",
      "genres.groupby(genres.genres).head(5)\n",
      "261/33:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "\n",
      "genres.groupby(genres.genres).head(1)\n",
      "261/34:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "261/35: grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"]))\n",
      "261/36:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"]))\n",
      "grouped\n",
      "261/37:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"])).reset_index(drop=True)\n",
      "grouped.head()\n",
      "261/38:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"]))#.reset_index(drop=True)\n",
      "grouped.head()\n",
      "261/39:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False))#.reset_index(drop=True)\n",
      "grouped.head()\n",
      "261/40:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False))#.reset_index(drop=True)\n",
      "grouped.head(10)\n",
      "261/41:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "grouped.head(10)\n",
      "261/42:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.head(10)\n",
      "genres\n",
      "261/43:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "grouped.nlargest(2)\n",
      "261/44:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(2)\n",
      "261/45:\n",
      "grouped = genres.groupby(genres.genres)#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/46:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/47:\n",
      "grouped = genres.groupby(genres.genres)#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/48:\n",
      "grouped = genres.groupby(genres.genres)[\"revenue\"]#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/49:\n",
      "grouped = genres.groupby(genres.genres)[\"revenue\"]#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(10)\n",
      "261/50:\n",
      "grouped = genres.groupby(genres.genres)[\"title\", \"revenue\"]#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(2)\n",
      "261/51:\n",
      "grouped = genres.groupby(genres.genres)[\"title\", \"revenue\"]#.apply(lambda x: x.sort_values([\"revenue\"], ascending=False))\n",
      "grouped.nlargest(2)\n",
      "261/52:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "genres.sort_values('revenue', ascending=False)\n",
      "        .groupby(genres.genres, sort=False).head(5)\n",
      "261/53:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(5)\n",
      "261/54:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "261/55: genres.sort_values('budget', ascending=False).head(10)\n",
      "261/56: genres.sort_values('popularity', ascending=False).head(10)\n",
      "261/57:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(r)\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "# loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "# plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/58:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False)#.groupby(genres.genres, sort=False).head(5)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(r)\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "# loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "# plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/59:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(x = r[\"title\"], y = r[\"revenue\"])\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "# loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "# plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/60:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "plt.figure(figsize=(20,12))\n",
      "r.plot(kind='bar',color='b',rot=0)\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "# loc, _ = plt.xticks()\n",
      "# loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "# plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "261/61:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "r.plot(kind='bar',color='b',rot=0)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/62:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "plot(r[\"title\"], r[\"revenue\"], kind='bar',color='b',rot=0)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/63:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "plt(r[\"title\"], r[\"revenue\"], kind='bar',color='b',rot=0)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/64:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "plt.plot(r[\"title\"], r[\"revenue\"], kind='bar',color='b',rot=0)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/65:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"],r[\"revenue\"])\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/66:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = genres.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/67:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/68:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/69:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/70:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(2)\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)#.groupby(genres.genres, sort=False).head(5)\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/71:\n",
      "r = genres.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/72:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "# plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/73:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/74:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "# plt.title(\"Movies released by month\",fontsize=20)\n",
      "plt.show()\n",
      "261/75:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "grouped.nlargest(10)\n",
      "261/76:\n",
      "grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "grouped.nlargest(10, \"revenue\")\n",
      "261/77:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(10, \"revenue\")\n",
      "genres.sort_values('revenue', ascending=False)\n",
      "        .groupby(genres.genres, sort=False).head(5)\n",
      "261/78:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(10, \"revenue\")\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(5)\n",
      "261/79:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(10, \"revenue\")\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(10)\n",
      "261/80:\n",
      "# grouped = genres.groupby(genres.genres).apply(lambda x: x.sort_values([\"revenue\"], ascending=False)).reset_index(drop=True)\n",
      "# grouped.nlargest(10, \"revenue\")\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "261/81: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "261/82: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "261/83: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "261/84: from mlxtend.frequent_patterns import apriori\n",
      "261/85: from mlxtend.frequent_patterns import apriori\n",
      "261/86:\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "\n",
      "print frequent_itemsets\n",
      "261/87:\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/88:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/89:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "r\n",
      "261/90:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "r.map()\n",
      "261/91:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 30)\n",
      "r.map()\n",
      "261/92:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 30)\n",
      "r\n",
      "261/93:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 80)\n",
      "r\n",
      "261/94:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r\n",
      "261/95:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.map(\"name\")\n",
      "261/96:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r[\"name\"]\n",
      "261/97:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.keys\n",
      "261/98:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.values\n",
      "261/99:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r\n",
      "261/100:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: x[\"name\"], axis=1)\n",
      "261/101:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: x[\"name\"])\n",
      "261/102:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: x)\n",
      "261/103:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: x[0])\n",
      "261/104:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: type(x))\n",
      "261/105:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: len(x))\n",
      "261/106:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: map(lambda y: y[\"name\"] , x) )\n",
      "261/107:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: map(lambda y: y[\"name\"] , x))\n",
      "261/108:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: list(map(lambda y: y[\"name\"] , x))\n",
      "261/109:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: list(map(lambda y: y[\"name\"] , x)))\n",
      "261/110:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: list(x.map(lambda y: y[\"name\"])))\n",
      "261/111:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "pd.set_option('display.max_colwidth', 200)\n",
      "r.apply(lambda x: list(map(lambda y: y[\"name\"], x)))\n",
      "261/112:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x)))\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/113:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/114:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/115:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: Series(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/116:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: pd.Series(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/117:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/118:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)[1]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/119:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)[]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/120:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/121:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r[0]\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/122:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/123:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "type(r)\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/124:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "# te = TransactionEncoder()\n",
      "\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/125:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/126:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/127:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/128:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "te_ary\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/129:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "te_ary.any(True)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/130:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "np.any(te_ary)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/131:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/132:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/133:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/134:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/135:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "# frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/136:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, use_colnames=True)\n",
      "# frequent_itemsets\n",
      "261/137:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/138:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, use_colnames=True)\n",
      "\n",
      "type(frequent_itemsets)\n",
      "261/139:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "frequent_itemsets = apriori(df, use_colnames=True)\n",
      "frequent_itemsets\n",
      "261/140:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.6, use_colnames=True)\n",
      "261/141:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.1, use_colnames=True)\n",
      "261/142:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.001, use_colnames=True)\n",
      "262/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/ed'])\n",
      "263/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "263/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "263/3: dataset_original.head()\n",
      "263/4: dataset_original.describe(include='all')\n",
      "263/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "263/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "263/7: dataset.head()\n",
      "263/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "263/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "263/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "263/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "263/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "263/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "263/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "263/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "263/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "263/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "263/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "263/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "263/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "263/21:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "263/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "263/23: genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "263/24:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "263/25: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "263/26:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "263/27: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "263/28:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "263/29: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "263/30:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "263/31: oscars.head(50)\n",
      "263/32:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "263/33:\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "263/34:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.1, use_colnames=True)\n",
      "263/35:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "263/36:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.1, use_colnames=True)\n",
      "263/37:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.05, use_colnames=True)\n",
      "263/38:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/39:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.01, use_colnames=True)\n",
      "263/41:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.01, use_colnames=True)\n",
      "263/42:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(2000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.01, use_colnames=True)\n",
      "263/43:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.01, use_colnames=True)\n",
      "263/44:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/45:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(5000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/46:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True)\n",
      "r.tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/47:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "\n",
      "te_ary = te.fit(r).transform(r)\n",
      "\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "df\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/48:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/49:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "263/50:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/51:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/52:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/53:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Overview\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/54:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "263/55:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "264/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "264/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "264/3: dataset_original.head()\n",
      "264/4: dataset_original.describe(include='all')\n",
      "264/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "264/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "264/7: dataset.head()\n",
      "264/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "264/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "264/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "264/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "264/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "264/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "264/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "264/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "264/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "264/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "264/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "264/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "264/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "264/21:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "264/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "264/23: genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "264/24:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "264/25: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "264/26:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "264/27: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "264/28:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "264/29: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "264/30:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "264/31: oscars.head(50)\n",
      "264/32:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "264/33:\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "264/34:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "264/35:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "264/36:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "264/37:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "265/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "265/3: dataset_original.head()\n",
      "265/4: dataset_original.describe(include='all')\n",
      "265/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "265/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "265/7: dataset.head()\n",
      "265/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "265/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "265/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "265/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "265/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "265/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "265/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "265/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "265/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "265/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "265/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "265/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "265/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "265/21:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "265/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "265/23: genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "265/24:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "265/25: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "265/26:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "265/27: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "265/28:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "265/29: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "265/30:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "265/31: oscars.head(50)\n",
      "265/32:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "265/33:\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "265/34:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True)\n",
      "265/35:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "265/36:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True)\n",
      "265/37:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=2, use_colnames=True)\n",
      "265/38:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "265/39:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=2, use_colnames=True)\n",
      "265/40:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=1, use_colnames=True)\n",
      "265/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/42:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r.reset_index(drop=True).tolist()\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/43:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r.tolist()\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/44:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "map(lambda x: x.split(\" \"), r.tolist())\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/45:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=1, use_colnames=True)\n",
      "265/46:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=1, use_colnames=True)\n",
      "265/47:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/48:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: x !in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/49:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: x !in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/50:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: x not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/51:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: x[0] not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/52:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/53:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r\n",
      "# r = list(filter(lambda x: [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x], r))\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/54:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r\n",
      "# r = list(filter(lambda x: [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x], r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/55:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = list(filter(lambda x: [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x], r))\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/56:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in r]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/57:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x for x in r]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/58:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y in x in r]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/59:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for y for x in r]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/60:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for x for r for y in x]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/61:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for x in r for y in x]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/62:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "r = list(map(lambda x: x.split(\" \"), r.tolist()))\n",
      "r = [y if y not in [\"a\", \"an\", \"and\", \"of\", \"the\", \"to\"] for x in r for y in x]\n",
      "r\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.5, use_colnames=True)\n",
      "265/63:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort(\"support\")\n",
      "265/64:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/65:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/66:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/67:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/68:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x)).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/69:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/70:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "# r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x))).tolist()\n",
      "r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/71:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "# r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x))).tolist()\n",
      "r = r.split(\" \").tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/72:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "# r = r.apply(lambda x: list(map(lambda y: filter(y not in [\"of\"], y[\"name\"]), x))).tolist()\n",
      "r = r.tolist().split(\" \")\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/73:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/74:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: x[\"name\"]).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/75:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: print(\"x\")).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/76:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: print(x)).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/77:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: print(x.split(\" \"))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/78:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(x not in [\"of\"], x.split(\" \"))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/79:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(x not in [\"of\"], x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/80:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y not in [\"of\"], x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "# list(filter(, r))\n",
      "# te = TransactionEncoder()\n",
      "# te_ary = te.fit(r).transform(r)\n",
      "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "# apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/81:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y not in [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\"], x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/82:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/83:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/84:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/85:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"it's\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/86:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"it's\", \"at\", \"up\", \"they\", \"when\" \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/87:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\" \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/88:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/89:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"him\", \"after\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/90:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"him\", \"after\", \"out\", \"find\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/91:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/92:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(500)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/93:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/94:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/95:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/96:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/97:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.09, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/98:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/99:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/100:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"will\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/101:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/102:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/103:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/104:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/105:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "# r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "# r = r.tolist()\n",
      "r\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/106:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/107:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "fp_growth(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/108:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fp_growth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/109:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/110:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "fp_growth(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/111:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "fpgrowth(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/112:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True).sort_values('support', ascending=False)\n",
      "265/113:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "# .sort_values('support', ascending=False)\n",
      "res[len(res[\"itemsets\"]) ==2]\n",
      "265/114: res\n",
      "265/115: res[\"itemsets\"]\n",
      "265/116: res[\"itemsets\"] == 2\n",
      "265/117: res[\"itemsets\"]\n",
      "265/118: res[\"itemsets\"].len()\n",
      "265/119: res[\"itemsets\"].length\n",
      "265/120: res[\"itemsets\"].length()\n",
      "265/121: res[\"itemsets\"].str.len()\n",
      "265/122: res[res[\"itemsets\"].str.len() == 2]\n",
      "265/123:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "# .sort_values('support', ascending=False)\n",
      "265/124:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/125:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 6].sort_values('support', ascending=False)\n",
      "265/126:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.006, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/127:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.007, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/128:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.007, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/129:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(1000)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.007, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/130:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/131:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0016, max_len=2, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 2].sort_values('support', ascending=False)\n",
      "265/132:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0016, max_len=10, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 10].sort_values('support', ascending=False)\n",
      "265/133:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.000016, max_len=10, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() == 10].sort_values('support', ascending=False)\n",
      "266/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "266/2:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "266/3: dataset_original.head()\n",
      "266/4: dataset_original.describe(include='all')\n",
      "266/5:\n",
      "# Missing values \n",
      "dataset_original.isna().sum()\n",
      "266/6:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "266/7: dataset.head()\n",
      "266/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "266/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"popularity\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "266/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.jointplot(x=\"runtime\", y=\"revenue\", data=dataset, height=11, color=\"b\")\n",
      "plt.show()\n",
      "266/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.distplot(dataset.revenue)\n",
      "266/12:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "266/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.show()\n",
      "266/14:\n",
      "temp = dataset[dataset.revenue != 0]\n",
      "temp['weekday'] = temp[['release_date']].apply(lambda x: datetime.datetime.strftime(x['release_date'], '%A'), axis=1)\n",
      "\n",
      "sns.catplot(x='weekday', y='revenue', data=temp, height=8)\n",
      "plt.title(\"Revenue by release weekday\")\n",
      "plt.show()\n",
      "266/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"Movies released by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "266/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"Movies released by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.show()\n",
      "266/17:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',color='b',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"Revenue by month\")\n",
      "plt.show()\n",
      "266/18:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "        \n",
      "dfx = text_to_dict(dataset)\n",
      "temp = dataset.copy()\n",
      "for col in dict_columns:\n",
      "       temp[col]=dfx[col]\n",
      "266/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=temp['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"Movies by category\")\n",
      "plt.xlabel(\"Movie number\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "266/20:\n",
      "sns.jointplot(x=\"budget\", y=\"revenue\", data=dataset, height=11, ratio=4, color=\"g\")\n",
      "plt.show()\n",
      "266/21:\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'],genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'],genres_mean.index)\n",
      "plt.show()\n",
      "266/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "266/23: genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "266/24:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "266/25: genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "266/26:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "266/27: genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "266/28:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/29: genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "266/30:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "266/31: oscars.head(50)\n",
      "266/32:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "oscars_winners\n",
      "266/33:\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "merged_inner = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film')\n",
      "merged_inner\n",
      "266/34:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "266/35:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.001, max_len=3, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "266/36:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "266/37:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "266/38:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(1000)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "266/39:\n",
      "# pca = PCA()\n",
      "# X_pca = pca.fit_transform(array)\n",
      "# X_pca.shape\n",
      "#\n",
      "266/40:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.002, max_len=3, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "266/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "266/42:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"], a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/43:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"].name, a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/44:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"].name(), a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/45:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"].str(), a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/46:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(500)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"].tolist(), a[\"support\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/47:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(a[\"itemsets\"], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/48:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([x.str for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/49:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([x.values() for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/50:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([list(x) for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/51:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([x for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/52:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([list(x)[0] for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/53:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([list(x)[0] for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/54:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([list(x) for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/55:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([\", \".join(list(x)) for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/56:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot([\", \".join(list(x)) for x in a[\"itemsets\"]], [x * items for x in a[\"support\"]])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "266/57:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "266/58:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"Two actors\", a, dataset.len())\n",
      "266/59:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"Two actors\", a, len(dataset))\n",
      "266/60:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "266/61:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"Actors\", a, items)\n",
      "266/62:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"Crew\", a, items)\n",
      "266/63:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"Keywords\", a, items)\n",
      "266/64:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"Overview\", a, items)\n",
      "266/65:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "# plot_itemset(\"Two actors\", a, len(dataset))\n",
      "a\n",
      "266/66:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"Two actors\", a, len(dataset))\n",
      "267/1:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "267/2:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "267/3:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "267/4:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "267/5:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "267/6:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/7:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/10:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/11:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "267/12:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/13:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/14:\n",
      "\n",
      "genres\n",
      "267/15:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "267/16:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/17:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "267/18:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "267/19:\n",
      "\n",
      "oscars.head(10)\n",
      "267/20:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "267/21:\n",
      "\n",
      "dataset = oscar_movies\n",
      "267/22:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/24:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/25:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/26:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/27:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/28:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/29:\n",
      "\n",
      "rednie najwiksze przychody z filmw oscarowych przynosz horrory przy stosunkowo niskich budetach. \n",
      "\n",
      "rednie najwiksze budety posiadaj filmy animowane. \n",
      "\n",
      "rednie najmniejsze przychody przynosz filmy dokumentalne, posiadaj one rwnie rednio najmniejszy budet.\n",
      "\n",
      "rednio najwiksz popularnoci ciesz si filmy fantasy a najmniejsz filmy dokumentalne.\n",
      "\n",
      "rednio najdusze s horrory a najkrtsze filmy dokumentalne.\n",
      "267/30:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "267/31:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "267/32:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "267/33:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "267/34:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/35:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/36:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/37:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/38:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/39:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "267/40:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/41:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/42:\n",
      "\n",
      "genres\n",
      "267/43:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "267/44:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/45:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "267/46:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "267/47:\n",
      "\n",
      "oscars.head(10)\n",
      "267/48:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "267/49:\n",
      "\n",
      "dataset = oscar_movies\n",
      "267/50:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/51:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/52:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/53:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/54:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/55:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/56:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/57:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/58:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/59:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/60:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "267/61:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "267/62:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "267/63:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "267/64:\n",
      "# Highest revenue movies in ieach genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/65:\n",
      "# Highest budget movies in ieach genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/66:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/67:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "267/68:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "267/69:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "267/70:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "267/71:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/72:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/73:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/74:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/75:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/76:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "267/77:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/78:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/79:\n",
      "\n",
      "genres\n",
      "267/80:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "267/81:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/82:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "267/83:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "267/84:\n",
      "\n",
      "oscars.head(10)\n",
      "267/85:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "267/86:\n",
      "\n",
      "dataset = oscar_movies\n",
      "267/87:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/88:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/89:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/90:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/91:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/92:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/93:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/94:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "267/95:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + items + \" highest revenue movies\", a, items)\n",
      "267/96:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "267/97:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "267/98:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "267/99:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "267/100:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/101:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/102:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/103:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/104:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/105:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "267/106:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/107:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/108:\n",
      "\n",
      "genres\n",
      "267/109:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "267/110:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/111:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "267/112:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "267/113:\n",
      "\n",
      "oscars.head(10)\n",
      "267/114:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "267/115:\n",
      "\n",
      "dataset = oscar_movies\n",
      "267/116:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "267/117:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "267/118:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "267/119:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/120:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "267/121:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "267/122:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "267/123:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "267/124:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/125:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "267/126:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "267/127:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "267/128:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "267/129:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "267/130:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/131:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/132:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/133:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "267/134:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "267/135:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + items + \" highest revenue movies\", a, items)\n",
      "267/136:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "267/137:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "269/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "270/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "270/2:\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "270/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "270/4:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "270/5:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "270/6:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "270/7:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "270/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "270/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "270/10:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "270/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "270/12:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "270/13:\n",
      "\n",
      "genres\n",
      "270/14:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "270/15:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "270/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "270/17:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "270/18:\n",
      "\n",
      "oscars.head(10)\n",
      "270/19:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "270/20:\n",
      "\n",
      "dataset = oscar_movies\n",
      "270/21:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "270/22:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "270/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "270/24:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "270/25:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "270/26:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "270/27:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "270/28:\n",
      "\n",
      "genres = dataset.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "270/29:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "270/30:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "270/31:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "270/32:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "270/33:\n",
      "\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "270/34:\n",
      "\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "270/35:\n",
      "\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "270/36:\n",
      "\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "270/37:\n",
      "\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "270/38:\n",
      "\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "270/39:\n",
      "\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "270/40:\n",
      "\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "270/41:\n",
      "\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "271/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "271/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "271/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "271/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "271/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "271/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "271/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "271/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "271/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "271/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "271/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "271/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "271/13: genres\n",
      "271/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "271/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "271/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "271/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "271/18: oscars.head(10)\n",
      "271/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "271/20: dataset_oscars = oscar_movies\n",
      "271/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "271/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "271/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "271/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "271/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "271/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "271/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "271/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "271/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "271/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "271/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "271/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "271/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "271/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "271/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "271/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "271/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "271/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "271/39:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "271/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "271/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "271/42:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "272/2:\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "272/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "272/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "272/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "272/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "272/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "272/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "272/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "272/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "272/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "272/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "272/13: genres\n",
      "272/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "272/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "272/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "272/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "272/18: oscars.head(10)\n",
      "272/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "272/20: dataset_oscars = oscar_movies\n",
      "272/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "272/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "272/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "272/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "272/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "272/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "272/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "272/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "272/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "272/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "272/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "272/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "272/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "272/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "272/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "272/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "272/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "272/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "272/39:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "272/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "272/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/43:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/44:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/45:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "272/46:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "274/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "274/2:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "274/3:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "274/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "274/5:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "274/6:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "274/7:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "274/8:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "274/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "274/10:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "274/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "274/12:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "274/13:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "274/14:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "274/15:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "274/16:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "274/17:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "274/18:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "274/19:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "274/20:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "274/21: oscars.head(10)\n",
      "274/22:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "274/23: dataset_oscars = oscar_movies\n",
      "274/24:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "274/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "274/27:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "274/28:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "274/29:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "274/30:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "274/31:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "274/32:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "274/33:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"23. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "274/34:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "274/35:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "274/36:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "275/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "275/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "275/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "275/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "275/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "275/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "275/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "275/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "275/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "275/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "275/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "275/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "275/13: genres\n",
      "275/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "275/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "275/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "275/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "275/18: oscars.head(10)\n",
      "275/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "275/20: dataset_oscars = oscar_movies\n",
      "275/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "275/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "275/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "275/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "275/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "275/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "275/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "275/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "275/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "275/30:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"23. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "275/31:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "275/32:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "275/33:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "275/34:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "275/35:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "275/36:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "275/37:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "275/38:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "275/39:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"27. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "275/40:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "275/41:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "275/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "275/43:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "275/44:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "275/45:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/46:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors 2- and 3-itemsets in movies\", a, len(dataset))\n",
      "275/47:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "275/48:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/49:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/50:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"35. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/51:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"36. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "275/52:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/53:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"38. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/54:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('39. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "275/55:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"32. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/56:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/57:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/58:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"35. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "275/59:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"36. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/60:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/61:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('38. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "275/62:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"32. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/63:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/64:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/65:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "275/66:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"35. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "275/67:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"36. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/68:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "275/69:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('38. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "276/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "278/1:\n",
      "X_train = X_train.reshape(60000, 784)\n",
      "X_test = X_test.reshape(10000, 784)\n",
      "X_train = X_train.astype('float32')\n",
      "X_test = X_test.astype('float32')\n",
      "X_train /= #TODO\n",
      "X_test /= #TODO\n",
      "print(\"Training matrix shape\", X_train.shape)\n",
      "print(\"Testing matrix shape\", X_test.shape)\n",
      "281/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Downloads/im_lab4'])\n",
      "282/1:\n",
      "\n",
      "import pandas as pd\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/2:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "282/3:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "y = train[:][0]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    print(\"Train Index: \", train_index, \"\\n\")\n",
      "    print(\"Test Index: \", test_index)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "285/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/eksploracja/lab-2'])\n",
      "282/4:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "y = train[:][1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    print(\"Train Index: \", train_index, \"\\n\")\n",
      "    print(\"Test Index: \", test_index)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "282/5:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "282/6:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "282/7:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/8:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/9:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/10:\n",
      "#4\n",
      "X = train\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/11:\n",
      "#4\n",
      "X = train[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/12:\n",
      "#4\n",
      "X = train[:][2:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/13:\n",
      "#4\n",
      "X = train[2:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/14:\n",
      "#4\n",
      "X = train[2:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/15:\n",
      "#4\n",
      "X = train[1:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/16:\n",
      "#4\n",
      "X = train[1:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/17:\n",
      "#4\n",
      "X = train[1:][2:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/18:\n",
      "#4\n",
      "X = train[:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/19:\n",
      "#4\n",
      "X = train[:,1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/20:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/21:\n",
      "#4\n",
      "X = (train.to_numpy)[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/22:\n",
      "#4\n",
      "X = train.to_numpy()[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/23:\n",
      "#4\n",
      "X = train.to_numpy()[:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/24:\n",
      "#4\n",
      "X = train.to_numpy()[:][1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/25:\n",
      "#4\n",
      "X = train.to_numpy()[:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/26:\n",
      "#4\n",
      "X = train.to_numpy()[1:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/27:\n",
      "#4\n",
      "X = train.to_numpy()[:][:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/28:\n",
      "#4\n",
      "X = train.to_numpy()[:, 1:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/29:\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "# y = train[:][1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/30:\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 2]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "X\n",
      "282/31:\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "# scores = []\n",
      "# tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "# cv = KFold(n_splits=10, random_state=42)\n",
      "# for train_index, test_index in cv.split(X):\n",
      "#     print(\"Train Index: \", train_index, \"\\n\")\n",
      "#     print(\"Test Index: \", test_index)\n",
      "# \n",
      "#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "#     tree.fit(X_train, y_train)\n",
      "#     scores.append(tree.score(X_test, y_test))\n",
      "y\n",
      "282/32:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    print(\"Train Index: \", train_index, \"\\n\")\n",
      "    print(\"Test Index: \", test_index)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "282/33:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/34:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    print(\"Train Index: \", train_index, \"\\n\")\n",
      "    print(\"Test Index: \", test_index)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "282/35:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "282/36:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "scores = []\n",
      "tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 2)\n",
      "cv = KFold(n_splits=10, random_state=42)\n",
      "for train_index, test_index in cv.split(X):\n",
      "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "    tree.fit(X_train, y_train)\n",
      "    scores.append(tree.score(X_test, y_test))\n",
      "scores.avg()\n",
      "282/37:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/38:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "depth_range = range(2, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X):\n",
      "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "        tree.fit(X_train, y_train)\n",
      "        scores.append(tree.score(X_test, y_test))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/39:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "depth_range = range(2, 21)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X):\n",
      "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "        tree.fit(X_train, y_train)\n",
      "        scores.append(tree.score(X_test, y_test))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/40:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 21)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X):\n",
      "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "        tree.fit(X_train, y_train)\n",
      "        scores.append(tree.score(X_test, y_test))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/41:\n",
      "\n",
      "#4\n",
      "X = train.to_numpy()[:, 2:]\n",
      "y = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X):\n",
      "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
      "        tree.fit(X_train, y_train)\n",
      "        scores.append(tree.score(X_test, y_test))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/42:\n",
      "\n",
      "from sklearn import tree\n",
      "decision_tree = tree.DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "282/43:\n",
      "#5\n",
      "\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "282/44:\n",
      "#5\n",
      "test = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "decision_tree.predict(test)\n",
      "282/45:\n",
      "#5\n",
      "test = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "f = open(\"results.txt\",\"csv\")\n",
      "f.write(decision_tree.predict(test))\n",
      "282/46:\n",
      "#5\n",
      "test = test[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "f = open(\"results.txt\",\"csv\")\n",
      "f.write(decision_tree.predict(test))\n",
      "282/47:\n",
      "#5\n",
      "test = test[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "f = open(\"results.csv\",\"w+\")\n",
      "f.write(decision_tree.predict(test))\n",
      "282/48:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/49:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "282/50:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "282/51:\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10, random_state=42)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        scores.append(tree.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "282/52:\n",
      "#5\n",
      "test_without_index = test[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "f = open(\"results.csv\",\"w+\")\n",
      "f.write(decision_tree.predict(test_without_index))\n",
      "282/53:\n",
      "#5\n",
      "test_without_index = test[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv',prediction, delimiter=',')\n",
      "282/54:\n",
      "#5\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv',prediction, delimiter=',')\n",
      "282/55:\n",
      "#6\n",
      "tree.plot_tree(decision_tree);\n",
      "282/56:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "282/57:\n",
      "#6\n",
      "tree.plot_tree(decision_tree);\n",
      "282/58:\n",
      "#6\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "282/59:\n",
      "#6\n",
      "plt.figure(figsize=(10, 10))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/60:\n",
      "#6\n",
      "plt.figure(figsize=(12, 12))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/61:\n",
      "#6\n",
      "plt.figure(figsize=(18, 12))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/62:\n",
      "#6\n",
      "plt.figure(figsize=(18, 10))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/63:\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True);\n",
      "plt.show()\n",
      "282/64:\n",
      "#6\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.feature_names, class_names=train.target_names);\n",
      "282/65:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.feature_names, class_names=train.target_names)\n",
      "plt.show()\n",
      "282/66:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.column_names, class_names=train.target_names)\n",
      "plt.show()\n",
      "282/67:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns, class_names=train.target_names)\n",
      "plt.show()\n",
      "282/68:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns)\n",
      "plt.show()\n",
      "282/69:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:])\n",
      "plt.show()\n",
      "282/70:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Survived\"])\n",
      "plt.show()\n",
      "282/71:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=train.columns[1])\n",
      "plt.show()\n",
      "282/72:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"survived\", \"died\"])\n",
      "plt.show()\n",
      "282/73:\n",
      "\n",
      "#6\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "286/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/2:\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "286/3:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "g\n",
      "# corr, _ = pearsonr(train[train[\"\"]], train)\n",
      "286/4:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "g.groups()\n",
      "# corr, _ = pearsonr(train[train[\"\"]], train)\n",
      "286/5:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "g.groups\n",
      "# corr, _ = pearsonr(train[train[\"\"]], train)\n",
      "286/6:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train[[7,16,50]]\n",
      "286/7:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train[7,16,50]\n",
      "286/8:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc([7,16,50])\n",
      "286/9:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc([7,16,50])\n",
      "286/10:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc([7,16,50])\n",
      "286/11:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc([[7,16,50]])\n",
      "286/12:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc(7,16,50)\n",
      "286/13:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc([7,16,50])\n",
      "286/14:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc[7,16,50]\n",
      "286/15:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.loc[7,16,50]\n",
      "286/16:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc[7,16,50]\n",
      "286/17:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[], train)\n",
      "train.iloc[[7,16,50]]\n",
      "286/18:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[[group]], train)\n",
      "train.iloc[[7,16,50], 1]\n",
      "286/19:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[[group]], train)\n",
      "train.iloc[[7,16,50], ]\n",
      "286/20:\n",
      "#3\n",
      "# TODO\n",
      "# g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(train[[group]], train)\n",
      "train.iloc[[7,16,50], 1]\n",
      "286/21:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(train[[group]], train[[group], 1])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/22:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(train[group], train[[group], 1])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/23:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/24:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    train[group.tolist()]\n",
      "    # corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "# names\n",
      "# correlations\n",
      "286/25:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    group.tolist()\n",
      "    # train[group.tolist()]\n",
      "    # corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "# names\n",
      "# correlations\n",
      "286/26:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names = group.tolist()\n",
      "    # train[group.tolist()]\n",
      "    # corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "names\n",
      "# correlations\n",
      "286/27:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Title\", \"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names = group.tolist()\n",
      "    # train[group.tolist()]\n",
      "    # corr, _ = pearsonr(train[group.tolist()], train[group.tolist(), 1])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "names\n",
      "# correlations\n",
      "286/28:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Title\", \"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/29:\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Title\", \"Survived\"].groupby(train[\"Title\"])\n",
      "# names = []\n",
      "# correlations = []\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "#     names.append(name)\n",
      "#     correlations.append(corr)\n",
      "# names\n",
      "# correlations\n",
      "286/30:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "# names = []\n",
      "# correlations = []\n",
      "# for name, group in g:\n",
      "#     corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "#     names.append(name)\n",
      "#     correlations.append(corr)\n",
      "# names\n",
      "# correlations\n",
      "286/31:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/32:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Survived\"], group[\"Title\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "# correlations\n",
      "286/33:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "# correlations\n",
      "286/34:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/35:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations.append(group[\"Survived\"])\n",
      "names\n",
      "correlations\n",
      "286/36:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"]\n",
      "names\n",
      "correlations\n",
      "286/37:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\", 1]\n",
      "names\n",
      "correlations\n",
      "286/38:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"][1]\n",
      "names\n",
      "correlations\n",
      "286/39:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"]\n",
      "names\n",
      "correlations\n",
      "286/40:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"].tolist)_\n",
      "names\n",
      "correlations\n",
      "286/41:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"], group[\"Survived\"])\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    correlations = group[\"Survived\"].tolist()\n",
      "names\n",
      "correlations\n",
      "286/42:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/43:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    names.append(name)\n",
      "    correlations.append(corr)\n",
      "names\n",
      "correlations\n",
      "286/44:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "286/45:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]]\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "286/46:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]]\n",
      "names = []\n",
      "# correlations = []\n",
      "# for name, group in g:\n",
      "#     # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "#     # names.append(name)\n",
      "#     # correlations.append(corr)\n",
      "#     names = group[\"Title\"].tolist()\n",
      "# names\n",
      "# correlations\n",
      "g\n",
      "286/47:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]]\n",
      "names = []\n",
      "correlations = []\n",
      "# for name, group in g:\n",
      "#     # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "#     # names.append(name)\n",
      "#     # correlations.append(corr)\n",
      "#     names = group[\"Title\"].tolist()\n",
      "# names\n",
      "g.astype(float).corr()\n",
      "# correlations\n",
      "286/48:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    names = group.astype(float).corr()\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "287/1:\n",
      "print('PyDev console: using IPython 7.13.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['/Users/kamildoleglo/Documents/university/eksploracja/lab-2'])\n",
      "286/49:\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    # names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    names = group[\"Title\"].corr(group[\"Survived\"])\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "286/50:\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "286/51:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/52:\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "286/53:\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        scores.append(tree.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "286/54:\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "286/55:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/56:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test)\n",
      "286/57:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap='viridis')\n",
      "286/58:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test)\n",
      "286/59:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues)\n",
      "286/60:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "286/61:\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "286/62:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/63:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "accuracy_score(y_true, y_pred)\n",
      "286/64:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score: \", f1_score(y_true, y_pred))\n",
      "286/65:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "286/66:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score: \", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:: \", average_precision_score(y_true, y_pred))\n",
      "286/67:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy: \", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score: \", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score: \", average_precision_score(y_true, y_pred))\n",
      "286/68:\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "286/69:\n",
      "#8\n",
      "# kryterium podziau (gini vs. entropy), najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "286/70:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for scores from 1 to 20:\", scores)\n",
      "\n",
      "min_elem = range(1, 20)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for min samples in leaf from 1 to 20:\", scores)\n",
      "286/71:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 40)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for scores from 1 to 40:\", scores)\n",
      "plot(list(depth_range), scores)\n",
      "\n",
      "min_elem = range(1, 40)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for min samples in leaf from 1 to 40:\", scores)\n",
      "286/72:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 40)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for scores from 1 to 40:\", scores)\n",
      "plt.plot(list(depth_range), scores)\n",
      "\n",
      "min_elem = range(1, 40)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "print(\"Acc values for min samples in leaf from 1 to 40:\", scores)\n",
      "286/73:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 40)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 40)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.show()\n",
      "286/74:\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.show()\n",
      "288/1:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "tree = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    tree.fit(xtrain, ytrain)\n",
      "    scores.append(tree.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        local_scores.append(tree.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "288/2:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "288/3:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "288/4:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    # names = group[\"Title\"].corr(group[\"Survived\"])\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "288/5:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        tree.fit(xtrain, ytrain)\n",
      "        scores.append(tree.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "288/6:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "288/7:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "289/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "289/2:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "289/3:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[[\"Title\", \"Survived\"]].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    # corr, _ = pearsonr(group[\"Title\"].tolist(), group[\"Survived\"].tolist())\n",
      "    names.append(name)\n",
      "    # correlations.append(corr)\n",
      "    # names = group[\"Title\"].corr(group[\"Survived\"])\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "# correlations\n",
      "289/4:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "289/5:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "289/6:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "289/7:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "289/8:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "289/9:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "289/10:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "289/11:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "    # names = group[\"Title\"].corr(group[\"Survived\"])\n",
      "    # names = group[\"Title\"].tolist()\n",
      "names\n",
      "correlations\n",
      "289/12:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.show()\n",
      "289/13:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.show()\n",
      "289/14:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio\")\n",
      "plt.show()\n",
      "289/15:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survived among all with the same title\")\n",
      "plt.show()\n",
      "289/16:\n",
      "\n",
      "#3\n",
      "# TODO\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "290/1:\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X, y = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y))\n",
      "290/2:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "290/3:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "290/4:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "290/5:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "290/6:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "290/7:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "290/8:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "290/9:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "290/10:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "290/11:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "290/12:\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X, y = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y))\n",
      "291/1:\n",
      "X_y_nyt = pd.read_csv('data/nyt-frame.csv')\n",
      "X_y_nyt.head(3)\n",
      "291/2:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "291/3:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "291/4:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "291/5:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "291/6:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "291/7:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "291/8:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/9:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "291/10:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "291/11:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "291/12:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X, y = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "291/13:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "291/14:\n",
      "X_y_nyt = pd.read_csv('data/nyt-frame.csv')\n",
      "X_y_nyt.head(3)\n",
      "291/15:\n",
      "X_y_nyt = pd.read_csv('nyt-frame.csv')\n",
      "X_y_nyt.head(3)\n",
      "291/16:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "291/17:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "291/18:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/19:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/20:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/21:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/22:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"taxi\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/23:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/24:\n",
      "\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores)\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/25:\n",
      "\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan')\n",
      "plt.show()\n",
      "291/26:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan')\n",
      "plt.show()\n",
      "291/27:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "291/28:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan metric\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "291/29:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "291/30:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "# 1- Dimension reduction of NYT\n",
      "dff = nyt._get_numeric_data()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "291/31:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "# 1- Dimension reduction of NYT\n",
      "dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "291/32:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "291/33:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "291/34:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "291/35:\n",
      "X_pca[0:5,:]\n",
      "variance = pca.explainedvariance\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/36:\n",
      "X_pca[0:5,:]\n",
      "variance = X_pca.explainedvariance\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/37:\n",
      "X_pca[0:5,:]\n",
      "variance = X_pca.explained_variance\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/38:\n",
      "X_pca[0:5,:]\n",
      "variance = PCA.explained_variance\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/39:\n",
      "X_pca[0:5,:]\n",
      "variance = X_pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/40:\n",
      "X_pca[0:5,:]\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/41:\n",
      "X_pca[0:5,:]\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance)\n",
      "plt.show()\n",
      "291/42:\n",
      "X_pca[0:5,:]\n",
      "variance = pca.explained_variance_\n",
      "plt.plot(variance, 'ro')\n",
      "plt.show()\n",
      "291/43:\n",
      "plt.plot(pca.explained_variance_, 'ro')\n",
      "plt.show()\n",
      "291/44:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "X_pca\n",
      "291/45:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "X_pca.shape\n",
      "data.head(5)\n",
      "291/46:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "data.head(5)\n",
      "y_nyt = data.loc[:,'class.labels']\n",
      "reds = y_nyt == 'art'\n",
      "blues = y_nyt == 'music'\n",
      "reds\n",
      "291/47:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "data.head(5)\n",
      "# y_nyt = data.loc[:,'class.labels']\n",
      "# reds = y_nyt == 'art'\n",
      "# blues = y_nyt == 'music'\n",
      "# reds\n",
      "291/48:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "data.head(5)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "reds = y_nyt == 'art'\n",
      "blues = y_nyt == 'music'\n",
      "reds\n",
      "291/49:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "291/50:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "291/51:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "291/52:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "291/53:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "291/54:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "291/55:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "291/56:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "291/57:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "291/58:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "291/59:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "291/60:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "291/61:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "291/62:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "data.head(5)\n",
      "291/63:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "292/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "292/2:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "292/3:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "292/4:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "292/5:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "292/6:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "292/7:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "292/8:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "292/9:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "292/10:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "292/11:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "292/12:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "292/13:\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "data.head(5)\n",
      "292/14:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "293/2:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "293/3:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "293/4:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "293/5:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "293/6:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "293/7:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "293/8:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "293/9:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "293/10:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "293/11:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left');\n",
      "plt.show()\n",
      "293/12:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "293/13:\n",
      "\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "data.head(5)\n",
      "293/14:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/15:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='euclidean with weights ')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/16:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/17:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/18:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "293/19:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "print(\"Euclidean with weights has the same values as Euclidean without them\")\n",
      "295/1:\n",
      "\n",
      "#1\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn import tree\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "train = pd.read_csv('ed-titanic-training.csv')\n",
      "test = pd.read_csv('ed-titanic-test.csv')\n",
      "train.head(3)\n",
      "295/2:\n",
      "\n",
      "#2\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "colormap = plt.cm.viridis\n",
      "plt.figure(figsize=(12,12))\n",
      "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
      "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0,\n",
      "square=True, cmap=colormap, linecolor='white', annot=True)\n",
      "295/3:\n",
      "\n",
      "#3\n",
      "g = train[\"Survived\"].groupby(train[\"Title\"])\n",
      "names = []\n",
      "correlations = []\n",
      "for name, group in g:\n",
      "    names.append(name)\n",
      "    survived = sum(1 for x in group.tolist() if x > 0)\n",
      "    correlations.append(survived/len(group.index))\n",
      "\n",
      "plt.scatter(names, correlations)\n",
      "plt.xticks(names)\n",
      "plt.xlabel(\"Title\")\n",
      "plt.ylabel(\"Survival ratio among all with the same title\")\n",
      "plt.show()\n",
      "295/4:\n",
      "\n",
      "#4\n",
      "\n",
      "X_train = train.to_numpy()[:, 2:]\n",
      "y_train = train.to_numpy()[:, 1]\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "295/5:\n",
      "\n",
      "#5, 6\n",
      "test_without_index = test.to_numpy()[:, 1:]\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_train, y_train)\n",
      "prediction = decision_tree.predict(test_without_index)\n",
      "np.savetxt('results.csv', prediction, delimiter=',')\n",
      "295/6:\n",
      "\n",
      "#7\n",
      "plt.figure(figsize=(15, 10))\n",
      "tree.plot_tree(decision_tree, filled=True, feature_names=train.columns[2:], class_names=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "295/7:\n",
      "\n",
      "#8\n",
      "# najmniejsza liczba\n",
      "# rekordw w liciu oraz maksymalna gboko drzewa\n",
      "classifier = DecisionTreeClassifier(criterion = \"entropy\", max_depth = depth)\n",
      "cv = KFold(n_splits=10)\n",
      "scores = []\n",
      "for train_index, test_index in cv.split(X_train):\n",
      "    xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "    classifier.fit(xtrain, ytrain)\n",
      "    scores.append(classifier.score(xtest, ytest))\n",
      "print(\"Gini acc:\", max_score, \", entropy acc:\", np.mean(scores))\n",
      "\n",
      "depth_range = range(1, 60)\n",
      "scores = []\n",
      "for depth in depth_range:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(depth_range), scores)\n",
      "plt.xlabel(\"Max depth\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "\n",
      "min_elem = range(1, 60)\n",
      "scores = []\n",
      "for elem in min_elem:\n",
      "    local_scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", min_samples_leaf = elem)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(list(min_elem), scores)\n",
      "plt.xlabel(\"Min elements\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.show()\n",
      "295/8:\n",
      "\n",
      "#9\n",
      "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
      "295/9:\n",
      "\n",
      "#10\n",
      "decision_tree = DecisionTreeClassifier(max_depth = 3, criterion='entropy')\n",
      "decision_tree.fit(X_new_train, y_new_train)\n",
      "plt.figure(figsize=(15, 10))\n",
      "plot_confusion_matrix(decision_tree, X_new_test, y_new_test, cmap=plt.cm.Blues, display_labels=[\"Died\", \"Survived\"])\n",
      "plt.show()\n",
      "295/10:\n",
      "\n",
      "#11\n",
      "y_pred = decision_tree.predict(X_new_test)\n",
      "y_true = y_new_test\n",
      "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
      "print(\"F1 score:\", f1_score(y_true, y_pred))\n",
      "print(\"Average precision-recall score:\", average_precision_score(y_true, y_pred))\n",
      "295/11:\n",
      "\n",
      "# Zad 6\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "iris = load_iris()\n",
      "X_iris, y_iris = load_iris(return_X_y=True)\n",
      "print('Class labels:', np.unique(y_iris))\n",
      "print(X_iris[0:3,:])\n",
      "print(y_iris[0:3])\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_iris):\n",
      "        xtrain, xtest, ytrain, ytest = X_iris[train_index], X_iris[test_index], y_iris[train_index], y_iris[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "295/12:\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "nyt = pd.read_csv('nyt-frame.csv')\n",
      "nyt.head(3)\n",
      "295/13:\n",
      "\n",
      "# 1- Dimension reduction of NYT\n",
      "# dff = nyt.to_numpy()\n",
      "data = nyt.iloc[:,9:]\n",
      "array = data.values\n",
      "pca = PCA(n_components=10)\n",
      "X_pca = pca.fit_transform(array)\n",
      "y_nyt = nyt.loc[:,'class.labels']\n",
      "data.head(5)\n",
      "295/14:\n",
      "\n",
      "k_values = [1, 3, 5, 7]\n",
      "scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, scores, label='euclidean metric')\n",
      "plt.xlabel(\"K\")\n",
      "plt.xticks(k_values)\n",
      "plt.ylabel(\"Score\")\n",
      "manhattan_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, metric=\"manhattan\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    manhattan_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, manhattan_scores, label='manhattan metric')\n",
      "weight_scores = []\n",
      "for k in k_values:\n",
      "    local_scores = []\n",
      "    classifier = KNeighborsClassifier(n_neighbors = k, weights=\"distance\")\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_pca):\n",
      "        xtrain, xtest, ytrain, ytest = X_pca[train_index], X_pca[test_index], y_nyt[train_index], y_nyt[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        local_scores.append(classifier.score(xtest, ytest))\n",
      "    weight_scores.append(np.mean(local_scores))\n",
      "plt.scatter(k_values, weight_scores, label='euclidean with weights')\n",
      "\n",
      "plt.legend(loc='upper left')\n",
      "plt.show()\n",
      "print(\"Euclidean with weights has the same values as Euclidean without them\")\n",
      "296/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "296/2:\n",
      "\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "296/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "296/4:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "296/5:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "296/6:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "296/7:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "296/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "296/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "296/10:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "296/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "296/12:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "296/13:\n",
      "\n",
      "genres\n",
      "296/14:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "296/15:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "296/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "296/17:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "296/18:\n",
      "\n",
      "oscars.head(10)\n",
      "296/19:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "296/20:\n",
      "\n",
      "dataset_oscars = oscar_movies\n",
      "296/21:\n",
      "\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "296/22:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "296/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "296/24:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "296/25:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "296/26:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "296/27:\n",
      "\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "296/28:\n",
      "\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "296/29:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "296/30:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "296/31:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "296/32:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "296/33:\n",
      "\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "296/34:\n",
      "\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "296/35:\n",
      "\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "296/36:\n",
      "\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "296/37:\n",
      "\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "296/38:\n",
      "\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "296/39:\n",
      "\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "296/40:\n",
      "\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "296/41:\n",
      "\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "296/42:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "296/43:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "296/44:\n",
      "\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "296/45:\n",
      "\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "296/46:\n",
      "\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "297/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "297/2:\n",
      "\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "297/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "297/4:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "297/5:\n",
      "\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "297/6:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "297/7:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "297/8:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "297/9:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "297/10:\n",
      "\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "297/11:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "297/12:\n",
      "\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "297/13:\n",
      "\n",
      "genres\n",
      "297/14:\n",
      "\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "297/15:\n",
      "\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "297/16:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "297/17:\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "297/18:\n",
      "\n",
      "oscars.head(10)\n",
      "297/19:\n",
      "\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "297/20:\n",
      "\n",
      "dataset_oscars = oscar_movies\n",
      "297/21:\n",
      "\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "297/22:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "297/23:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "297/24:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "297/25:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "297/26:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "297/27:\n",
      "\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "297/28:\n",
      "\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "297/29:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "297/30:\n",
      "\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "297/31:\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "297/32:\n",
      "\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "297/33:\n",
      "\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "297/34:\n",
      "\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "297/35:\n",
      "\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "297/36:\n",
      "\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "297/37:\n",
      "\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "297/38:\n",
      "\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "297/39:\n",
      "\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "297/40:\n",
      "\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "297/41:\n",
      "\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "297/42:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "297/43:\n",
      "\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "297/44:\n",
      "\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "297/45:\n",
      "\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "297/46:\n",
      "\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "298/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "298/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "298/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "298/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "298/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "298/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "298/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "298/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "298/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "298/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "298/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "298/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "298/13: genres\n",
      "298/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "298/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "298/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "298/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "298/18: oscars.head(10)\n",
      "298/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "298/20: dataset_oscars = oscar_movies\n",
      "298/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "298/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "298/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "298/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "298/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "298/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "298/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "298/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "298/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "298/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "298/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "298/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "298/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "298/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "298/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "298/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "298/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "298/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "298/39:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "298/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "298/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "298/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "298/43:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "298/44:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "298/45:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "298/46:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "299/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "299/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "299/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "299/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "299/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "299/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "299/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "299/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "299/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "299/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "299/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "299/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "299/13: genres\n",
      "299/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "299/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "299/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "299/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "299/18: oscars.head(10)\n",
      "299/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "299/20: dataset_oscars = oscar_movies\n",
      "299/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "299/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "299/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "299/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "299/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "299/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "299/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "299/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "299/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "299/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "299/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "299/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "299/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "299/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "299/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "299/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "299/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "299/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "299/39:\n",
      "def plot_itemset(title, result, dataset_size):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=45)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "299/40:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "299/41:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "299/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "299/43:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "299/44:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "299/45:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "299/46:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "299/47:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df))\n",
      "299/48:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), 90)\n",
      "300/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "300/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "300/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "300/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "300/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "300/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "300/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "300/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "300/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "300/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "300/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "300/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "300/13: genres\n",
      "300/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "300/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "300/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "300/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "300/18: oscars.head(10)\n",
      "300/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "300/20: dataset_oscars = oscar_movies\n",
      "300/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "300/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "300/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "300/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "300/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "300/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "300/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "300/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "300/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "300/30:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"23. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "300/31:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "300/32:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "300/33:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "300/34:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"27. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "300/35:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "300/36:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "300/37:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "300/38:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "300/39:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "300/40:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "300/41:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "300/42:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "300/43:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"32. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "300/44:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "300/45:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "300/46:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "300/47:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"35. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "300/48:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"36. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "300/49:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "300/50:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"38. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "300/51:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('39. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "300/52:\n",
      "# Required imports\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "from sklearn import preprocessing\n",
      "from sklearn.cluster import KMeans\n",
      "from wordcloud import WordCloud, STOPWORDS \n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from sklearn.cluster import DBSCAN\n",
      "import math\n",
      "300/53:\n",
      "crewDataset = oscar_movies[['id','title','crew']].copy()\n",
      "crewDataset['crew'] = crewDataset['crew'].apply(lambda x: ['\\\"'+i['name']+'\\\"-\\\"'+i['job']+'\\\"' for i in x] if x != {} else [])\n",
      "crewDataset = crewDataset['crew']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(crewDataset).transform(crewDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(4,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"40. Frequent itemsets for Oscars movies crew\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 40: Zbiory czste dla czonkw ekip filmowych dla filmw Oskarowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste jednowymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Wrd ekip filmowych dla filmw oskarowych pojawiaj si grupy grajce wsplnie. Najczciej przewijajc si grup s producenci wykonawczy: Bob Weinstein oraz Harvey Weinstein.\n",
      "#     - Grupy czste wystpuj zazwyczaj w obrbie danej funkcji na planie\n",
      "#     - Wzrost min-support powoduje, e zaczynaj dominowa krtsze itemsety. Tendencja trudna do zaobserwowania ze wzgldu na du rnorodno wrd filmw Oskarowych.\n",
      "300/54:\n",
      "# Film crem most frequent itemsets in films\n",
      "crewDataset = dataset[['id','title','crew']].copy()\n",
      "crewDataset['crew'] = crewDataset['crew'].apply(lambda x: ['\\\"'+i['name']+'\\\"-\\\"'+i['job']+'\\\"' for i in x] if x != {} else [])\n",
      "crewDataset = crewDataset['crew']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(crewDataset).transform(crewDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(10,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"41. Frequent itemsets for all movies crew\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 41: Zbiory czste dla czonkw ekip filmowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 10;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste jednowymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Wrd ekip filmowych pojawiaj si grupy czonkw, ktrzy czsto wsppracuj ze sob. Najczciej przewijajc si grup s osoby odpowiedzialne za castingi: Tricia Wood oraz Debora Aquila.\n",
      "#     - Czste zbiory wystpuj najczciej wrd castingowcw\n",
      "300/55:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: [classify_movie_budget(x)])\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"42. Frequent itemsets for Oscar movies - parameters ['production_companies','budget']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 42: Zbiory czste dla wytwrni filmowych oraz budetw filmw w kategorii filmy Oskarowe\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste dwuwymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Due wytwrnie filmowe takie, jak Warner Bros, czy Universal Pictures tworz gwnie produkcje wysokobudetowe.\n",
      "#     - Ciekawe, jest e dla wytwrni 20th Century Fox Film pojawiaj si zbiory czste dla produkcji w rnych kategoriach budetowych.\n",
      "300/56:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: [classify_movie_budget(x)])\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(20,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"43. Frequent itemsets for movies - parameters ['production_companies','budget']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 43: Zbiory czste dla wytwrni filmowych oraz budetw filmw\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 20;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste dwuwymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Due wytwrnie filmowe takie, jak Warner Bros, czy Universal Pictures tworz gwnie produkcje wysokobudetowe.\n",
      "300/57:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"44. Frequent itemsets for Oscar movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 44: Zbiory czste dla wytwrni filmowych, obsady oraz sw kluczowych dla filmw Oskarowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Dozwolone zbiory czste dla wielu elementw w obrbie jednej cechy.\n",
      "300/58:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(11,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 45: Zbiory czste dla wytwrni filmowych, obsady oraz sw kluczowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 11;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Dozwolone zbiory czste dla wielu elementw w obrbie jednej cechy.\n",
      "# Wnioski: \n",
      "#    - Czste zbiory tworz si zazwyczaj w obrbie jednego wymiaru. \n",
      "#    - Dla duych wartoci min_support dopinuj krtke zbiory czste.\n",
      "300/59:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"46. Frequent itemsets for Oscras movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 46: Zbiory czste dla wytwrni filmowych, budetw oraz sw kluczowych dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "300/60:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(8,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"47. Frequent itemsets for movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 47: Zbiory czste dla wytwrni filmowych, budetw oraz sw kluczowych \n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 8;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Wrd czstych zbiorw pojawiaj si wysokobudetowe filmy Worner Bros ze sowem kluczowym dc comics\n",
      "#    - Wrd czstych zbiorw pojawiaj si rwnie wysokobudetowe filmy wytwrni 20th Century Fox Film ze sowem kluczowym alien\n",
      "300/61:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"48. Frequent itemsets for Oscras movies - parameters ['cast', 'crew']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 48: Zbiory czste dla obsady i ekipy filmowej dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja co najmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Dla filmw Oskarowych wrd zbiorw czstych dla obsady i ekipy pojaiwaj si grupy. Wskazuje, to na fakt istnienia zalenoci, e producenci/reyserzy maj swoiuch ulubionych aktorw.\n",
      "300/62:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"49. Frequent itemsets for Oscars movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 49: Zbiory czste dla obsady, budetu i wytwrni dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Liczba czstych zbiorw i ich support jest niewielki dla filmw oskarowych dla wektora cech: budet, obsada, wytwrnia\n",
      "300/63:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(7,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"50. Frequent itemsets for movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 50: Zbiory czste dla obsady, budetu i wytwrni filmowej\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 7;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "300/64:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(2,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"51. Frequent itemsets for Oscar movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 51: Zbiory czste dla obsady, reysera i wytwrni dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 2;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "300/65:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"52. Frequent itemsets for movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 52: Zbiory czste dla obsady, reysera i wytwrni dla wszystkich filmw\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "300/66:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['revenue','budget','popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 10]\n",
      "processedDataset = processedDataset[processedDataset['revenue'] < 2000000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] < 2500000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "fig = plt.figure(figsize=(12, 12))\n",
      "ax = Axes3D(fig)\n",
      "\n",
      "random_state = 170\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "ax.scatter(processedDataset['revenue'], processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"53. Movie clusters (KMeans k = 4 ) by revenue, budget and popularity\")\n",
      "plt.xlabel(\"Revenue\")\n",
      "plt.ylabel(\"Budget\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 53: Klastrowanie zbioru filmw dla cech revenue, budget, popularity\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 4;\n",
      "# Wnioski:\n",
      "#    - Trudno wyodrbni spjne grupy. Du grup stanowi filmy niskobudetowe, ktre przyniosy niski dochd i nie byy popularne.\n",
      "300/67:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"54. Movie clustering (KMeans) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 54: Klastrowanie zbioru filmw dla cech revenue, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 3;\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe i niskodochodowe.\n",
      "300/68:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"55. Movie clustering (DBSCAN) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 55: Klastrowanie zbioru filmw dla cech revenue, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe i niskodochodowe.\n",
      "300/69:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"56. Movie clustering (KMeans) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 56: Klastrowanie zbioru filmw dla cech popularity, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=3;\n",
      "# Uwagi: dokonano normalizacji danych. Odfiltrowano zerowe budety.\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe, ktre wykazuj popularnoc poniej przecitnej.\n",
      "300/70:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"57. Movie clustering (DBSCAN) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 57: Klastrowanie zbioru filmw dla cech popularity, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "300/71:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 100]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.06, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"58. Movie clustering (DBSCAN) by budget and popularity for Oscars movies\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 58: Klastrowanie zbioru filmw dla cech popularity, budget dla filmw Oskarowych\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps=0.06;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "300/72:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['runtime', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] < 200]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] > 50]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['runtime'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"59. Movie clustering (KMeans) by runtime and popularity\")\n",
      "plt.xlabel(\"Runtime\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 59: Klastrowanie zbioru filmw dla cech popularity, runtime\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=4;\n",
      "# Uwagi: dokonano normalizacji danych. Usunito skrajne wartoci dla czasu trwania dla lepszej wizualizacji.\n",
      "300/73:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(11,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 45: Zbiory czste dla wytwrni filmowych, obsady oraz sw kluczowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 11;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Dozwolone zbiory czste dla wielu elementw w obrbie jednej cechy.\n",
      "# Wnioski: \n",
      "#    - Czste zbiory tworz si zazwyczaj w obrbie jednego wymiaru. \n",
      "#    - Dla duych wartoci min_support dopinuj krtke zbiory czste.\n",
      "300/74:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"46. Frequent itemsets for Oscras movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 46: Zbiory czste dla wytwrni filmowych, budetw oraz sw kluczowych dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "300/75:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(8,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"47. Frequent itemsets for movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 47: Zbiory czste dla wytwrni filmowych, budetw oraz sw kluczowych \n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 8;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Wrd czstych zbiorw pojawiaj si wysokobudetowe filmy Worner Bros ze sowem kluczowym dc comics\n",
      "#    - Wrd czstych zbiorw pojawiaj si rwnie wysokobudetowe filmy wytwrni 20th Century Fox Film ze sowem kluczowym alien\n",
      "300/76:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"48. Frequent itemsets for Oscras movies - parameters ['cast', 'crew']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 48: Zbiory czste dla obsady i ekipy filmowej dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja co najmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Dla filmw Oskarowych wrd zbiorw czstych dla obsady i ekipy pojaiwaj si grupy. Wskazuje, to na fakt istnienia zalenoci, e producenci/reyserzy maj swoiuch ulubionych aktorw.\n",
      "300/77:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"49. Frequent itemsets for Oscars movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 49: Zbiory czste dla obsady, budetu i wytwrni dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Liczba czstych zbiorw i ich support jest niewielki dla filmw oskarowych dla wektora cech: budet, obsada, wytwrnia\n",
      "300/78:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(7,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"50. Frequent itemsets for movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 50: Zbiory czste dla obsady, budetu i wytwrni filmowej\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 7;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "300/79:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(2,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"51. Frequent itemsets for Oscar movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 51: Zbiory czste dla obsady, reysera i wytwrni dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 2;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "300/80:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"52. Frequent itemsets for movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 52: Zbiory czste dla obsady, reysera i wytwrni dla wszystkich filmw\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "300/81:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['revenue','budget','popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 10]\n",
      "processedDataset = processedDataset[processedDataset['revenue'] < 2000000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] < 2500000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "fig = plt.figure(figsize=(12, 12))\n",
      "ax = Axes3D(fig)\n",
      "\n",
      "random_state = 170\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "ax.scatter(processedDataset['revenue'], processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"53. Movie clusters (KMeans k = 4 ) by revenue, budget and popularity\")\n",
      "plt.xlabel(\"Revenue\")\n",
      "plt.ylabel(\"Budget\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 53: Klastrowanie zbioru filmw dla cech revenue, budget, popularity\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 4;\n",
      "# Wnioski:\n",
      "#    - Trudno wyodrbni spjne grupy. Du grup stanowi filmy niskobudetowe, ktre przyniosy niski dochd i nie byy popularne.\n",
      "300/82:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"54. Movie clustering (KMeans) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 54: Klastrowanie zbioru filmw dla cech revenue, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 3;\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe i niskodochodowe.\n",
      "300/83:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"55. Movie clustering (DBSCAN) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 55: Klastrowanie zbioru filmw dla cech revenue, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe i niskodochodowe.\n",
      "300/84:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"56. Movie clustering (KMeans) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 56: Klastrowanie zbioru filmw dla cech popularity, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=3;\n",
      "# Uwagi: dokonano normalizacji danych. Odfiltrowano zerowe budety.\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe, ktre wykazuj popularnoc poniej przecitnej.\n",
      "300/85:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"57. Movie clustering (DBSCAN) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 57: Klastrowanie zbioru filmw dla cech popularity, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "300/86:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 100]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.06, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"58. Movie clustering (DBSCAN) by budget and popularity for Oscars movies\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 58: Klastrowanie zbioru filmw dla cech popularity, budget dla filmw Oskarowych\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps=0.06;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "300/87:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['runtime', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] < 200]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] > 50]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['runtime'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"59. Movie clustering (KMeans) by runtime and popularity\")\n",
      "plt.xlabel(\"Runtime\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 59: Klastrowanie zbioru filmw dla cech popularity, runtime\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=4;\n",
      "# Uwagi: dokonano normalizacji danych. Usunito skrajne wartoci dla czasu trwania dla lepszej wizualizacji.\n",
      "301/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "301/2:\n",
      "# Common functions\n",
      "\n",
      "# Source: https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values\n",
      "def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n",
      "    def _show_on_single_plot(ax):\n",
      "        if h_v == \"v\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() / 2\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_height())\n",
      "                ax.text(_x, _y, value, ha=\"center\") \n",
      "        elif h_v == \"h\":\n",
      "            for p in ax.patches:\n",
      "                _x = p.get_x() + p.get_width() + float(space)\n",
      "                _y = p.get_y() + p.get_height()\n",
      "                value = int(p.get_width())\n",
      "                ax.text(_x, _y, value, ha=\"left\")\n",
      "\n",
      "    if isinstance(axs, np.ndarray):\n",
      "        for idx, ax in np.ndenumerate(axs):\n",
      "            _show_on_single_plot(ax)\n",
      "    else:\n",
      "        _show_on_single_plot(axs)\n",
      "        \n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def plot_itemset(title, result, dataset_size, rotation=90):\n",
      "    plt.figure(figsize=(20,12))\n",
      "    sns.barplot([\", \".join(list(x)) for x in result[\"itemsets\"]], [x * dataset_size for x in result[\"support\"]])\n",
      "    plt.xticks(fontsize=12,rotation=rotation)\n",
      "    plt.title(title,fontsize=20)\n",
      "    plt.show()\n",
      "    \n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "301/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "301/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "301/5:\n",
      "print(\"Oldest: \", dataset[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset[\"release_date\"].max())\n",
      "301/6:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.year.sort_values())\n",
      "plt.title(\"1. Movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "301/7:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"2. Movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n",
      "301/8:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset['release_date'].dt.month.sort_values())\n",
      "plt.title(\"3. Movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "301/9:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset.groupby(dataset[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"4. Mean revenue by month\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "301/10:\n",
      "import ast\n",
      "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "\n",
      "def text_to_dict(df):\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      " \n",
      "dataset = text_to_dict(dataset)\n",
      "301/11:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"5. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "301/12:\n",
      "genres = dataset.explode(\"genres\").copy()\n",
      "genres = genres[genres.genres.notnull()].reset_index(drop=True)\n",
      "genres[\"genres\"]=genres[\"genres\"].apply(lambda x : x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"6. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"7. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"8. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"9. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "301/13: genres\n",
      "301/14:\n",
      "# popularity of an actor expressed as number of films\n",
      "n = 100\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "print(\"Actors number: \", actorMovieDataset.shape[0])\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(actorMovieDataset['cast'].head(n), actorMovieDataset['count'].head(n))\n",
      "plt.title(\"10. The popularity of actors as the number of roles (top {0})\".format(n))\n",
      "plt.xlabel(\"Actor name\")\n",
      "plt.ylabel(\"Role number\")\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "plt.show()\n",
      "301/15:\n",
      "# number of actors with concrette role number\n",
      "actorMovieDataset = dataset_original[['id','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"11. Actors with given number of roles\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "301/16:\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,2)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "snbp.set_yscale('log')\n",
      "plt.title(\"12. Actors with given number of roles (logarithmic scale)\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "\n",
      "plt.show()\n",
      "301/17:\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars.info()\n",
      "301/18: oscars.head(10)\n",
      "301/19:\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset=\"id\")\n",
      "oscar_movies = oscar_movies.drop(columns=[\"film\", \"id\"])\n",
      "oscar_movies\n",
      "301/20: dataset_oscars = oscar_movies\n",
      "301/21:\n",
      "print(\"Oldest: \", dataset_oscars[\"release_date\"].min())\n",
      "print(\"Newest: \", dataset_oscars[\"release_date\"].max())\n",
      "301/22:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.year.sort_values())\n",
      "plt.title(\"13. Oscar movie releases by year\",fontsize=20)\n",
      "plt.xticks(fontsize=12,rotation=90)\n",
      "plt.xlabel(\"year\")\n",
      "plt.show()\n",
      "301/23:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.weekday.sort_values())\n",
      "plt.title(\"14. Oscar movie releases by weekday\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"weekday\")\n",
      "plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/24:\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.countplot(dataset_oscars['release_date'].dt.month.sort_values())\n",
      "plt.title(\"15. Oscar movie releases by month\",fontsize=20)\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "301/25:\n",
      "plt.figure(figsize=(20,12))\n",
      "dataset_oscars.groupby(dataset_oscars[\"release_date\"].dt.month).agg('mean')['revenue'].plot(kind='bar',rot=0)\n",
      "plt.ylabel('Mean revenue (100 million dollars)')\n",
      "plt.title(\"16. Mean revenue by month for oscar movies\")\n",
      "loc, _ = plt.xticks()\n",
      "loc, labels = loc, [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
      "plt.xticks(loc, labels,fontsize=20)\n",
      "plt.xlabel(\"month\")\n",
      "plt.show()\n",
      "301/26:\n",
      "plt.figure(figsize=(20,12))\n",
      "genre=dataset_oscars['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "count=Counter([i for j in genre for i in j]).most_common(20)\n",
      "sns.barplot([val[1] for val in count],[val[0] for val in count])\n",
      "plt.title(\"17. Movies by category\")\n",
      "plt.xlabel(\"Movie count\")\n",
      "plt.ylabel(\"Category\")\n",
      "plt.show()\n",
      "301/27:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres['revenue'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres['budget'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres['popularity'],genres.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres['runtime'],genres.index)\n",
      "plt.show()\n",
      "301/28:\n",
      "genres = dataset_oscars.explode(\"genres\").reset_index(drop=True)\n",
      "genres = genres[genres.genres.notnull()]\n",
      "genres['genres']=genres.genres.apply(lambda x :x['name'])\n",
      "genres_mean=genres.groupby(genres.genres).agg('mean')\n",
      "plt.figure(figsize=(20,12))\n",
      "plt.subplot(2,2,1)\n",
      "plt.title(\"18. Mean revenue and genre\")\n",
      "sns.barplot(genres_mean['revenue'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.title(\"19. Mean budget and genre\")\n",
      "sns.barplot(genres_mean['budget'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,3)\n",
      "plt.title(\"20. Mean popularity and genre\")\n",
      "sns.barplot(genres_mean['popularity'], genres_mean.index)\n",
      "\n",
      "plt.subplot(2,2,4)\n",
      "plt.title(\"21. Mean runtime and genre\")\n",
      "sns.barplot(genres_mean['runtime'], genres_mean.index)\n",
      "plt.show()\n",
      "301/29:\n",
      "# number of actors with concrette role number for oscar movies\n",
      "actorMovieDataset = oscar_movies[['title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset.explode('cast').reset_index(drop=True)\n",
      "actorMovieDataset = actorMovieDataset.groupby(['cast']).size().reset_index(name='count').sort_values('count', ascending=False)\n",
      "actorMovieRolesDataset = actorMovieDataset.groupby(['count']).size().reset_index(name='actors_with_given_role_number')\n",
      "x = actorMovieRolesDataset['count']\n",
      "y = actorMovieRolesDataset['actors_with_given_role_number']\n",
      "\n",
      "plt.figure(figsize=(20,12))\n",
      "#plt.subplot(2,1,1)\n",
      "snbp = sns.barplot(x='count',y='actors_with_given_role_number',data=actorMovieRolesDataset)\n",
      "show_values_on_bars(snbp, \"v\", 0.5)\n",
      "plt.title(\"22. Actors with given number of roles for oscar movies\")\n",
      "plt.xlabel(\"Roles number\")\n",
      "plt.ylabel(\"Actors number\")\n",
      "plt.show()\n",
      "301/30:\n",
      "plt.figure(figsize=(20,12))\n",
      "r = dataset.sort_values('revenue', ascending=False).head(10)\n",
      "sns.barplot(r[\"title\"], r[\"revenue\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"23. Top 10 movies by revenue\",fontsize=20)\n",
      "plt.show()\n",
      "301/31:\n",
      "r = dataset.sort_values('budget', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"budget\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"24. Top 10 movies by budget\",fontsize=20)\n",
      "plt.show()\n",
      "301/32:\n",
      "r = dataset.sort_values('popularity', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"popularity\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"25. Top 10 movies by popularity\",fontsize=20)\n",
      "plt.show()\n",
      "301/33:\n",
      "r = dataset.sort_values('runtime', ascending=False).head(10)\n",
      "plt.figure(figsize=(20,12))\n",
      "sns.barplot(r[\"title\"], r[\"runtime\"])\n",
      "plt.xticks(fontsize=12,rotation=45)\n",
      "plt.title(\"26. Top 10 movies by runtime\",fontsize=20)\n",
      "plt.show()\n",
      "301/34:\n",
      "# Movies with highest revenue in each genre\n",
      "genres.sort_values('revenue', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "301/35:\n",
      "# Movies with highest budget in each genre\n",
      "genres.sort_values('budget', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "301/36:\n",
      "# Movies with highest popularity in each genre\n",
      "genres.sort_values('popularity', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "301/37:\n",
      "# Movies with longest runtime in each genre\n",
      "genres.sort_values('runtime', ascending=False).groupby(genres.genres, sort=False).head(1)\n",
      "301/38:\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.02, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"27. Actors itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "301/39:\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"cast\"]\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "res = fpgrowth(df, min_support=0.0015, max_len=3, use_colnames=True)\n",
      "a = res[res[\"itemsets\"].str.len() >= 2].sort_values('support', ascending=False)\n",
      "plot_itemset(\"28. Actors 2- and 3-itemsets in highest revenue movies\", a, len(dataset))\n",
      "301/40:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"crew\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"29. Crew itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "301/41:\n",
      "items = 500\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"Keywords\"].head(items)\n",
      "r = r.apply(lambda x: list(map(lambda y: y[\"name\"], x))).reset_index(drop=True).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.03, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"30. Keywords itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "301/42:\n",
      "filtered_words = [\"of\", \"in\", \"a\", \"an\", \"to\", \"the\", \"this\", \"have\", \"been\", \"about\", \"only\", \"which\", \"them\", \"was\", \"while\", \"not\", \"will\", \"two\", \"can\", \"all\", \"get\", \"and\", \"is\", \"his\", \"no\", \"no\", \"now\", \"new\", \"gets\", \"go\", \"he\", \"her\", \"she\", \"with\", \"one\", \"for\", \"must\", \"be\", \"if\", \"him\", \"after\", \"out\", \"find\", \"it\", \"that\", \"from\", \"on\", \"by\", \"as\", \"who\", \"their\", \"are\", \"has\", \"true\", \"but\", \"it's\", \"at\", \"up\", \"they\", \"when\", \"into\"]\n",
      "items = 1000\n",
      "r = dataset.sort_values('revenue', ascending=False)[\"overview\"].head(items)\n",
      "r = r.apply(lambda x: list(filter(lambda y: y.lower() not in filtered_words, x.split(\" \")))).tolist()\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(r).transform(r)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "a = apriori(df, min_support=0.05, use_colnames=True).sort_values('support', ascending=False)\n",
      "plot_itemset(\"31. Overview words itemsets in \" + str(items) + \" highest revenue movies\", a, items)\n",
      "301/43:\n",
      "# Datasets loading\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset = dataset_original\n",
      "\n",
      "oscars = pd.read_csv('./tmdb-box-office-prediction/the_oscar_award.csv')\n",
      "oscars_winners = oscars[oscars[\"winner\"] == True].drop(columns=[\"winner\", \"year_film\", \"year_ceremony\", \"ceremony\", \"category\", \"name\"])\n",
      "oscars_winners = oscars_winners[oscars_winners[\"film\"].notnull()]\n",
      "dataset_lc = dataset.copy()\n",
      "oscars_winners_lc = oscars_winners.copy()\n",
      "dataset_lc['title'] = dataset_lc['title'].str.lower()\n",
      "oscars_winners_lc['film'] = oscars_winners_lc['film'].str.lower()\n",
      "oscar_movies = pd.merge(left=dataset_lc, right=oscars_winners_lc, left_on='title', right_on='film').drop_duplicates(subset='id')\n",
      "oscar_movies = oscar_movies.drop(columns=['film'])\n",
      "\n",
      "dataset = text_to_dict(dataset)\n",
      "oscar_movies = text_to_dict(oscar_movies)\n",
      "301/44:\n",
      "# Keywords clustering for all movies\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(8,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"32. Keywords group that occured together in more than 8 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "301/45:\n",
      "# Keywords clustering for oscar movies\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset['Keywords']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(keywordsMovieDataset).transform(keywordsMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"33. Keywords group that occured together in more than 3 oscar films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "301/46:\n",
      "# Actors clustering for all movies\n",
      "actorMovieDataset = dataset[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df, min_support=calculate_support(5,len(df)), max_len=5,use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"34. Actors group that played together in more than 5 films order by common films number\", frequent_itemsets, len(df), rotation=90)\n",
      "301/47:\n",
      "def calculate_support(min_occurence_number, dataset_size):\n",
      "    return min_occurence_number / dataset_size\n",
      "\n",
      "# Actors clustering for oscar movies\n",
      "actorMovieDataset = oscar_movies[['id','title','cast']].copy()\n",
      "actorMovieDataset['cast'] = actorMovieDataset['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "actorMovieDataset = actorMovieDataset['cast']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(actorMovieDataset).transform(actorMovieDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df, min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "# Uncomment to filter\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets.sort_values('length', ascending=True)\n",
      "plot_itemset(\"35. Actors group that played together in more than 3 oscars films\", frequent_itemsets, len(df), rotation=0)\n",
      "301/48:\n",
      "# Keyword words cloud analysis\n",
      "def createWordCloudForWordsString(wordsString):\n",
      "    stopwords = set(STOPWORDS) \n",
      "    wordcloud = WordCloud(width = 800, height = 800, \n",
      "                background_color ='white', \n",
      "                stopwords = stopwords, \n",
      "                min_font_size = 10).generate(wordsString) \n",
      "    return wordcloud\n",
      "\n",
      "keywordsMovieDataset = oscar_movies[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "oscarMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    oscarMoviesKeywords = oscarMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(oscarMoviesKeywords)) \n",
      "plt.title(\"36. Wordcloud for oscar movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "301/49:\n",
      "keywordsMovieDataset = dataset[['id','title','Keywords']].copy()\n",
      "keywordsMovieDataset['Keywords'] = keywordsMovieDataset['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\n",
      "keywordsMovieDataset = keywordsMovieDataset.explode('Keywords').reset_index(drop=True)\n",
      "keywordsList = keywordsMovieDataset['Keywords'].values.tolist()\n",
      "\n",
      "allMoviesKeywords = ' '\n",
      "\n",
      "for word in keywordsList: \n",
      "    allMoviesKeywords = allMoviesKeywords + str(word) + ' '\n",
      "\n",
      "# Plot oscar movies keywords wordcloud                \n",
      "plt.figure(figsize = (8, 8)) \n",
      "plt.imshow(createWordCloudForWordsString(allMoviesKeywords)) \n",
      "plt.title(\"37. Wordcloud for all movies keywords\",fontsize=20)  \n",
      "plt.gca().axes.get_xaxis().set_visible(False)\n",
      "plt.gca().axes.get_yaxis().set_visible(False)\n",
      "plt.show()\n",
      "301/50:\n",
      "# Making corelation matrix\n",
      "dataset['isOscarMovie'] = dataset['id'].apply(lambda x: 1 if x in oscar_movies['id'] else 0)\n",
      "col = ['revenue','budget','popularity','runtime','isOscarMovie']\n",
      "plt.subplots(figsize=(10, 8))\n",
      "plt.title('38. Correlation matrix for movie success markers')\n",
      "corr = dataset[col].corr()\n",
      "sns.heatmap(corr, annot=True, xticklabels=col,yticklabels=col, linewidths=.5)\n",
      "301/51:\n",
      "# Required imports\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from mlxtend.preprocessing import TransactionEncoder\n",
      "from mlxtend.frequent_patterns import apriori\n",
      "from mlxtend.frequent_patterns import fpgrowth\n",
      "from sklearn import preprocessing\n",
      "from sklearn.cluster import KMeans\n",
      "from wordcloud import WordCloud, STOPWORDS \n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from sklearn.cluster import DBSCAN\n",
      "import math\n",
      "301/52:\n",
      "crewDataset = oscar_movies[['id','title','crew']].copy()\n",
      "crewDataset['crew'] = crewDataset['crew'].apply(lambda x: ['\\\"'+i['name']+'\\\"-\\\"'+i['job']+'\\\"' for i in x] if x != {} else [])\n",
      "crewDataset = crewDataset['crew']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(crewDataset).transform(crewDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(4,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"39. Frequent itemsets for Oscars movies crew\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 39: Zbiory czste dla czonkw ekip filmowych dla filmw Oskarowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste jednowymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Wrd ekip filmowych dla filmw oskarowych pojawiaj si grupy grajce wsplnie. Najczciej przewijajc si grup s producenci wykonawczy: Bob Weinstein oraz Harvey Weinstein.\n",
      "#     - Grupy czste wystpuj zazwyczaj w obrbie danej funkcji na planie\n",
      "#     - Wzrost min-support powoduje, e zaczynaj dominowa krtsze itemsety. Tendencja trudna do zaobserwowania ze wzgldu na du rnorodno wrd filmw Oskarowych.\n",
      "301/53:\n",
      "# Film crem most frequent itemsets in films\n",
      "crewDataset = dataset[['id','title','crew']].copy()\n",
      "crewDataset['crew'] = crewDataset['crew'].apply(lambda x: ['\\\"'+i['name']+'\\\"-\\\"'+i['job']+'\\\"' for i in x] if x != {} else [])\n",
      "crewDataset = crewDataset['crew']\n",
      "\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(crewDataset).transform(crewDataset)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(10,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"40. Frequent itemsets for all movies crew\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 40: Zbiory czste dla czonkw ekip filmowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 10;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste jednowymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Wrd ekip filmowych pojawiaj si grupy czonkw, ktrzy czsto wsppracuj ze sob. Najczciej przewijajc si grup s osoby odpowiedzialne za castingi: Tricia Wood oraz Debora Aquila.\n",
      "#     - Czste zbiory wystpuj najczciej wrd castingowcw\n",
      "301/54:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: [classify_movie_budget(x)])\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"41. Frequent itemsets for Oscar movies - parameters ['production_companies','budget']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 41: Zbiory czste dla wytwrni filmowych oraz budetw filmw w kategorii filmy Oskarowe\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste dwuwymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Due wytwrnie filmowe takie, jak Warner Bros, czy Universal Pictures tworz gwnie produkcje wysokobudetowe.\n",
      "#     - Ciekawe, jest e dla wytwrni 20th Century Fox Film pojawiaj si zbiory czste dla produkcji w rnych kategoriach budetowych.\n",
      "301/55:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: [classify_movie_budget(x)])\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(20,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"42. Frequent itemsets for movies - parameters ['production_companies','budget']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 42: Zbiory czste dla wytwrni filmowych oraz budetw filmw\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 20;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste dwuwymiarowe.   \n",
      "# Wnioski:\n",
      "#     - Due wytwrnie filmowe takie, jak Warner Bros, czy Universal Pictures tworz gwnie produkcje wysokobudetowe.\n",
      "301/56:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(3,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"43. Frequent itemsets for Oscar movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 43: Zbiory czste dla wytwrni filmowych, obsady oraz sw kluczowych dla filmw Oskarowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Dozwolone zbiory czste dla wielu elementw w obrbie jednej cechy.\n",
      "301/57:\n",
      "# Making multidimensional frequent datasets for movies vector\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast','Keywords','production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [item for items in x for item in items])\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = apriori(df,min_support=calculate_support(11,len(df)), max_len=5, use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"44. Frequent itemsets for movies - parameters ['cast', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "# Wykres 44: Zbiory czste dla wytwrni filmowych, obsady oraz sw kluczowych\n",
      "# Algorytm: apriori\n",
      "# Parametry: min_support = 11;\n",
      "# Uwagi: Prezentacja przynajmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Dozwolone zbiory czste dla wielu elementw w obrbie jednej cechy.\n",
      "# Wnioski: \n",
      "#    - Czste zbiory tworz si zazwyczaj w obrbie jednego wymiaru. \n",
      "#    - Dla duych wartoci min_support dopinuj krtke zbiory czste.\n",
      "301/58:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for Oscras movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 45: Zbiory czste dla wytwrni filmowych, budetw oraz sw kluczowych dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "301/59:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(8,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"46. Frequent itemsets for movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 46: Zbiory czste dla wytwrni filmowych, budetw oraz sw kluczowych \n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 8;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Wrd czstych zbiorw pojawiaj si wysokobudetowe filmy Worner Bros ze sowem kluczowym dc comics\n",
      "#    - Wrd czstych zbiorw pojawiaj si rwnie wysokobudetowe filmy wytwrni 20th Century Fox Film ze sowem kluczowym alien\n",
      "301/60:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"47. Frequent itemsets for Oscras movies - parameters ['cast', 'crew']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 47: Zbiory czste dla obsady i ekipy filmowej dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja co najmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Dla filmw Oskarowych wrd zbiorw czstych dla obsady i ekipy pojaiwaj si grupy. Wskazuje, to na fakt istnienia zalenoci, e producenci/reyserzy maj swoiuch ulubionych aktorw.\n",
      "301/61:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"48. Frequent itemsets for Oscars movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 48: Zbiory czste dla obsady, budetu i wytwrni dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Liczba czstych zbiorw i ich support jest niewielki dla filmw oskarowych dla wektora cech: budet, obsada, wytwrnia\n",
      "301/62:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(7,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"49. Frequent itemsets for movies - parameters ['budget', 'cast', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 49: Zbiory czste dla obsady, budetu i wytwrni filmowej\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 7;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "301/63:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(2,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"50. Frequent itemsets for Oscar movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 50: Zbiory czste dla obsady, reysera i wytwrni dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 2;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "301/64:\n",
      "#Frequet itemsets for resolved vectors\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast', 'crew', 'production_companies']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x if i['job'] == 'Director'] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(4,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"51. Frequent itemsets for movies - parameters ['cast', 'director', 'production_company']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 51: Zbiory czste dla obsady, reysera i wytwrni dla wszystkich filmw\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 4;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "301/65:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['revenue','budget','popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 10]\n",
      "processedDataset = processedDataset[processedDataset['revenue'] < 2000000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] < 2500000]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "fig = plt.figure(figsize=(12, 12))\n",
      "ax = Axes3D(fig)\n",
      "\n",
      "random_state = 170\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "ax.scatter(processedDataset['revenue'], processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"52. Movie clusters (KMeans k = 4 ) by revenue, budget and popularity\")\n",
      "plt.xlabel(\"Revenue\")\n",
      "plt.ylabel(\"Budget\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 52: Klastrowanie zbioru filmw dla cech revenue, budget, popularity\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 4;\n",
      "# Wnioski:\n",
      "#    - Trudno wyodrbni spjne grupy. Du grup stanowi filmy niskobudetowe, ktre przyniosy niski dochd i nie byy popularne.\n",
      "301/66:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"53. Movie clustering (KMeans) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 53: Klastrowanie zbioru filmw dla cech revenue, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n = 3;\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe i niskodochodowe.\n",
      "301/67:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'revenue']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['revenue'], c=y_pred)\n",
      "plt.title(\"54. Movie clustering (DBSCAN) by budget and revenue\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Revenue\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 54: Klastrowanie zbioru filmw dla cech revenue, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe i niskodochodowe.\n",
      "301/68:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"55. Movie clustering (KMeans) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 55: Klastrowanie zbioru filmw dla cech popularity, budget\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=3;\n",
      "# Uwagi: dokonano normalizacji danych. Odfiltrowano zerowe budety.\n",
      "# Wnioski:\n",
      "#  - Grupy nie s wyrane. Najwiksz grup stanowi filmy niskobudetowe, ktre wykazuj popularnoc poniej przecitnej.\n",
      "301/69:\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.01, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"56. Movie clustering (DBSCAN) by budget and popularity\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 56: Klastrowanie zbioru filmw dla cech popularity, budget\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps = 0.01;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "301/70:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['budget', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 100]\n",
      "processedDataset = processedDataset[processedDataset['budget'] > 0]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "y_pred = DBSCAN(eps=0.06, min_samples=2).fit(processedDataset)\n",
      "y_pred =y_pred.labels_\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['budget'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"57. Movie clustering (DBSCAN) by budget and popularity for Oscars movies\")\n",
      "plt.xlabel(\"Budget\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 57: Klastrowanie zbioru filmw dla cech popularity, budget dla filmw Oskarowych\n",
      "# Algorytm: DBSCAN\n",
      "# Parametry: eps=0.06;\n",
      "# Uwagi: dokonano normalizacji danych.\n",
      "301/71:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['runtime', 'popularity']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.dropna()\n",
      "processedDataset = processedDataset[processedDataset['popularity'] < 20]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] < 200]\n",
      "processedDataset = processedDataset[processedDataset['runtime'] > 50]\n",
      "processedDataset=(processedDataset-processedDataset.min())/(processedDataset.max()-processedDataset.min())\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "\n",
      "random_state = 13\n",
      "y_pred = KMeans(n_clusters=4, random_state=random_state).fit_predict(processedDataset)\n",
      "\n",
      "plt.subplot(111)\n",
      "plt.scatter(processedDataset['runtime'], processedDataset['popularity'], c=y_pred)\n",
      "plt.title(\"58. Movie clustering (KMeans) by runtime and popularity\")\n",
      "plt.xlabel(\"Runtime\")\n",
      "plt.ylabel(\"Popularity\")\n",
      "plt.show()\n",
      "\n",
      "# Wykres 58: Klastrowanie zbioru filmw dla cech popularity, runtime\n",
      "# Algorytm: KMeans\n",
      "# Parametry: clusters_n=4;\n",
      "# Uwagi: dokonano normalizacji danych. Usunito skrajne wartoci dla czasu trwania dla lepszej wizualizacji.\n",
      "301/72:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for Oscars movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 45: Zbiory czste dla wytwrni filmowych, budetw oraz sw kluczowych dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "301/73:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['Keywords', 'production_companies', 'budget']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['Keywords'] = processedDataset['Keywords'].apply(lambda x: [i['name']+'-Keyword' for i in x] if x != {} else [])\n",
      "processedDataset['production_companies'] = processedDataset['production_companies'].apply(lambda x: [i['name']+'-Company' for i in x] if x != {} else [])\n",
      "processedDataset['budget'] = processedDataset['budget'].apply(lambda x: classify_movie_budget(x))\n",
      "processedDataset = processedDataset.explode('Keywords').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('production_companies').reset_index(drop=True)\n",
      "processedDataset = processedDataset[processedDataset['budget'] != 'no data']\n",
      "processedDataset = processedDataset[processedDataset['Keywords'] != 'nan']\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "\n",
      "processedDataset\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 3)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"45. Frequent itemsets for Oscar movies - parameters ['budget', 'Keywords', 'production_companies']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 45: Zbiory czste dla wytwrni filmowych, budetw oraz sw kluczowych dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja trzyelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "301/74:\n",
      "processedDataset = oscar_movies.copy()\n",
      "features_vector = ['cast', 'crew']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "\n",
      "processedDataset['cast'] = processedDataset['cast'].apply(lambda x: [i['name']+'-Cast' for i in x] if x != {} else [])\n",
      "processedDataset['crew'] = processedDataset['crew'].apply(lambda x: [i['name']+'-'+i['job'] for i in x] if x != {} else [])\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset = processedDataset.explode('crew').reset_index(drop=True)\n",
      "\n",
      "processedDataset['elements'] = processedDataset.values.tolist() \n",
      "processedDataset['elements'] = processedDataset['elements'].apply(lambda x: [str(item) for item in x])\n",
      "ds = processedDataset['elements']\n",
      "te = TransactionEncoder()\n",
      "te_ary = te.fit(ds).transform(ds)\n",
      "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
      "frequent_itemsets = fpgrowth(df,min_support=calculate_support(3,len(df)), use_colnames=True)\n",
      "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
      "\n",
      "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['length'] >= 2)]\n",
      "frequent_itemsets = frequent_itemsets.sort_values(['support', 'length'], ascending=[False, False])\n",
      "plot_itemset(\"47. Frequent itemsets for Oscar movies - parameters ['cast', 'crew']\", frequent_itemsets, len(df), rotation=90)\n",
      "\n",
      "# Wykres 47: Zbiory czste dla obsady i ekipy filmowej dla filmw Oskarowych\n",
      "# Algorytm: fpgrowth\n",
      "# Parametry: min_support = 3;\n",
      "# Uwagi: Prezentacja co najmniej dwuelementowych itemsetw. Zbiry czste wielowymiarowe. Zbiory czste dla wielu elementw w obrbie jednej cechy nie s dozwolone.\n",
      "# Wnioski: \n",
      "#    - Dla filmw Oskarowych wrd zbiorw czstych dla obsady i ekipy pojaiwaj si grupy. Wskazuje, to na fakt istnienia zalenoci, e producenci/reyserzy maj swoiuch ulubionych aktorw.\n",
      "302/1:\n",
      "import numpy as np\n",
      "from qiskit import *\n",
      "%matplotlib inline\n",
      "\n",
      "from qiskit import Aer\n",
      "from qiskit.visualization import plot_state_city\n",
      "from qiskit.visualization import plot_histogram\n",
      "\n",
      "from qiskit import IBMQ\n",
      "from qiskit.providers.ibmq import least_busy\n",
      "from qiskit.tools.monitor import job_monitor\n",
      "305/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "306/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "306/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "306/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "306/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "306/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/6:\n",
      "\n",
      "full_data_subset = dataset[[\"Budget\"]]\n",
      "306/7: full_data_subset = dataset[[\"Budget\"]]\n",
      "306/8: dataset[\"budget\"]\n",
      "306/9: dataset[\"budget\", \"revenue\"]\n",
      "306/10: dataset[[\"budget\", \"revenue\"]]\n",
      "306/11:\n",
      "dataset[[\"budget\", \"revenue\"]]\n",
      "cpi.update()\n",
      "306/12: dataset[[\"budget\", \"revenue\"]]\n",
      "306/13: dataset\n",
      "306/14:\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/15:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/16:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/17:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(df[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "306/18:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "307/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "307/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "307/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "307/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "307/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "307/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "307/7:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "307/8:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "307/9:\n",
      "dataset = dataset_original.copy()\n",
      "dataset = text_to_dict(dataset)\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "308/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "308/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "308/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "308/4:\n",
      "dataset = dataset_original.copy()\n",
      "dataset = text_to_dict(dataset)\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "308/5:\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "308/6:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/7:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/8:\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "308/9:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/10:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/11:\n",
      "def map_cast(cast_lists):\n",
      "    a = {}\n",
      "    for l in cast_lists:\n",
      "        for name in l:\n",
      "            if(name !in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "308/12:\n",
      "def map_cast(cast_lists):\n",
      "    a = {}\n",
      "    for l in cast_lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "308/13: dataset[\"cast\"]\n",
      "308/14: map_cast(dataset[\"cast\"])\n",
      "308/15:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/16:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "308/17:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/18:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/19:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/20:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/21:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/22:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/23:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\", \"crew\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: [cast_mapping[i] for i in x[\"cast\"]] , axis=1)\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/24:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "dataset\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/25:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/26:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/27:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/28:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/29: dataset[\"cast\"] = dataset.apply(lambda x: numpy.array(x[\"cast\"]) , axis=1)\n",
      "308/30: dataset[\"cast\"] = dataset.apply(lambda x: np.array(x[\"cast\"]) , axis=1)\n",
      "308/31:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/32:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/33: dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "308/34:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "308/35: dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "309/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "309/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "309/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "309/4:\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "309/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "309/7:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "309/8:\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "309/9: # dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "309/10:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/11:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/12:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]] #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]]\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/13:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/14:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/15: dataset.apply(lambda x: np.vectorize(x[\"cast\"]) , axis=1)\n",
      "309/16:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\"].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/17:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/18:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(criterion = \"gini\", max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/19:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/20:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"cast\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/21:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"title\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/22:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"title\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "prepare(X_train)\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "309/23:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "cast_names = cast_mapping.keys()\n",
      "cast_names\n",
      "# for g in top_cast_names:\n",
      "#     train['cast_name_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\n",
      "309/24:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "cast_names = cast_mapping.keys()\n",
      "cast_names\n",
      "for name in cast_names:\n",
      "    dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "309/25:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "309/26:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset\n",
      "309/27:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset\n",
      "309/28:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/29:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset.first()[\"crew\"]\n",
      "309/30:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset.first(0)[\"crew\"]\n",
      "309/31:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[0][\"crew\"]\n",
      "309/32:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0]\n",
      "309/33:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset[\"crew\"][0]\n",
      "309/34:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "dataset['cast'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset[\"crew\"][0]\n",
      "309/35:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "dataset['cast'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset[\"crew\"][0]\n",
      "309/36:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset[\"crew\"][0]\n",
      "309/37:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0]\n",
      "309/38:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0][\"job\"]\n",
      "309/39:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "type(dataset[\"crew\"][0])\n",
      "309/40:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0]\n",
      "309/41:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0][0]\n",
      "309/42:\n",
      "# dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0][1]\n",
      "309/43:\n",
      "## dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"][0]\n",
      "309/44:\n",
      "## dataset['director'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# dataset['crew'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/45:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in tqdm(json_cols + ['belongs_to_collection']) :\n",
      "    train[col] = train[col].apply(lambda x : get_dictionary(x))\n",
      "    test[col] = test[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "def get_json_dict(df) :\n",
      "    global json_cols\n",
      "    result = dict()\n",
      "    for e_col in json_cols :\n",
      "        d = dict()\n",
      "        rows = df[e_col].values\n",
      "        for row in rows :\n",
      "            if row is None : continue\n",
      "            for i in row :\n",
      "                if i['name'] not in d :\n",
      "                    d[i['name']] = 0\n",
      "                d[i['name']] += 1\n",
      "        result[e_col] = d\n",
      "    return result\n",
      "\n",
      "train_dict = get_json_dict(dataset)\n",
      "309/46:\n",
      "from tqdm import tqdm\n",
      "\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in tqdm(json_cols + ['belongs_to_collection']) :\n",
      "    train[col] = train[col].apply(lambda x : get_dictionary(x))\n",
      "    test[col] = test[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "def get_json_dict(df) :\n",
      "    global json_cols\n",
      "    result = dict()\n",
      "    for e_col in json_cols :\n",
      "        d = dict()\n",
      "        rows = df[e_col].values\n",
      "        for row in rows :\n",
      "            if row is None : continue\n",
      "            for i in row :\n",
      "                if i['name'] not in d :\n",
      "                    d[i['name']] = 0\n",
      "                d[i['name']] += 1\n",
      "        result[e_col] = d\n",
      "    return result\n",
      "\n",
      "train_dict = get_json_dict(dataset)\n",
      "309/47:\n",
      "from tqdm import tqdm\n",
      "\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in tqdm(json_cols + ['belongs_to_collection']) :\n",
      "    train[col] = train[col].apply(lambda x : get_dictionary(x))\n",
      "    test[col] = test[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "def get_json_dict(df) :\n",
      "    global json_cols\n",
      "    result = dict()\n",
      "    for e_col in json_cols :\n",
      "        d = dict()\n",
      "        rows = df[e_col].values\n",
      "        for row in rows :\n",
      "            if row is None : continue\n",
      "            for i in row :\n",
      "                if i['name'] not in d :\n",
      "                    d[i['name']] = 0\n",
      "                d[i['name']] += 1\n",
      "        result[e_col] = d\n",
      "    return result\n",
      "\n",
      "train_dict = get_json_dict(dataset)\n",
      "309/48:\n",
      "from tqdm import tqdm\n",
      "\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in tqdm(json_cols + ['belongs_to_collection']) :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "def get_json_dict(df) :\n",
      "    global json_cols\n",
      "    result = dict()\n",
      "    for e_col in json_cols :\n",
      "        d = dict()\n",
      "        rows = df[e_col].values\n",
      "        for row in rows :\n",
      "            if row is None : continue\n",
      "            for i in row :\n",
      "                if i['name'] not in d :\n",
      "                    d[i['name']] = 0\n",
      "                d[i['name']] += 1\n",
      "        result[e_col] = d\n",
      "    return result\n",
      "\n",
      "train_dict = get_json_dict(dataset)\n",
      "309/49: train_dict\n",
      "309/50: dataset\n",
      "309/51: dataset[\"production_companies\"]\n",
      "309/52: dataset[\"production_companies\"][0]\n",
      "309/53: dataset[\"production_companies\"][0][0]\n",
      "309/54: dataset[\"production_companies\"][0][0][\"name\"]\n",
      "309/55: dataset[\"crew\"][0][0][\"name\"]\n",
      "309/56: dataset[\"crew\"][0]\n",
      "309/57: dataset[\"crew\"][0].index(lambda x: x[\"job\"] == \"Director\")\n",
      "309/58: next(x for x in dataset[\"crew\"][0] if x[\"job\"] == \"Director\")\n",
      "309/59: next(x[\"name\"] for x in dataset[\"crew\"][0] if x[\"job\"] == \"Director\")\n",
      "309/60: next(x[\"name\"] for x in dataset[\"crew\"][1] if x[\"job\"] == \"Director\")\n",
      "309/61:\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: next(y[\"name\"] for y in x[\"crew\"] if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/62:\n",
      "# dataset['director'] = dataset['crew'].apply(lambda x: next(y[\"name\"] for y in x[\"crew\"] if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/63:\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: next(y[\"name\"] for y in x[\"crew\"] if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "309/64:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: get_director(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "310/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "310/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "310/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "310/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "310/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "310/7:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "310/8:\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "310/9:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in json_cols + ['belongs_to_collection'] :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "310/10:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: get_director(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/11:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset['director'] = dataset['crew'].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/12:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset['crew'].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/13:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset[\"crew\"]\n",
      "310/14:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "# dataset[\"crew\"].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset\n",
      "310/15:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset\n",
      "310/16:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"]#.apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "dataset\n",
      "310/17:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"]#.apply(lambda x: print(x[\"crew\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/18:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: get_director(x))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/19:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [get_director(x)])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/20:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: x[\"job\"])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/21:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: x[\"job\"][0])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/22:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(y[\"name\"] for y in data if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/23:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(y[\"name\"] for y in x if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/24:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(y[\"name\"] for y in x if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/25:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [y[\"name\"] for y in x if y[\"job\"] == \"Director\"])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/26:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(y[\"name\"] for y in x if y[\"job\"] == \"Director\"))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/27:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/28:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [y[\"name\"] for y in x if y[\"job\"] == \"Director\"])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/29:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [y[\"name\"] for y in x if y[\"job\"] == \"Director\"][0])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/30:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: [y[\"name\"] for y in x if y[\"job\"] == \"Director\"])\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/31:\n",
      "def get_director(data):\n",
      "    try:\n",
      "        next(y[\"name\"] for y in data if y[\"job\"] == \"Director\")\n",
      "    except:\n",
      "        \"\"\n",
      "\n",
      "dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "# dataset\n",
      "310/32: dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "310/33:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "310/34:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"director\"],prefix='director',drop_first=True)\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "310/35:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reyser, scenariusz, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"director\"],prefix='director',drop_first=True)\n",
      "# X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "311/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "311/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "311/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "311/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "311/7:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "311/8:\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "311/9:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in json_cols + ['belongs_to_collection'] :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "311/10: dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "311/11:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset\n",
      "311/12: dataset\n",
      "311/13:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "pd.get_dummies(dataset[\"director\"], prefix='director', drop_first=True)\n",
      "# X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/14:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "onehot_encoder = OneHotEncoder(sparse=False)\n",
      "onehot_encoded = onehot_encoder.fit_transform(dataset[\"director\"])\n",
      "dataset\n",
      "311/15:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "onehot_encoder = OneHotEncoder(sparse=False)\n",
      "onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "dataset\n",
      "311/16:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd\n",
      "311/17:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[\"director\"]\n",
      "311/18:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 9]\n",
      "311/19:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 10]\n",
      "311/20:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 11]\n",
      "311/21:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 12]\n",
      "311/22:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 13]\n",
      "311/23:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 14]\n",
      "311/24:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 20]\n",
      "311/25:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 25]\n",
      "311/26:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd)\n",
      "# dataset\n",
      "asd[:, 23]\n",
      "311/27:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "onehot_encoder = OneHotEncoder(sparse=False)\n",
      "onehot_encoded = onehot_encoder.fit_transform(asd[:, 23])\n",
      "dataset\n",
      "311/28:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "onehot_encoder = OneHotEncoder(sparse=False)\n",
      "onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "dataset\n",
      "311/29:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "311/30:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "311/31:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "311/32:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset\n",
      "311/33:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/34:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "311/35:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director\"]].to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/36:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset[[\"budget\", \"director_*\"]].to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/37:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.* budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/38:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.* budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/39:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/40:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/41:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "X_train\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/42:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "y_train\n",
      "# X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/43:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -3))\n",
      "311/44:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "y_train\n",
      "# X_train\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/45:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/46:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 20)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        s = classifier.score(xtest, ytest)\n",
      "        scores.append(s)\n",
      "    print(\"Score: \", s, \" for depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/47:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/48:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeClassifier(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/49:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "311/50: pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "311/51:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset\n",
      "311/52:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset[\"cast\"]\n",
      "312/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "312/2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "312/3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "312/4:\n",
      "dataset = dataset_original.copy()\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "312/5:\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "312/6:\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "312/7:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "312/8:\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# # crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "312/9:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords', 'crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in json_cols + ['belongs_to_collection'] :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "312/10: dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "312/11:\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "processedDataset = dataset.copy()\n",
      "features_vector = ['cast']\n",
      "processedDataset = processedDataset[features_vector]\n",
      "processedDataset = processedDataset.explode('cast').reset_index(drop=True)\n",
      "processedDataset\n",
      "312/12:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -3))\n",
      "312/13:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "\n",
      "# depth_range = range(1, 15)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "312/14: pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "312/15:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset\n",
      "312/16:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset[\"cast\"]\n",
      "312/17:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "dataset\n",
      "312/18:\n",
      "pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset\n",
      "312/19:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "dataset.cast.apply(ast.literal_eval)\n",
      "\n",
      "# depth_range = range(1, 15)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "312/20:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset\n",
      "dataset.cast.str.join('|').str.get_dummies().add_prefix('dummy_name_')\n",
      "312/21:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset\n",
      "# dataset.cast.str.join('|').str.get_dummies(drop_first=True).add_prefix('actor_')\n",
      "\n",
      "dataset.explode(\"cast\")\n",
      "312/22:\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset\n",
      "# dataset.cast.str.join('|').str.get_dummies(drop_first=True).add_prefix('actor_')\n",
      "\n",
      "dataset.explode(\"cast\")[\"cast\"]\n",
      "312/23:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -5))\n",
      "dataset = dataset.explode(\"cast\")\n",
      "312/24:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget|cast\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "312/25:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "asd = dataset.copy().to_numpy()\n",
      "# onehot_encoder = OneHotEncoder(sparse=False)\n",
      "# onehot_encoded = onehot_encoder.fit_transform(asd[:, 23].reshape(-1, 1))\n",
      "# dataset\n",
      "# pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "# dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "# dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -5))\n",
      "# dataset = dataset.explode(\"cast\")\n",
      "dataset = pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "312/26:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "\n",
      "X_train = dataset.filter(regex=(\"director_.*|budget|actor_.*\")).to_numpy() #aktorzy, reyser, budet\n",
      "y_train = dataset[[\"revenue\"]].to_numpy()\n",
      "\n",
      "depth_range = range(1, 15)\n",
      "best_depth = 1\n",
      "max_score = 0\n",
      "for depth in depth_range:\n",
      "    scores = []\n",
      "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
      "    cv = KFold(n_splits=10)\n",
      "    for train_index, test_index in cv.split(X_train):\n",
      "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "        classifier.fit(xtrain, ytrain)\n",
      "        scores.append(classifier.score(xtest, ytest))\n",
      "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
      "    if np.mean(scores) > max_score:\n",
      "        max_score = np.mean(scores)\n",
      "        best_depth = depth\n",
      "print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "313/1:\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "313/2:\n",
      "\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "313/3:\n",
      "\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "313/4:\n",
      "\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "313/5:\n",
      "\n",
      "# X_train = train.to_numpy()[:, 2:]\n",
      "# y_train = train.to_numpy()[:, 1]\n",
      "# depth_range = range(1, 20)\n",
      "# best_depth = 1\n",
      "# max_score = 0\n",
      "# for depth in depth_range:\n",
      "#     scores = []\n",
      "#     classifier = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth)\n",
      "#     cv = KFold(n_splits=10)\n",
      "#     for train_index, test_index in cv.split(X_train):\n",
      "#         xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
      "#         classifier.fit(xtrain, ytrain)\n",
      "#         scores.append(classifier.score(xtest, ytest))\n",
      "#     if np.mean(scores) > max_score:\n",
      "#         max_score = np.mean(scores)\n",
      "#         best_depth = depth\n",
      "# print(\"Max score: \", max_score, \" for depth: \", best_depth)\n",
      "313/6:\n",
      "\n",
      "# class MultiHotEncoder(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self):\n",
      "#         self.categories=[]\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         self.categories=[]\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             self.categories+=list(df.columns)\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=X.index)\n",
      "#         for name,series in X.iteritems():\n",
      "#             df=to_split(series).str.get_dummies()\n",
      "#             df.columns=name+\"_\"+df.columns\n",
      "#             result=result.join(df)\n",
      "#         for i,cate in enumerate(self.categories):\n",
      "#             if result.columns[i]!=cate:\n",
      "#                 result.insert(i,cate,0)\n",
      "#         return result.reset_index(drop=True)\n",
      "#\n",
      "#\n",
      "# def to_split(series,sep=\"|\",index=\"name\",threshold=0):\n",
      "#     count=multi_count(series,index)\n",
      "#     result=pd.Series(index=series.index)\n",
      "#     for i in range(series.size):\n",
      "#         l=eval(series.iloc[i])\n",
      "#         result.iloc[i]=\"\"\n",
      "#         for x in l:\n",
      "#             if count[x[index]]>=threshold:\n",
      "#                 result.iloc[i]+=x[index]+sep\n",
      "#     return result\n",
      "#\n",
      "# class DataFrameTransformer(BaseEstimator,TransformerMixin):\n",
      "#     def __init__(self,transformers):\n",
      "#         self.transformers=transformers\n",
      "#\n",
      "#     def fit(self,X,y=None):\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]!=\"passthrough\" and i[1]!=\"drop\":i[1].fit(X[i[2]])\n",
      "#         return self\n",
      "#\n",
      "#     def transform(self,X,y=None):\n",
      "#         result=pd.DataFrame(index=range(X.shape[0]))\n",
      "#         for i in self.transformers:\n",
      "#             if i[1]==\"passthrough\":x=X[i[2]].reset_index(drop=True)\n",
      "#             elif i[1]==\"drop\":continue\n",
      "#             else:x=i[1].transform(X[i[2]])\n",
      "#             if type(x) is not pd.DataFrame:x=pd.DataFrame(x,columns=i[2])\n",
      "#             result=result.join(x)\n",
      "#         return result\n",
      "313/7:\n",
      "\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "313/8:\n",
      "\n",
      "cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "313/9:\n",
      "\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "313/10:\n",
      "\n",
      "# dataset['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
      "cast_names = cast_mapping.keys()\n",
      "cast_names\n",
      "for name in cast_names:\n",
      "    dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "313/11: %history -g\n",
      "313/12: %history\n",
      "313/13: %history -g\n",
      "   1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns \n",
      "import statsmodels\n",
      "from collections import Counter\n",
      "import cpi\n",
      "import ast\n",
      "import json\n",
      "import datetime\n",
      "from sklearn.decomposition import PCA\n",
      "from collections import Counter\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "   2:\n",
      "def text_to_dict(df):\n",
      "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
      "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
      "    for column in dict_columns:\n",
      "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
      "    return df\n",
      "\n",
      "def classify_movie_budget(budget):\n",
      "    if budget == 0:\n",
      "        return 'no data'\n",
      "    elif budget < 400000:\n",
      "        return 'micro-budget'\n",
      "    elif budget < 2000000:\n",
      "        return 'low-budget'\n",
      "    elif budget < 10000000:\n",
      "        return 'middle-budget'\n",
      "    else:\n",
      "        return 'high-budget'\n",
      "   3:\n",
      "dataset_original = pd.read_csv('./tmdb-box-office-prediction/train.csv')\n",
      "dataset_original.info()\n",
      "   4:\n",
      "dataset = dataset_original.copy()\n",
      "dataset['cast'] = dataset['cast'].apply(lambda x: {} if pd.isna(x) else [x.get('name') for x in ast.literal_eval(x)])\n",
      "\n",
      "\n",
      "def fix_date(date):\n",
      "    x = pd.to_datetime(date, format=\"%m/%d/%y\")\n",
      "    if x.year > 2020:\n",
      "        year = x.year - 100\n",
      "    else:\n",
      "        year = x.year\n",
      "    return datetime.datetime(year,x.month,x.day)\n",
      "\n",
      "dataset[\"release_date\"] = dataset_original[\"release_date\"].apply(fix_date)\n",
      "\n",
      "def adjust_price_to_inflation(price, date):\n",
      "    return int(cpi.inflate(price, date.year))\n",
      "\n",
      "dataset[\"budget\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"budget\"], x[\"release_date\"]), axis=1)\n",
      "dataset[\"revenue\"] = dataset.apply(lambda x: adjust_price_to_inflation(x[\"revenue\"], x[\"release_date\"]), axis=1)\n",
      "   5:\n",
      "def map_list_to_integer(lists):\n",
      "    a = {}\n",
      "    for l in lists:\n",
      "        for name in l:\n",
      "            if(name not in a):\n",
      "                a[name] = len(a)\n",
      "    return a\n",
      "   6:\n",
      "# cast_mapping = map_list_to_integer(dataset[\"cast\"])\n",
      "# crew_mapping = map_list_to_integer(dataset[\"crew\"])\n",
      "# dataset[\"cast\"] = dataset.apply(lambda x: np.asarray([cast_mapping[i] for i in x[\"cast\"]]) , axis=1)\n",
      "   7: # dataset[\"cast\"] = dataset.apply(lambda x: np.asarray(x[\"cast\"]) , axis=1)\n",
      "   8:\n",
      "# cast_names = cast_mapping.keys()\n",
      "# cast_names\n",
      "# for name in cast_names:\n",
      "#     dataset['cast_name_' + name] = dataset['cast'].apply(lambda x: 1 if name in x else 0)\n",
      "   9:\n",
      "json_cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'Keywords','crew']\n",
      "\n",
      "def get_dictionary(s):\n",
      "    try:\n",
      "        d = eval(s)\n",
      "    except:\n",
      "        d = {}\n",
      "    return d\n",
      "\n",
      "for col in json_cols + ['belongs_to_collection'] :\n",
      "    dataset[col] = dataset[col].apply(lambda x : get_dictionary(x))\n",
      "    \n",
      "dataset[\"director\"] = dataset[\"crew\"].apply(lambda x: next(iter([y[\"name\"] for y in x if y[\"job\"] == \"Director\"]), None))\n",
      "dataset = pd.get_dummies(dataset,prefix=['director'], columns = ['director'], drop_first=True)\n",
      "\n",
      "dataset[\"revenue\"] = dataset[\"revenue\"].apply(lambda x: round(x, -5))\n",
      "# dataset.cast.str.join('|').str.get_dummies(drop_first=True).add_prefix('actor_')\n",
      "\n",
      "dataset = dataset.explode(\"cast\")\n",
      "dataset = pd.get_dummies(dataset,prefix=['actor'], columns = ['cast'], drop_first=True)\n",
      "  10: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "X_train = dataset.filter(regex=(\"director_.*|budget|actor_.*\")).to_numpy() #aktorzy, reyser, budet\n",
    "y_train = dataset[[\"revenue\"]].to_numpy()\n",
    "\n",
    "# pd.get_dummies(dataset[\"cast\"],prefix='cast',drop_first=True)\n",
    "X_train\n",
    "depth_range = range(1, 15)\n",
    "best_depth = 1\n",
    "max_score = 0\n",
    "for depth in depth_range:\n",
    "    scores = []\n",
    "    classifier = DecisionTreeRegressor(max_depth = depth)\n",
    "    cv = KFold(n_splits=10)\n",
    "    for train_index, test_index in cv.split(X_train):\n",
    "        xtrain, xtest, ytrain, ytest = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "        classifier.fit(xtrain, ytrain)\n",
    "        scores.append(classifier.score(xtest, ytest))\n",
    "    print(\"Score: \", np.mean(scores), \"depth: \", depth)\n",
    "    if np.mean(scores) > max_score:\n",
    "        max_score = np.mean(scores)\n",
    "        best_depth = depth\n",
    "print(\"Max score: \", max_score, \" for depth: \", best_depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}